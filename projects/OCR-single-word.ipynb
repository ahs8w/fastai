{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:21:50.199070Z",
     "start_time": "2018-11-07T23:21:49.903643Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:21:51.890627Z",
     "start_time": "2018-11-07T23:21:50.292753Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:02.631235Z",
     "start_time": "2018-11-07T23:22:02.581292Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = Path('data/IAM_handwriting')\n",
    "TMP_PATH = PATH/'tmp'\n",
    "CSV = PATH/'words.csv'\n",
    "\n",
    "# !ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:21:52.492087Z",
     "start_time": "2018-11-07T23:21:52.435842Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prelim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Manual DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:38:46.865953Z",
     "start_time": "2018-11-07T23:38:45.770977Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maxTextLen = 32\n",
    "samples = []\n",
    "chars = set()\n",
    "\n",
    "f=open(f'{PATH}/ascii/words.txt')\n",
    "for line in f:\n",
    "    # ignore comment line\n",
    "    if not line or line[0]=='#':\n",
    "        continue\n",
    "\n",
    "    lineSplit = line.strip().split(' ')\n",
    "    assert len(lineSplit) >= 9\n",
    "\n",
    "    fileName = lineSplit[0]\n",
    "\n",
    "    # GT text are columns starting at 9\n",
    "    gtText = ''.join(lineSplit[8:])[:maxTextLen]\n",
    "    char_len = len(gtText)\n",
    "    chars = chars.union(set(list(gtText)))\n",
    "\n",
    "    # put sample into list\n",
    "    samples.append([fileName, gtText, char_len])\n",
    "    \n",
    "data = np.stack(samples)\n",
    "df = pd.DataFrame(data, columns=['filename', 'word', 'char_len'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:38:47.600235Z",
     "start_time": "2018-11-07T23:38:47.508416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['char_len'] = df.char_len.astype('int32')\n",
    "df = df.loc[df['char_len'] > 3]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Numericalize characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:39:51.720393Z",
     "start_time": "2018-11-07T23:39:51.667708Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itos = sorted(list(chars))\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(1, ' ')\n",
    "itos.insert(2, '_unk_')\n",
    "itos.insert(3, '_eos_')\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:39:52.887885Z",
     "start_time": "2018-11-07T23:39:52.683282Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joined_labels = list(df.word)\n",
    "\n",
    "stoi = collections.defaultdict(lambda: 2, {v:k for k,v in enumerate(itos)})\n",
    "ids = np.array([np.array([stoi[letter] for letter in word] + [3]) for word in joined_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:39:54.924175Z",
     "start_time": "2018-11-07T23:39:54.459363Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert to strings (as labels)\n",
    "str_ids = np.array([' '.join(str(l) for l in w) for w in ids]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:39:55.867368Z",
     "start_time": "2018-11-07T23:39:55.706884Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['char_ids'] = str_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:40:00.902609Z",
     "start_time": "2018-11-07T23:40:00.850646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(itos, open(TMP_PATH/'char_itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:40:07.434509Z",
     "start_time": "2018-11-07T23:40:07.261916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(CSV, columns=['filename', 'char_ids'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:10.891965Z",
     "start_time": "2018-11-07T23:22:10.687294Z"
    }
   },
   "outputs": [],
   "source": [
    "itos = pickle.load(open(TMP_PATH/'char_itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:10.891965Z",
     "start_time": "2018-11-07T23:22:10.687294Z"
    }
   },
   "source": [
    "#### Full ~57,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:40:38.184939Z",
     "start_time": "2018-11-07T23:40:38.078139Z"
    }
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(CSV)\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample ~5,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:41:32.856937Z",
     "start_time": "2018-11-07T23:41:32.778949Z"
    }
   },
   "outputs": [],
   "source": [
    "CSV = 'sample_words.csv'\n",
    "\n",
    "samp = csv.sample(5000).reset_index(drop=True)\n",
    "samp.to_csv(CSV, index=False)\n",
    "\n",
    "csv = pd.read_csv(CSV)\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get val_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:41:45.437864Z",
     "start_time": "2018-11-07T23:41:45.386098Z"
    }
   },
   "outputs": [],
   "source": [
    "val_idxs = np.array(csv.sample(frac=0.15).index)\n",
    "len(val_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelData object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:41:47.371935Z",
     "start_time": "2018-11-07T23:41:47.321729Z"
    }
   },
   "outputs": [],
   "source": [
    "f = resnet34\n",
    "sz = 64\n",
    "bs = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:41:48.022407Z",
     "start_time": "2018-11-07T23:41:47.906706Z"
    }
   },
   "outputs": [],
   "source": [
    "aug_tfms = [RandomRotate(3, mode=1), RandomLighting(0.05, 0.05)]\n",
    "\n",
    "tfms = tfms_from_model(f, sz, crop_type=CropType.NO, aug_tfms=aug_tfms)\n",
    "data = ImageClassifierData.from_csv(PATH, 'words', CSV, bs=bs, val_idxs=val_idxs, \n",
    "                                    suffix='.png', tfms=tfms, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:41:48.580228Z",
     "start_time": "2018-11-07T23:41:48.528015Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad ends of lines with pad token for language model\n",
    "data.aug_dl.pre_pad=False\n",
    "data.trn_dl.pre_pad=False\n",
    "data.val_dl.pre_pad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Verify dataset transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "denorm = data.trn_ds.denorm\n",
    "x,y = next(iter(data.aug_dl))\n",
    "x = denorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T16:50:49.637353Z",
     "start_time": "2018-08-22T16:50:49.610149Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def label_text(pred):\n",
    "    ints = to_np(pred).astype(int)\n",
    "    ints = np.trim_zeros(ints)   # remove padding (0)\n",
    "    return ''.join([itos[i] for i in ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:13:27.456730Z",
     "start_time": "2018-11-07T23:13:27.405634Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None, title=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "\n",
    "    # Attention\n",
    "#     ax.set_yticks([16, 48, 80, 112], minor=True)\n",
    "#     ax.set_xticks([16, 48, 80, 112], minor=True)\n",
    "#     ax.set_yticks([32, 64, 96, 128], minor=False)\n",
    "#     ax.set_xticks([32, 64, 96, 128], minor=False)\n",
    "#     ax.yaxis.grid(True, which='both')\n",
    "#     ax.xaxis.grid(True, which='both')\n",
    "\n",
    "    if title: ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:13:37.366464Z",
     "start_time": "2018-11-07T23:13:34.925535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,4, figsize=(10, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    t = label_text(y[i])\n",
    "    ax=show_img(x[i], ax=ax, title=t)\n",
    "    \n",
    "plt.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:36.764775Z",
     "start_time": "2018-11-07T23:22:36.711260Z"
    }
   },
   "outputs": [],
   "source": [
    "# categorical cross entropy loss\n",
    "# list of probabilities for each word in vocab; target is correct word\n",
    "\n",
    "def seq2seq_loss(input, target):\n",
    "    target = target.permute(1,0).contiguous()\n",
    "    tsl = target.size(0)\n",
    "    sl,bs,nc = input.size()\n",
    "    \n",
    "    if sl>tsl: target = F.pad(target, (0,0,0,sl-tsl))\n",
    "    if tsl>sl: target = target[:sl]\n",
    "        \n",
    "    targ = target.view(-1)\n",
    "    pred = input.view(-1, nc)\n",
    "\n",
    "    # combination of LogSoftmax and NLLLoss\n",
    "    return F.cross_entropy(pred, targ.long(), reduction='sum')/bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:38.552423Z",
     "start_time": "2018-11-07T23:22:38.494291Z"
    }
   },
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "# pulled from Sean Nareen's deepspeech decoder module\n",
    "# https://github.com/SeanNaren/deepspeech.pytorch/blob/master/decoder.py\n",
    "\n",
    "def cer(t, p):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "    Arguments:\n",
    "        t (string): target space-separated sentence\n",
    "        p (string): prediction space-separated sentence\n",
    "    \"\"\"\n",
    "    t, p, = t.replace(' ', ''), p.replace(' ', '')\n",
    "    return Lev.distance(t, p)/len(t)\n",
    "\n",
    "def wer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:39.055316Z",
     "start_time": "2018-11-07T23:22:38.997429Z"
    }
   },
   "outputs": [],
   "source": [
    "def error_label(pred):\n",
    "    ints = to_np(pred).astype(int)\n",
    "    ints = ints[np.nonzero(ints)]\n",
    "    return ''.join([itos[i] for i in ints])\n",
    "\n",
    "def char_error_rate(preds, targs):\n",
    "    bs,sl = targs.size()      #=> ([bs, sl])\n",
    "    # preds.size()            #=> ([sl, bs, vs])\n",
    "        \n",
    "    res = torch.argmax(preds, dim=2)\n",
    "    error = 0\n",
    "    for i in range(bs):\n",
    "        p = error_label(res[:,i])\n",
    "        t = error_label(targs[i])\n",
    "        error += cer(t,p)\n",
    "    return error/bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:40.861841Z",
     "start_time": "2018-11-07T23:22:40.807356Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, drop=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        slices = {128: -4, 256: -3, 512: -2}\n",
    "        s = slices[embed_size]\n",
    "        \n",
    "        net = f(True)\n",
    "        modules = list(net.children())[:s]    #(8,8,256)\n",
    "        self.base = nn.Sequential(*modules)\n",
    "        \n",
    "        #mlp\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.linear1 = nn.Linear(embed_size, embed_size)\n",
    "        self.linear2 = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        bs,nf,h,w = inp.size()\n",
    "        \n",
    "        features = self.base(inp)\n",
    "        features = features.flatten(2, 3).permute(2, 0, 1)\n",
    "        \n",
    "        # initial hidden is an average of features fed through an MLP\n",
    "        h = self.drop(features)\n",
    "        h = self.linear2(F.relu(self.linear1(h))).mean(0)\n",
    "                        \n",
    "        return features, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:41.168402Z",
     "start_time": "2018-11-07T23:22:41.111492Z"
    }
   },
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_1 = nn.Linear(embed_size, embed_size)\n",
    "        self.attn_2 = nn.Linear(embed_size, embed_size)\n",
    "        self.drop   = nn.Dropout(drop)\n",
    "        self.attn_v = rand_p(embed_size)\n",
    "        \n",
    "    def forward(self, hidden, feats):\n",
    "        # this is called once for each timestep\n",
    "        w1e = self.attn_1(feats)                    # ([64, 30, 256])\n",
    "        w2h = self.attn_2(hidden)                   # ([30, 256])\n",
    "        u = torch.tanh(w1e + w2h)                   # ([64, 30, 256])\n",
    "        u = self.drop(u)\n",
    "        a = torch.softmax(u @ self.attn_v, 0)       # ([64, 30]) - attention mask\n",
    "        context = (a.unsqueeze(2) * feats).sum(0)   # ([30, 256]) - weighted sum of features w/ attention\n",
    "        \n",
    "        return context, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:41.487213Z",
     "start_time": "2018-11-07T23:22:41.430919Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, drop=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(embed_size, embed_size, num_layers)\n",
    "        if self.num_layers > 1: self.rnn.dropout = drop\n",
    "\n",
    "    def forward(self, hidden, context):\n",
    "        _, h = self.rnn(context.unsqueeze(0), hidden.expand(self.num_layers, -1, -1).contiguous())\n",
    "        \n",
    "        return h[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:41.919636Z",
     "start_time": "2018-11-07T23:22:41.863518Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepOutputLayer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, drop=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w0 = nn.Linear(embed_size, embed_size)\n",
    "        self.w1 = nn.Linear(embed_size, embed_size)\n",
    "        self.w2 = nn.Linear(embed_size, embed_size)\n",
    "        self.w3 = nn.Linear(embed_size*3, vocab_size)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, prev, hidden, context):\n",
    "        # this is called once for each timestep\n",
    "        #(30,256)\n",
    "        a = self.w0(prev)\n",
    "        b = self.w1(hidden)\n",
    "        c = self.w2(context)\n",
    "        out = torch.cat([a,b,c], 1)\n",
    "        \n",
    "        return self.w3(self.drop(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:22:42.375943Z",
     "start_time": "2018-11-07T23:22:42.315906Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, seq_len, n_layers=1, drop=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = Encoder(hidden_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.decoder = RNNDecoder(hidden_size, n_layers)\n",
    "        \n",
    "        self.embed   = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.drop    = nn.Dropout(drop)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        self.output  = DeepOutputLayer(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, ret_attn=False):\n",
    "        bs = x.size(0)\n",
    "        feats, hidden = self.encoder(x)\n",
    "        \n",
    "        res,attns = [],[]\n",
    "        dec_inp = torch.ones(bs, device=device).long()\n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            prev = self.drop(self.embed(dec_inp))\n",
    "            \n",
    "            context, a = self.attention(hidden, feats)\n",
    "            hidden = self.decoder(hidden, context)\n",
    "            out = self.output(prev, hidden, context)\n",
    "            \n",
    "            res.append(out)\n",
    "            attns.append(a)\n",
    "            dec_inp = out.data.max(1)[1]\n",
    "            \n",
    "        res = torch.stack(res)\n",
    "        if ret_attn: res = res,torch.stack(attns)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T23:41:54.767846Z",
     "start_time": "2018-11-07T23:41:54.097610Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "rnn = Net(256, len(itos), 20, 1)\n",
    "learn = RNN_Learner(data, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.clip = 0.25\n",
    "learn.crit = seq2seq_loss\n",
    "learn.metrics = [char_error_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:02:24.346499Z",
     "start_time": "2018-11-07T23:50:29.163366Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "learn.fit(lr, 5, cycle_len=1, use_clr=(10,4))\n",
    "# 1 epoch - 72.759241  24.869639  0.46297\n",
    "# 6 epoch - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.val_dl))\n",
    "learn.model.eval()\n",
    "preds,attns = learn.model(x, ret_attn=True)   #attns ([sl, 256, bs])\n",
    "res = torch.argmax(preds, dim=-1)\n",
    "\n",
    "seq2seq_loss(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_text(pred):\n",
    "    ints = to_np(pred).astype(int)\n",
    "    ints = np.trim_zeros(ints)   # remove padding (0)\n",
    "    return ''.join([itos[i] for i in ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(label_text(res[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None, title=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    if title: ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imgs = data.trn_ds.denorm(x)\n",
    "\n",
    "fig, axes = plt.subplots(5,2, figsize=(20, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    t = label_text(y[i])     #=>  char indices\n",
    "    p = label_text(res[:,i])\n",
    "    ax=show_img(imgs[i], ax=ax, title=p)#t+' '+p)\n",
    "    \n",
    "plt.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx  = 0\n",
    "img  = imgs[idx]\n",
    "pred = res[:,idx]\n",
    "attn = to_np(attns[:,:,idx])  # first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_attn_to_img(att):\n",
    "    num = sz // 16\n",
    "    a = att.reshape(num,num)\n",
    "    return np.repeat(np.repeat(a,num, axis=0), num, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 3, figsize=(20, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    a = scale_attn_to_img(attn[i])\n",
    "    ax.imshow(img, alpha=None)\n",
    "    ax.imshow(a, cmap='Blues', interpolation='nearest', alpha=0.3)\n",
    "    ax.set_title(itos[pred[i].item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
