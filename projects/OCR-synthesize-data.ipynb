{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.vision import *\n",
    "# import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = Path('data/IAM_handwriting')\n",
    "TMP_PATH = PATH/'tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def standardize_imgs(imgs, baseheight):\n",
    "    resized_imgs = []\n",
    "    for img in imgs:\n",
    "        hpercent = (baseheight / float(img.size[1]))\n",
    "        wsize = int((float(img.size[0]) * float(hpercent)))\n",
    "        img = img.resize((wsize, baseheight), PIL.Image.ANTIALIAS)\n",
    "        resized_imgs.append(img)\n",
    "    return resized_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def resize_max(im, size=1000):\n",
    "    \"Resize an image so that the largest dimension is of specified size\"\n",
    "    r,c = im.size\n",
    "    ratio = size/max(r,c)\n",
    "    return im.resize((int(r*ratio), int(c*ratio)), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def square_max(im, top_left=False, size=None):\n",
    "    '''\n",
    "    Add whitespace to square an image by its largest dimension or specified size.\n",
    "    Args:\n",
    "        top_left: image is aligned with the top_left corner\n",
    "        size: size of final squared image.  If left blank size = largest dimension\n",
    "    '''\n",
    "    \n",
    "    r,c = im.size\n",
    "    if size is not None and size > max(r,c):\n",
    "        sz = size\n",
    "    else:\n",
    "        sz = max(r,c)\n",
    "        \n",
    "    new_im = Image.new('RGB', (sz, sz), color=(255,255,255))  # new white image\n",
    "    \n",
    "    # box logic\n",
    "    if top_left:\n",
    "        box = (0,0)\n",
    "    else:\n",
    "        if sz == r:\n",
    "            box = (0,random.randint(0,sz-c)) \n",
    "        elif sz == c:\n",
    "            box = (random.randint(0,sz-r),0)\n",
    "        else:\n",
    "            box = (random.randint(0,sz-r),random.randint(0,sz-c))\n",
    "            \n",
    "    new_im.paste(im, box=box)\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def resize_dir(fn, src, targ=None):\n",
    "    if targ is None: targ = src\n",
    "    dirs = os.listdir(src)\n",
    "    for item in tqdm(dirs):\n",
    "        if os.path.isdir(src/item): continue     # skip if src dir\n",
    "#         if os.path.isfile(targ/item): continue   # skip if file exists in targ dir\n",
    "        im = Image.open(src/item)\n",
    "        rsz = fn(im)\n",
    "        rsz.save(targ/item)\n",
    "        rsz.close()\n",
    "        im.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def resize_to_square(src_dir, targ_dir, size):\n",
    "    \"Resize and square all images in src_dir and save in targ_dir\"\n",
    "    resize_dir(partial(resize_max, size=size), src_dir, targ_dir)\n",
    "    resize_dir(square_max, targ_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_sample(df, path, row=2, col=2):\n",
    "    fig, axes = plt.subplots(row,col, figsize=(20, 10))\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        row = df.iloc[i]\n",
    "        im = Image.open(path/row.filename)\n",
    "        ax.imshow(im)\n",
    "        label = row.labels\n",
    "    #     label = '\\n'.join(textwrap.wrap(row.labels, 70))\n",
    "        ax.set_title(label)\n",
    "\n",
    "    plt.tight_layout(pad=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Synthesize Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## From DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{PATH}/ascii/lines.txt', names=['filename','result','value'], escapechar='\\\\',\n",
    "                          delim_whitespace=True, skiprows=23, header=None, usecols=[0,1,8])\n",
    "df['text'] = df.apply(lambda row: row.value.replace('|', ' '), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanup(x):\n",
    "    return x.replace(' .', '.').replace(' ,', ',').replace(\" 'll\", \"'ll\").replace(\n",
    "                     ' !', '!').replace(' :', ':').replace(' ;', ';').replace(\n",
    "                     ' ?', '?').replace(\" 'd\", \"'d\").replace(\" 're\", \"'re\").replace(\n",
    "                     \" 's\", \"'s\").replace(\" 't\", \"'t\")\n",
    "\n",
    "df['text'] = df.apply(lambda row: cleanup(row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate character lengths\n",
    "lgts = df.text.apply(len)  \n",
    "df['text_len'] = lgts.astype('int32')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df.result != 'err']\n",
    "df = df.loc[df['text_len'] > 20]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.text_len.min(), df.text_len.median(), df.text_len.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## remove lines in paragraph validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get paragraph val idxs\n",
    "PG_CSV = PATH/'paragraph_chars.csv'\n",
    "pg_csv = pd.read_csv(PG_CSV)\n",
    "val_idxs = np.array(pg_csv.sample(frac=0.15, random_state=42).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names = pg_csv.filename[val_idxs].values\n",
    "names = [name[:-4] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fnames = df.filename.values\n",
    "fnames = [name[:-3] for name in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "st = set(names)\n",
    "vals = [i for i, e in enumerate(fnames) if e in st]\n",
    "len(vals), len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove vals from df\n",
    "df.drop(df.index[vals], inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_img(imgs, targ_path, num_lines, max_size, pad=30):\n",
    "    w = 1\n",
    "    h = num_lines\n",
    "        \n",
    "    widths, heights = zip(*(i.size for i in imgs))\n",
    "    \n",
    "    median_height = int(np.median(heights))\n",
    "    stzd_imgs = standardize_imgs(imgs, median_height)\n",
    "    lines = [stzd_imgs[i:i + w] for i in range(0, len(stzd_imgs), w)]\n",
    "    \n",
    "    total_width = max([np.sum([word.size[0] for word in line]) for line in lines]) + (pad*(w+1))   \n",
    "    total_height = (median_height * h) + (pad*(h+1)) #sum(heights)\n",
    "\n",
    "    new_im = Image.new('RGB', (total_width, total_height), color=(255,255,255))\n",
    "\n",
    "    y_offset = pad\n",
    "    x_offset = pad\n",
    "    \n",
    "    for line in lines:\n",
    "        x_offset = pad\n",
    "        for word in line:\n",
    "            new_im.paste(word, (x_offset,y_offset))\n",
    "            x_offset += word.size[0] + pad\n",
    "        y_offset += median_height + pad\n",
    "    \n",
    "    if max_size: \n",
    "        resize_max(new_im, max_size).save(targ_path)\n",
    "    else:\n",
    "        new_im.save(targ_path/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# number of words/image\n",
    "def create_synth_data(num, num_lines, src_path, targ_path, max_size=1000, offset=0):\n",
    "    d={}\n",
    "    for i in tqdm(range(num)):\n",
    "        samp = df.sample(num_lines)\n",
    "        files = list(map(lambda x: x+'.png', samp.filename.values))\n",
    "        imgs  = [Image.open(src_path/f) for f in files]\n",
    "        \n",
    "        # split into rows with \\n\n",
    "        label = '\\n'.join([' '.join(row) for row in np.array_split(samp.text.values, num_lines)])\n",
    "#         label = ' '.join(samp.text.values)\n",
    "\n",
    "        fname = str(num_lines)+'_'+'{:04d}'.format(i+offset)+'.png'\n",
    "        create_img(imgs, targ_path/fname, num_lines, max_size)\n",
    "        [f.close() for f in imgs]\n",
    "        d[fname] = label\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "src_path = PATH/'lines'\n",
    "targ_path = PATH/'cat_lines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#single\n",
    "num_lines = 3\n",
    "d = create_synth_data(10, num_lines, src_path, targ_path)\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synth = pd.DataFrame({'filename': list(d.keys()), 'labels': list(d.values())})\n",
    "synth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_sample(synth, targ_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#multi\n",
    "for i in tqdm(range(3,15)):   #tqdm([11,12,13,14]):   #tqdm([7,8,9,10]):\n",
    "    num_lines = i\n",
    "    d = create_synth_data(1000, num_lines, src_path, targ_path)\n",
    "    synth = pd.DataFrame({'filename': list(d.keys()), 'labels': list(d.values())})\n",
    "    \n",
    "    joined_labels = list(synth.labels) #list(map(lambda x: ' '.join(x), labels))\n",
    "\n",
    "    stoi = collections.defaultdict(lambda: 82, {v:k for k,v in enumerate(itos)})\n",
    "    ids = np.array([np.array([stoi[letter] for letter in word] + [3]) for word in joined_labels])\n",
    "\n",
    "    # convert to strings (as labels)\n",
    "    str_ids = np.array([' '.join(str(l) for l in w) for w in ids]).reshape(-1,1)\n",
    "    synth['char_ids'] = str_ids\n",
    "    \n",
    "    CSV = str(targ_path)+'_'+str(num_lines)+'.csv'\n",
    "    synth.to_csv(CSV, columns=['filename', 'char_ids'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_sample(a, targ_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Synthesize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### From pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{PATH}/ascii/words.txt', names=['filename','result','value'], escapechar='\\\\', delim_whitespace=True, skiprows=23, header=None, usecols=[0,1,8])\n",
    "df.rename(columns={'value': 'word'}, inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove errors\n",
    "df = df[df.result != 'err']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate character lengths\n",
    "lgts = df.word.apply(len)  \n",
    "df['char_len'] = lgts.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lots of errors from pd.read_csv\n",
    "# only keep rows w/ word length < 20\n",
    "df = df[df.char_len < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['char_len'] > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### via manually created DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maxTextLen = 32\n",
    "samples = []\n",
    "chars = set()\n",
    "\n",
    "f=open(f'{PATH}/ascii/words.txt')\n",
    "for line in f:\n",
    "    # ignore comment line\n",
    "    if not line or line[0]=='#':\n",
    "        continue\n",
    "\n",
    "    lineSplit = line.strip().split(' ')\n",
    "    assert len(lineSplit) >= 9\n",
    "\n",
    "    fileName = lineSplit[0]\n",
    "\n",
    "    # GT text are columns starting at 9\n",
    "    gtText = ''.join(lineSplit[8:])[:maxTextLen]\n",
    "    char_len = len(gtText)\n",
    "    chars = chars.union(set(list(gtText)))\n",
    "\n",
    "    # put sample into list\n",
    "    samples.append([fileName, gtText, char_len])\n",
    "    \n",
    "samples = np.stack(samples)\n",
    "df = pd.DataFrame(samples, columns=['filename', 'word', 'char_len'], )\n",
    "del samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['char_len'] = df.char_len.astype('int32')\n",
    "df = df.loc[df['char_len'] > 3]\n",
    "df = df.loc[df['char_len'] < 20]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## num words / line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_img(src_path, targ_path, files, fname, sz=None, pad=30):\n",
    "    if sz==None: sz=(1, len(files))  #(w,h)\n",
    "    w = sz[1]\n",
    "    h = sz[0]\n",
    "        \n",
    "    imgs = [ PIL.Image.open(src_path/f) for f in files ]\n",
    "    widths, heights = zip(*(i.size for i in imgs))\n",
    "    \n",
    "    median_height = int(np.median(heights))\n",
    "    stzd_imgs = standardize_imgs(imgs, median_height)\n",
    "    lines = [stzd_imgs[i:i + w] for i in range(0, len(stzd_imgs), w)]\n",
    "    \n",
    "    total_width = max([np.sum([word.size[0] for word in line]) for line in lines]) + (pad*(w+1))   \n",
    "    total_height = (median_height * h) + (pad*(h+1)) #sum(heights)\n",
    "\n",
    "    new_im = Image.new('RGB', (total_width, total_height), color=(255,255,255))\n",
    "\n",
    "    y_offset = pad\n",
    "    x_offset = pad\n",
    "    \n",
    "    for line in lines:\n",
    "        x_offset = pad\n",
    "        for word in line:\n",
    "            new_im.paste(word, (x_offset,y_offset))\n",
    "            x_offset += word.size[0] + pad\n",
    "        y_offset += median_height + pad\n",
    "        \n",
    "    new_im.save(targ_path/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# number of words/image\n",
    "def create_synth_data(src_path, targ_path, num, sz, offset=0, randomize=False, pre=''):\n",
    "    d={}\n",
    "    for i in tqdm(range(num)):\n",
    "        if randomize:\n",
    "            r = random.randint(1,sz[0])\n",
    "            c = random.randint(1,sz[1])\n",
    "        else:\n",
    "            r,c = sz\n",
    "            \n",
    "        num_samp = np.product((r,c))\n",
    "        res = df.sample(num_samp)\n",
    "        files = list(map(lambda x: x+'.png', res.filename.values))\n",
    "        \n",
    "        # split into rows with \\n\n",
    "        label = '\\n'.join([' '.join(row) for row in np.array_split(res.word.values, r)])\n",
    "#         label = ' '.join(res.word.values)\n",
    "        \n",
    "        fname = pre+'{:04d}'.format(i+offset)+'.png'\n",
    "        create_img(src_path, targ_path, files, fname, (r,c))\n",
    "        d[fname] = label\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "src_path = PATH/'words'\n",
    "synth_path = PATH/'small_synth_words'\n",
    "!rm -rf {synth_path}\n",
    "\n",
    "os.makedirs(synth_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = create_synth_data(src_path, synth_path, 20000, (4,3), randomize=True, pre='sm_')\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synth = pd.DataFrame({'filename': list(d.keys()), 'labels': list(d.values())})\n",
    "synth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# resize_dir(partial(resize_max, size=512), src_path)\n",
    "resize_dir(partial(square_max, size=1000), synth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_sample(synth, synth_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## size of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "synth_path = PATH/'large_synth_words_test'\n",
    "!rm -rf {synth_path}\n",
    "\n",
    "os.makedirs(synth_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_img(sz, fname, pad=30, median_height=None):\n",
    "    # TODO: randomize padding\n",
    "    new_im = Image.new('RGB', (sz,sz), color=(255,255,255))\n",
    "    \n",
    "    res   = df.sample(50)\n",
    "    files = list(map(lambda x: x+'.png', res.filename.values))\n",
    "    lbls  = res.word.values.tolist()\n",
    "    imgs  = [ PIL.Image.open(PATH/'words'/f) for f in files ]\n",
    "    \n",
    "    if median_height is None:\n",
    "        w, h  = zip(*(i.size for i in imgs))\n",
    "        # standardize heights and sort longest to shortest words\n",
    "        median_height = int(np.median(h))        # TODO: randomize this between mean/std\n",
    "        \n",
    "    stzd_imgs = standardize_imgs(imgs, median_height)\n",
    "    \n",
    "    #loop through standardized images and find the next image which satisfies the condition\n",
    "    labels = []\n",
    "    y_offset = pad\n",
    "    while y_offset+median_height+pad < sz:        \n",
    "        x_offset = pad\n",
    "        \n",
    "        gen = (i for i,x in enumerate(stzd_imgs) if x.size[0]+x_offset+pad <= sz)\n",
    "        lines = []\n",
    "        for idx in gen:\n",
    "            word = stzd_imgs.pop(idx)            \n",
    "            lines.append(lbls.pop(idx))\n",
    "            new_im.paste(word, (x_offset,y_offset))\n",
    "            x_offset += word.size[0] + pad\n",
    "        y_offset += median_height+pad\n",
    "        labels.append(' '.join(lines))\n",
    "\n",
    "    new_im.save(synth_path/fname)    \n",
    "    return '\\n'.join(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# size of image\n",
    "def create_synth_data(qty, sz, fname_offset=0):\n",
    "    d={}\n",
    "    for i in tqdm(range(qty)):\n",
    "        fname = '{:04d}'.format(i+fname_offset)+'.png'\n",
    "        p = random.randint(10,20)\n",
    "        h = random.randint(25,35)\n",
    "        d[fname] = create_img(sz, fname, pad=p, median_height=h)\n",
    "    return d\n",
    "\n",
    "d = create_synth_data(20000, 512)\n",
    "len(d)\n",
    "# ~25min to create 5000 1000x1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synth = pd.DataFrame({'filename': list(d.keys()), 'labels': list(d.values())})\n",
    "synth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_sample(synth, synth_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Numericalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# same as used in single word / multi-word\n",
    "itos = pickle.load(open(TMP_PATH/'char_itos.pkl', 'rb'))\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joined_labels = list(synth.labels) #list(map(lambda x: ' '.join(x), labels))\n",
    "\n",
    "stoi = collections.defaultdict(lambda: 82, {v:k for k,v in enumerate(itos)})\n",
    "ids = np.array([np.array([stoi[letter] for letter in word] + [3]) for word in joined_labels])\n",
    "\n",
    "# convert to strings (as labels)\n",
    "str_ids = np.array([' '.join(str(l) for l in w) for w in ids]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "synth['char_ids'] = str_ids\n",
    "synth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = str(synth_path) + '.csv'\n",
    "synth.to_csv(CSV, columns=['filename', 'char_ids'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # multi-line\n",
    "# CSV = str(targ_path)+'_'+str(num_lines)+'.csv'\n",
    "# synth.to_csv(CSV, columns=['filename', 'char_ids'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Add to existing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/'large_synth_words_10000.csv'\n",
    "csv = pd.read_csv(CSV)\n",
    "len(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CSV = PATH/'synth_words_50000.csv'\n",
    "CSV = PATH/'large_synth_words_50000.csv'\n",
    "\n",
    "new = pd.concat([csv, synth[['filename', 'char_ids']]], ignore_index=True)\n",
    "new.to_csv(CSV, columns=['filename', 'char_ids'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Concatenate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv(PATH/'cat_lines_11.csv')\n",
    "b = pd.read_csv(PATH/'cat_lines_12.csv')\n",
    "c = pd.read_csv(PATH/'cat_lines_13.csv')\n",
    "d = pd.read_csv(PATH/'cat_lines_14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new = pd.concat([a,b,c,d], ignore_index=True)\n",
    "len(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new.to_csv(PATH/'cat_lines_11-14.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_string(row):\n",
    "    return ''.join([itos[int(c)] for c in row.split(' ')])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(20, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    row = a.iloc[i]\n",
    "    im = Image.open(targ_path/row.filename)\n",
    "    ax.imshow(im)\n",
    "    label = to_string(row.char_ids)\n",
    "#     label = '\\n'.join(textwrap.wrap(row.labels, 70))\n",
    "    ax.set_title(label)\n",
    "\n",
    "plt.tight_layout(pad=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itos = pickle.load(open(TMP_PATH/'synth_word_itos.pkl', 'rb'))\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joined_labels = list(synth.labels)\n",
    "\n",
    "stoi = collections.defaultdict(lambda: 2, {v:k for k,v in enumerate(itos)})\n",
    "ids = np.array([np.array([stoi[word] for word in line.split(' ')]+[3]) for line in joined_labels])\n",
    "\n",
    "# convert to strings (as labels)\n",
    "str_ids = np.array([' '.join(str(l) for l in w) for w in ids]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "synth['word_ids'] = str_ids\n",
    "synth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Add to existing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/'large_synth_word_ids_10000.csv'\n",
    "csv = pd.read_csv(CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CSV = PATH/'synth_words_50000.csv'\n",
    "CSV = PATH/'large_synth_word_ids_50000.csv'\n",
    "\n",
    "new = pd.concat([csv, synth[['filename', 'word_ids']]], ignore_index=True)\n",
    "new.to_csv(CSV, columns=['filename', 'word_ids'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Modify csv/itos to match previous versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itos_old = pickle.load(open(TMP_PATH/'synth_word_itos.pkl', 'rb'))\n",
    "\n",
    "# same as used in single word / multi-word\n",
    "itos = pickle.load(open(TMP_PATH/'char_itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = [''.join([itos_old[int(c)] for c in line.split(' ')]) for line in csv.char_ids]\n",
    "csv['words'] = res\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joined_labels = list(csv.words) #list(map(lambda x: ' '.join(x), labels))\n",
    "\n",
    "stoi = collections.defaultdict(lambda: 2, {v:k for k,v in enumerate(itos)})\n",
    "ids = np.array([np.array([stoi[letter] for letter in word]+[3]) for word in joined_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert to strings (as labels)\n",
    "str_ids = np.array([' '.join(str(l) for l in w) for w in ids]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv['char_ids'] = str_ids\n",
    "csv = csv[['filename', 'char_ids']]\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def label_text(pred):\n",
    "#     ints = to_np(pred).astype(int)\n",
    "#     ints = np.trim_zeros(ints)   # remove padding (0)\n",
    "    return ''.join([itos[int(i)] for i in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5,1, figsize=(10, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    row = synth.iloc[i]\n",
    "    im = Image.open(synth_path/row.filename)\n",
    "    ax.imshow(im)\n",
    "    ax.set_title(label_text(row.char_ids.split(' ')))\n",
    "    \n",
    "plt.tight_layout(pad=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "targ_path = PATH/'test_resize'\n",
    "os.makedirs(targ_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "src_path = PATH/'cat_lines'\n",
    "# targ_path = PATH/'resized_cat_lines'\n",
    "# os.makedirs(targ_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "resize_to_square(src_path, targ_path, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# resize_dir(partial(resize_max, size=512), src_path)\n",
    "resize_dir(partial(square_max, size=1000), src_path, targ_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# v1 Batch Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "\n",
    "path = untar_data(URLs.PETS)\n",
    "path_hr = path/'images'\n",
    "path_lr = path/'small-96'\n",
    "path_mr = path/'small-256'\n",
    "\n",
    "il = ImageList.from_folder(path_hr)\n",
    "\n",
    "def resize_one(fn, i, path, size):\n",
    "    dest = path/fn.relative_to(path_hr)\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    img = PIL.Image.open(fn)\n",
    "    targ_sz = resize_to(img, size, use_min=True)\n",
    "    img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')\n",
    "    img.save(dest, quality=75)\n",
    "\n",
    "# create smaller image sets the first time this nb is run\n",
    "sets = [(path_lr, 96), (path_mr, 256)]\n",
    "for p,size in sets:\n",
    "    if not p.exists(): \n",
    "        print(f\"resizing to {size} into {p}\")\n",
    "        parallel(partial(resize_one, path=p, size=size), il.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert datasets to new itos.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itos = pickle.load(open(TMP_PATH/'char_itos.pkl', 'rb'))\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "voc = pickle.load(open(TMP_PATH/'itos.pkl', 'rb'))\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert_char_itos(old_itos, new_itos, path, old_fname, new_fname):\n",
    "    old_csv = pd.read_csv(path/old_fname)\n",
    "    # convert to text and remove _eos_ token\n",
    "    res = [''.join([old_itos[int(c)] for c in line.split(' ')[:-1]]) for line in old_csv.char_ids]    \n",
    "    # xxunk: 3, xxeos: 2\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(new_itos)})\n",
    "    ids = np.array([np.array([stoi[letter] for letter in word] + [2]) for word in list(res)])\n",
    "\n",
    "    # convert to strings (as labels)\n",
    "    str_ids = np.array([' '.join(str(l) for l in w) for w in ids]).reshape(-1,1)\n",
    "    old_csv['char_ids'] = str_ids\n",
    "    old_csv.to_csv(path/new_fname, columns=['filename', 'char_ids'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# old,new = 'small_synth_words.csv','sm_synth.csv'\n",
    "# old,new = 'multi_synth_words.csv','3x2_synth.csv'\n",
    "# old,new = 'paragraphs.csv','pg.csv'\n",
    "# old,new = 'mix_words_dl.csv','full_mix.csv'\n",
    "# old,new = 'mix_words.csv','mix.csv'\n",
    "old,new = 'downloaded_images.csv', 'dl.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "convert_char_itos(itos, voc, PATH, old, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(PATH/new)\n",
    "\n",
    "res = [''.join([voc[int(c)] for c in line.split(' ')[:-1]]) for line in csv.char_ids]\n",
    "csv['text'] = res\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
