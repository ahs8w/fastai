{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks.tracker import *\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = Path('data/IAM_handwriting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Metrics, Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tensor2im(x):\n",
    "    x = x.detach().numpy() * 255\n",
    "    x = np.uint8(x)[0]\n",
    "    return PIL.Image.fromarray(x, mode='L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred,targ = self.loss_prep(pred, target)\n",
    "        pred = F.log_softmax(pred, dim=-1)  # need this for KLDivLoss\n",
    "        true_dist = pred.data.clone()\n",
    "        true_dist.fill_(self.smoothing / pred.size(1))                  # fill with 0.0012\n",
    "        true_dist.scatter_(1, targ.data.unsqueeze(1), self.confidence)  # [0.0012, 0.0012, 0.90, 0.0012]\n",
    "        return F.kl_div(pred, true_dist, reduction='sum')/bs\n",
    "    \n",
    "    def loss_prep(self, pred, target):\n",
    "        \"equalize input/target sl; combine bs/sl dimensions\"\n",
    "        bs,tsl = target.shape\n",
    "        _ ,sl,vocab = pred.shape\n",
    "\n",
    "        # F.pad( front,back for dimensions: 1,0,2 )\n",
    "        if sl>tsl: target = F.pad(target, (0,sl-tsl))\n",
    "\n",
    "        # this should only be used when testing for small seq_lens\n",
    "        # if tsl>sl: target = target[:,:sl]\n",
    "\n",
    "        if tsl>sl: pred = F.pad(pred, (0,0,0,tsl-sl))\n",
    "        # not ideal => adds 96 logits all 0s...\n",
    "\n",
    "        targ = target.contiguous().view(-1).long()\n",
    "        pred = pred.contiguous().view(-1, vocab)\n",
    "        return pred, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cer(preds, targs):\n",
    "    bs = targs.size(0)\n",
    "    res = torch.argmax(preds, dim=-1)\n",
    "    error = 0\n",
    "    for i in range(bs):\n",
    "        p = char_label_text(res[i])   #.replace(' ', '')\n",
    "        t = char_label_text(targs[i]) #.replace(' ', '')\n",
    "        error += Lev.distance(t, p)/len(t)\n",
    "    return error, bs\n",
    "\n",
    "def char_label_text(pred, sep=''):\n",
    "    ints = to_np(pred).astype(int)\n",
    "    nonzero = ints[np.nonzero(ints)] #[:-1]  #remove eos token\n",
    "    return sep.join([itos[i] for i in nonzero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "class CER(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = 'cer'\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.errors, self.total = 0, 0\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        error,size = cer(last_output, last_target)\n",
    "        self.errors += error\n",
    "        self.total += size\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        return add_metrics(last_metrics, self.errors/self.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rshift(tgt, bos_token=1):\n",
    "    \"Shift y to the right by prepending token\"\n",
    "    bos = torch.zeros((tgt.size(0),1), device=device).type_as(tgt) + bos_token\n",
    "    return torch.cat((bos, tgt[:,:-1]), dim=-1)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    return torch.tril(torch.ones((1,size,size), device=device).byte())\n",
    "    #return torch.tril(torch.ones((1,1,size,size), device=device).byte())  # complex batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TeacherForce(LearnerCallback):\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        s = rshift(last_target).long()\n",
    "        mask = subsequent_mask(s.size(-1))\n",
    "        return {'last_input':(last_input, s, mask), 'last_target':last_target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## sm synth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'edited_sm_synth.csv' #'small_synth_words.csv'\n",
    "CSV = PATH/fname\n",
    "FOLDER = 'edited_sm_synth'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sz,bs = 128,100\n",
    "sz,bs = 256,100\n",
    "\n",
    "num_lines,seq_len = 4,50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'edited_line2.csv'\n",
    "CSV = PATH/fname\n",
    "FOLDER = 'square_lines'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sz,bs = 256,100\n",
    "# sz,bs = 256,100\n",
    "\n",
    "seq_len = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Concat Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/f'edited_cat_lines.csv'\n",
    "FOLDER = 'edited_cat_lines'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nums = '3-6'   #'7-10'  #'11-14'\n",
    "CSV = PATH/f'cat_lines_{nums}.csv'\n",
    "\n",
    "FOLDER = 'resized_cat_lines'\n",
    "\n",
    "csv = pd.read_csv(CSV)\n",
    "test = pd.read_csv(PATH/'test_pg.csv')\n",
    "\n",
    "len(csv), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lengths = np.array([len(i.split(' ')) for i in csv.char_ids.values])\n",
    "lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sz,bs = 512,20   #1000,5  #800,8   #512,20\n",
    "seq_len = 600 #600 #450  #300\n",
    "stats = (np.array([0.941, 0.941, 0.941], dtype=np.float32), np.array([0.128, 0.128, 0.128], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Concat All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv(PATH/'cat_lines_3.csv').sample(1000)\n",
    "b = pd.read_csv(PATH/'cat_lines_4.csv').sample(1000)\n",
    "c = pd.read_csv(PATH/'cat_lines_5.csv').sample(1000)\n",
    "d = pd.read_csv(PATH/'cat_lines_6.csv').sample(1000)\n",
    "e = pd.read_csv(PATH/'cat_lines_7.csv').sample(1000)\n",
    "f = pd.read_csv(PATH/'cat_lines_8.csv').sample(1000)\n",
    "g = pd.read_csv(PATH/'cat_lines_9.csv').sample(1000)\n",
    "h = pd.read_csv(PATH/'cat_lines_10.csv').sample(1000)\n",
    "i = pd.read_csv(PATH/'cat_lines_11.csv').sample(1000)\n",
    "j = pd.read_csv(PATH/'cat_lines_12.csv').sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k = pd.read_csv(PATH/'paragraph_chars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_idxs = np.array(k.sample(frac=0.15, random_state=42).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn = k[~k.index.isin(val_idxs)]\n",
    "test = k[k.index.isin(val_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new = pd.concat([a,b,c,d,e,f,g,h,i,j,trn], ignore_index=True)\n",
    "len(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# new.to_csv(PATH/'cat_lines_pg.csv', index=False)\n",
    "# test.to_csv(PATH/'test_pg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'edited_pg.csv' #'paragraphs.csv'\n",
    "CSV = PATH/fname\n",
    "FOLDER = 'paragraphs'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[:10]\n",
    "sz,bs,seq_len = 256,10,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sz,bs = 1000,5  #1024,5  #1000,5   #~2000x1000 full size\n",
    "# sz,bs = 800,8\n",
    "sz,bs = 512,10\n",
    "\n",
    "seq_len = 700   #~400 chars/paragraph - max: 705\n",
    "# stats = (np.array([0.941, 0.941, 0.941], dtype=np.float32), np.array([0.128, 0.128, 0.128], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Downloaded Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/'downloaded_images.csv'\n",
    "FOLDER = 'downloaded_images'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# csv['filename'] = csv['filename'].apply(lambda x: f\"dl_{x}\")\n",
    "# csv.head()\n",
    "# csv.to_csv(CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sz,bs = 1000,5  #1024,5  #1000,5   #~2000x1000 full size\n",
    "# sz,bs = 800,8\n",
    "sz,bs = 512,41\n",
    "\n",
    "seq_len = 700   #~400 chars/paragraph - max: 705\n",
    "stats = (np.array([0.941, 0.941, 0.941], dtype=np.float32), np.array([0.128, 0.128, 0.128], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'edited_font.csv'\n",
    "CSV = PATH/fname\n",
    "FOLDER = 'fonts_resize'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[df.num_lines<5]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sz,bs = 256,20\n",
    "# sz,bs = 400,10\n",
    "sz,bs = 512,50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'mix.csv'\n",
    "CSV = PATH/fname\n",
    "FOLDER = 'mix'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sz,bs = 512,10\n",
    "# sz,bs = 800,5\n",
    "\n",
    "seq_len = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Combo/Cat Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 6 and fewer\n",
    "fname = 'combo_cat6lines.csv'\n",
    "sz,bs = 512,30\n",
    "seq_len = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 6 and greater of combo/cat lines + all paragraph (2-13 lines)\n",
    "fname = 'combo_cat_pg.csv'\n",
    "sz,bs = 512,10\n",
    "seq_len = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# full mix sorted by num_lines\n",
    "fname = 'combo_cat_pg_dl_sorted.csv'\n",
    "sz,bs = 512,10\n",
    "seq_len = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/fname\n",
    "FOLDER = 'combo_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FOLDER = 'uploads'\n",
    "df = pd.read_csv(PATH/'uploads.csv')\n",
    "len(df)\n",
    "\n",
    "sz,bs = 512,14\n",
    "seq_len = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FOLDER = 'paragraphs'\n",
    "df = pd.read_csv(PATH/'test_pg.csv')\n",
    "len(df)\n",
    "\n",
    "sz,bs = 512,15\n",
    "seq_len = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False, max_zoom=1, max_rotate=0, max_warp=0.1)\n",
    "\n",
    "def force_gray(image): return image.convert('L').convert('RGB')\n",
    "\n",
    "def label_collater(samples:BatchSamples, pad_idx:int=0):\n",
    "    \"Function that collect samples and pads ends of labels.\"\n",
    "    data = to_data(samples)\n",
    "    ims, lbls = zip(*data)\n",
    "    imgs = torch.stack(list(ims))\n",
    "    if len(data) is 1:\n",
    "        labels = torch.zeros(1,1).long()\n",
    "        return imgs, labels    \n",
    "    max_len = max([len(s) for s in lbls])\n",
    "    labels = torch.zeros(len(data), max_len+1).long() + pad_idx  # add 1 to max_len to account for bos token\n",
    "    for i,lbl in enumerate(lbls):\n",
    "        labels[i,:len(lbl)] = torch.from_numpy(lbl)  #padding end    \n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itos = pickle.load(open(PATH/'itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple batches (BS, Seq Len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CharTokenizer(BaseTokenizer):\n",
    "    def tokenizer(self, t:str) -> List[str]: return list(t)\n",
    "            \n",
    "class CharVocab(Vocab):\n",
    "    def __init__(self, itos:Collection[str]):\n",
    "        self.itos = itos\n",
    "        self.stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(self.itos)})\n",
    "\n",
    "    def textify(self, nums:Collection[int], sep=''):\n",
    "        return sep.join([self.itos[i] for i in nums]) if sep is not None else [self.itos[i] for i in nums]\n",
    "\n",
    "class SequenceList(TextList):    \n",
    "    def __init__(self, items:Iterator, vocab:Vocab, **kwargs):\n",
    "        toknizr = Tokenizer(tok_func=CharTokenizer, pre_rules=[], post_rules=[], special_cases=[BOS,EOS,UNK,PAD])\n",
    "        procs = [TokenizeProcessor(tokenizer=toknizr, include_bos=False, include_eos=True),\n",
    "                 NumericalizeProcessor(vocab=vocab)]\n",
    "        super().__init__(items, vocab, sep='', pad_idx=0, processor=procs)\n",
    "    \n",
    "    def analyze_pred(self, pred:Tensor):\n",
    "        return torch.argmax(pred, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER, after_open=force_gray)\n",
    "#         .split_none()\n",
    "        .split_by_rand_pct(valid_pct=0.15, seed=42)\n",
    "        #.label_from_df(label_cls=TextList, sep='', pad_idx=0, vocab=vocab, processor=procs)\n",
    "        .label_from_df(label_cls=SequenceList, vocab=CharVocab(itos))\n",
    "        .transform(tfms, size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        #.transform(tfms, size=sz, resize_method=ResizeMethod.PAD, padding_mode='border')\n",
    "        # maintains aspect ratio but too small for good results => mostly whitespace\n",
    "        .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "        #.normalize()\n",
    "        # this sets x values to an odd range (~.3,-6)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test dataset only!!!\n",
    "\n",
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER, after_open=force_gray)\n",
    "        .split_none()\n",
    "        .label_from_df(label_cls=SequenceList, vocab=CharVocab(itos))\n",
    "        .transform([], size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Complex Batches (BS, Lines, Char Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_1d_array_to_2d_tensor(a, split_idx=4):\n",
    "    \"Requires global num_lines variable to be set...\"\n",
    "    b = np.split(a, np.where(a == split_idx)[0])\n",
    "    maxlen = len(max(b,key=len))\n",
    "    res = torch.zeros((maxlen, num_lines))\n",
    "    for i,arr in enumerate(b):\n",
    "        res[:len(arr),i] = torch.from_numpy(arr)\n",
    "    return res\n",
    "\n",
    "def batch_line_collater(samples:BatchSamples, pad_idx:int=0):\n",
    "    \"Function that collect samples and pads ends of labels.\"\n",
    "    data = to_data(samples)\n",
    "    ims, lbls = zip(*data)\n",
    "    imgs = torch.stack(list(ims))\n",
    "    if len(data) is 1:\n",
    "        labels = torch.zeros(1,1,1).long()\n",
    "        return imgs, labels\n",
    "\n",
    "    res = []\n",
    "    for lbl in lbls:\n",
    "        res.append(split_1d_array_to_2d_tensor(lbl))\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(res, batch_first=True)\n",
    "    return imgs, labels.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CharTokenizer():\n",
    "    def __init__(self, n_cpus:int=None):\n",
    "        self.n_cpus = ifnone(n_cpus, defaults.cpus)\n",
    "\n",
    "    def tokenize(self, t:str): return list(t)+['xxeos']\n",
    "\n",
    "    def _process_all_1(self, texts:Collection[str]) -> List[List[str]]:\n",
    "        \"Process a list of `texts` in one process.\"\n",
    "        return [self.tokenize(str(t)) for t in texts]\n",
    "\n",
    "    def process_all(self, texts:Collection[str]) -> List[List[str]]:\n",
    "        \"Process a list of `texts`.\"\n",
    "        if self.n_cpus <= 1: return self._process_all_1(texts)\n",
    "        with ProcessPoolExecutor(self.n_cpus) as e:\n",
    "            return sum(e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus)), [])\n",
    "\n",
    "class CharVocab(Vocab):\n",
    "    def __init__(self, itos:Collection[str]):\n",
    "        self.itos = itos\n",
    "        self.stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(self.itos)})\n",
    "\n",
    "    def textify(self, nums:Collection[int]):\n",
    "        nums = nums[:-1]  #remove bos/eos tokens\n",
    "        return ''.join([self.itos[i] for i in nums.astype(int)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequenceList(ItemList):\n",
    "    _processor = [partial(TokenizeProcessor, tokenizer=CharTokenizer(), include_bos=False), NumericalizeProcessor]\n",
    "\n",
    "    def __init__(self, items:Iterator, itos:List[str]=None, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.vocab=CharVocab(itos)\n",
    "        self.pad_idx=0\n",
    "        self.copy_new += ['vocab', 'pad_idx']\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return o if self.vocab is None else Text(o, self.vocab.textify(o))\n",
    "\n",
    "    def reconstruct(self, t:Tensor):\n",
    "        o = t.numpy()\n",
    "        o = o[np.nonzero(o)].flatten()\n",
    "        return Text(o, self.vocab.textify(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER)\n",
    "        .split_by_rand_pct(valid_pct=0.15, seed=42)\n",
    "        .label_from_df(label_cls=SequenceList, itos=itos)\n",
    "        .transform(tfms, size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs, device=device, collate_fn=batch_line_collater)\n",
    "        .normalize()\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SequenceList (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class CharTokenizer():\n",
    "#     def __init__(self, n_cpus:int=None):\n",
    "#         self.n_cpus = ifnone(n_cpus, defaults.cpus)\n",
    "\n",
    "#     def tokenize(self, t:str): return list(t)+['xxeos']\n",
    "\n",
    "#     def _process_all_1(self, texts:Collection[str]) -> List[List[str]]:\n",
    "#         \"Process a list of `texts` in one process.\"\n",
    "#         return [self.tokenize(str(t)) for t in texts]\n",
    "\n",
    "#     def process_all(self, texts:Collection[str]) -> List[List[str]]:\n",
    "#         \"Process a list of `texts`.\"\n",
    "#         if self.n_cpus <= 1: return self._process_all_1(texts)\n",
    "#         with ProcessPoolExecutor(self.n_cpus) as e:\n",
    "#             return sum(e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus)), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequenceItem(ItemBase):\n",
    "    def __init__(self,data,vocab): self.data,self.vocab = data,vocab        \n",
    "    def __str__(self): return self.textify(self.data)\n",
    "    def __hash__(self): return hash(str(self))\n",
    "    def textify(self, data): return ''.join([self.vocab[i] for i in data[:-1]])\n",
    "        \n",
    "class ArrayProcessor(PreProcessor):\n",
    "    \"Convert df column (string of ints) into np.array\"\n",
    "    def __init__(self, ds:ItemList=None): None\n",
    "    def process_one(self,item): return np.array(item.split(), dtype=np.int64)\n",
    "    def process(self, ds): super().process(ds)\n",
    "        \n",
    "class ItosProcessor(PreProcessor):\n",
    "    def __init__(self, ds:ItemList=None): self.itos = ds.itos\n",
    "    def process(self, ds:ItemList): ds.itos = self.itos\n",
    "        \n",
    "class SequenceList(ItemList):\n",
    "    _processor = [ItosProcessor, ArrayProcessor]\n",
    "    \n",
    "    def __init__(self, items:Iterator, itos:List[str]=None, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.itos = itos\n",
    "        self.copy_new += ['itos']\n",
    "        self.c = len(self.items)\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return SequenceItem(o, self.itos)\n",
    "\n",
    "    def reconstruct(self,t):\n",
    "        # Converting padded tensor back into np.array\n",
    "        o = t.numpy()\n",
    "        o = o[np.nonzero(o)]                  # remove 0 padding\n",
    "        return SequenceItem(o, self.itos)\n",
    "    \n",
    "    def analyze_pred(self,pred):\n",
    "        return torch.argmax(pred, dim=-1)\n",
    "        # method called in learn.predict() or learn.show_results()\n",
    "        # to transform predictions in an output tensor suitable for reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # This slows down training...\n",
    "# class CustomSampler(Sampler):\n",
    "#     \"sort dataset by longest y sequences\"\n",
    "#     def __init__(self, dataset):\n",
    "#         self.dataset = dataset\n",
    "#         self.lengths = [len(i) for i in dataset.y.items]\n",
    "#         self.sorted_idxs = np.flip(np.argsort(self.lengths))\n",
    "#     def __len__(self): return len(self.dataset)\n",
    "#     def __iter__(self): return iter(self.sorted_idxs)\n",
    "\n",
    "# ds = data.train_ds\n",
    "# tfms = data.train_dl.tfms\n",
    "# sampler = CustomSampler(ds)\n",
    "# dl = DataLoader(ds, bs, shuffle=False, sampler=sampler, num_workers=num_cpus(), collate_fn=custom_collater, drop_last=True)\n",
    "# ddl = DeviceDataLoader(dl, device, tfms, custom_collater)\n",
    "# data.train_dl = ddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.batch_stats()\n",
    "# no normalization: [tensor([0.9403, 0.9403, 0.9403]), tensor([0.1604, 0.1604, 0.1604])]\n",
    "# imagenet_stats:   [tensor([1.9973, 2.1714, 2.3839]), tensor([0.6974, 0.7130, 0.7098])]\n",
    "# normalize():      [tensor([0.0225, 0.0225, 0.0225]), tensor([0.9676, 0.9676, 0.9676])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def char_label_text(pred):\n",
    "    return self.sp.DecodeIds(pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SPMProcessor(PreProcessor):\n",
    "    def __init__(self, ds:ItemList=None):\n",
    "        self.sp = ds.sp if ds is not None else None\n",
    "\n",
    "    def process_one(self,item): return self.sp.EncodeAsIds(item)\n",
    "    def process(self, ds): super().process(ds)\n",
    "    \n",
    "class SPMList(ItemList):\n",
    "    _processor = [SPMProcessor]\n",
    "\n",
    "    def __init__(self, items:Iterator, sp_processor, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.sp = sp_processor\n",
    "        self.copy_new += ['sp']\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return Text(o, self.sp.DecodeIds(o))\n",
    "\n",
    "    def reconstruct(self, t:Tensor):\n",
    "        return Text(t, self.sp.DecodeIds(t.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(PATH/'spm_train.model'))\n",
    "sp.SetEncodeExtraOptions(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER)\n",
    "        .split_by_rand_pct(valid_pct=0.15, seed=42)\n",
    "        .label_from_df(label_cls=SPMList, sp_processor=sp)\n",
    "        .transform(tfms, size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "        .normalize()\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_itos = word_itos[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_itos = pickle.load(open(PATH/'word_itos_60k.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab = Vocab(word_itos)\n",
    "procs = [TokenizeProcessor(include_bos=False, include_eos=True), NumericalizeProcessor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER, after_open=force_gray)\n",
    "        .split_by_rand_pct(valid_pct=0.15, seed=42)\n",
    "        .label_from_df(label_cls=TextList, pad_idx=0, vocab=vocab, processor=procs)\n",
    "        .transform(tfms, size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "toknizr = Tokenizer(special_cases=[PAD,UNK,PAD,BOS,EOS,TK_MAJ,TK_UP,TK_REP,TK_WREP])\n",
    "procs = [TokenizeProcessor(tokenizer=toknizr, include_bos=False, include_eos=True), NumericalizeProcessor(max_vocab=10000)]\n",
    "\n",
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER, after_open=force_gray)\n",
    "        .split_by_rand_pct(valid_pct=0.15, seed=42)\n",
    "        .label_from_df(label_cls=TextList, pad_idx=0, processor=procs)\n",
    "        .transform(tfms, size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_itos = data.train_ds.vocab.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=2, ds_type=DatasetType.Train, figsize=(18,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transformer Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# LayerNorm = nn.LayerNorm\n",
    "LayerNorm = partial(nn.LayerNorm, eps=1e-4)  # accomodates mixed precision training\n",
    "# LayerNorm = partial(nn.BatchNorm2d, eps=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"A residual connection followed by a layer norm.  Note: (for code simplicity) norm is first.\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder: self-attn and feed forward\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, src, tgt_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder: self-attn, src-attn, and feed forward\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)  # wraps layer in residual,dropout,norm\n",
    " \n",
    "    def forward(self, x, src, tgt_mask=None):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))  # acts as a weak LM\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, src, src))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    depth = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(depth)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e4)  #changed from: -1e9 to accomodate mixed precision  \n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SingleHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.2):\n",
    "        super(SingleHeadedAttention, self).__init__()\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):        \n",
    "        query, key, value = [l(x) for l, x in zip(self.linears, (query, key, value))]\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, h=8, dropout=0.2):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h        # assume d_v always equals d_k\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        if mask is not None: mask = mask.unsqueeze(1)\n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        q, k, v = [l(x).view(bs, -1, self.h, self.d_k).transpose(1,2) for l, x in zip(self.linears, (q, k, v))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(bs, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class GeLU(nn.Module):\n",
    "#     def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.2):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_model*4)\n",
    "        self.w_2 = nn.Linear(d_model*4, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         self.activation = GeLU() #nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.gelu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=2000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        log_increment = math.log(1e4) / d_model\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2) * -log_increment)  \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe.unsqueeze_(0)\n",
    "\n",
    "        self.register_buffer('pe', pe)    #(1,max_len,d_model)\n",
    "        # registered buffers are Tensors (not Variables)\n",
    "        # not a parameter but still want in the state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        return self.decode(self.encode(src), tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def decode(self, src, tgt, tgt_mask=None):\n",
    "        return self.decoder(self.tgt_embed(tgt), src, tgt_mask)\n",
    "    \n",
    "    def generate(self, outs):\n",
    "        return self.generator(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResnetBase(nn.Module):\n",
    "    def __init__(self, em_sz, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        slices = {128: -4, 256: -3, 512: -2}\n",
    "        s = slices[em_sz]\n",
    "\n",
    "        net = models.resnet34(True)\n",
    "        modules = list(net.children())[:s]\n",
    "        self.base = nn.Sequential(*modules)\n",
    "        \n",
    "        self.linear = nn.Linear(em_sz, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(2,3).permute(0,2,1)\n",
    "        x = self.linear(x) * 8\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_full_model(vocab, d_model, N=4, drops=0.2, attn_type='multi', attn_heads=8):\n",
    "    c = deepcopy\n",
    "    \n",
    "    if attn_type=='multi':\n",
    "        attn = MultiHeadedAttention(d_model, attn_heads)\n",
    "    else:\n",
    "        attn = SingleHeadedAttention(d_model)\n",
    "        \n",
    "    ff = PositionwiseFeedForward(d_model, drops)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), drops), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), drops), N),\n",
    "        nn.Sequential(\n",
    "            Embeddings(d_model, vocab), PositionalEncoding(d_model, drops, 2000)\n",
    "        ),\n",
    "        nn.Linear(d_model, vocab),\n",
    "    )\n",
    "        \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, img_encoder, transformer):\n",
    "        super(Img2Seq, self).__init__()\n",
    "        self.img_enc = img_encoder\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def forward(self, src, tgt=None, tgt_mask=None, seq_len=700):\n",
    "        # inference (greedy decode)\n",
    "        if tgt is None:\n",
    "            with torch.no_grad():\n",
    "                feats = self.transformer.encode(self.img_enc(src))\n",
    "                bs = src.size(0)\n",
    "                tgt = torch.ones((bs,1), dtype=torch.long, device=device)\n",
    "\n",
    "                res = []\n",
    "                for i in progress_bar(range(seq_len)):\n",
    "                    mask = subsequent_mask(tgt.size(-1))\n",
    "                    dec_outs = self.transformer.decode(feats, tgt, mask)\n",
    "                    prob = self.transformer.generate(dec_outs[:,-1])\n",
    "                    res.append(prob)\n",
    "                    pred = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "                    if (pred==0).all(): break\n",
    "                    tgt = torch.cat([tgt,pred], dim=-1)\n",
    "                out = torch.stack(res).transpose(1,0).contiguous()\n",
    "                \n",
    "        #training\n",
    "        else:\n",
    "            feats = self.img_enc(src)\n",
    "            dec_outs = self.transformer(feats, tgt, tgt_mask)    # ([bs, sl, d_model])\n",
    "            out = self.transformer.generate(dec_outs)            # ([bs, sl, vocab])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_learner(data, d_model, em_sz, N=4, drops=0.2, attn_type='multi', attn_heads=8):\n",
    "    img_encoder = ResnetBase(em_sz, d_model)\n",
    "    transformer = make_full_model(len(itos), d_model, N=N, drops=drops, attn_type=attn_type, attn_heads=attn_heads)\n",
    "    net = Img2Seq(img_encoder, transformer)\n",
    "    return Learner(data, net, loss_func=LabelSmoothing(smoothing=0.1),\n",
    "                    metrics=[CER()], callback_fns=[TeacherForce])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment - Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = make_learner(data, 512, 512, N=6, drops=0.1, attn_type='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load('combo_512_9')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, max_lr=1e-3, callbacks=[SaveModelCallback(learn, name='combo_512_10')])\n",
    "#sm\n",
    "#5cycle,1e-3\n",
    "\n",
    "# 2.474220\t4.081600\t0.036482  N:4,F.gelu,sz:256,em_sz:512,single,drop:0  'sm_256_1'\n",
    "# greedy:    1.36697   .066\n",
    "# 4.430638\t4.402001\t0.034730  2nd run, lr:1.5e-5, add tfms, drop:0.2   'sm_256_2'\n",
    "# greedy:    0.37206   .04145\n",
    "\n",
    "# 5.158808\t4.964441\t0.042544  \"\", w/ tfms, drop:0.2   'sm_256_3'\n",
    "# greedy:    0.50646   .04103\n",
    "# 4.266788\t4.694098\t0.039822  2nd run, lr:1.5e-5   'sm_256_4'\n",
    "# greedy:    0.38972   .03879\n",
    "\n",
    "# 3.678168\t3.962710\t0.035466  N:8,tfms   'sm_256_5'\n",
    "# greedy:    0.38097   .04405\n",
    "\n",
    "# 2.840161\t3.429177\t0.030243  N:4,tfms,multi(8)  'sm_256_6'\n",
    "# greedy:    0.32775   .04201\n",
    "\n",
    "# 3.377438\t3.786088\t0.033931   N:6,tfms,drop:0.1,multi(8)   'sm_256_7'\n",
    "# greedy:    0.36201   .04266\n",
    "\n",
    "# combo_cat6 - preload 'sm_256_7'\n",
    "# 7.570516\t6.670641\t0.015070    'combo_512_7'\n",
    "# greedy:    6.29406   .02186\n",
    "\n",
    "# combo_cat_pg - preload 'combo_512_7'\n",
    "# 25.674347\t21.161726\t0.015869    'combo_512_8'\n",
    "# greedy:    114.878   .03654\n",
    "#   test:    115.247   .05014\n",
    "\n",
    "# combo_cat_pg_dl_sorted - preload 'combo_512_8', 5cycle, 1.5e-4\n",
    "# 9.800321\t5.676855\t0.006939    'combo_512_9'\n",
    "# greedy:    34.8757   .01523\n",
    "\n",
    "#   test:    91.3226   .05102\n",
    "#   test:    104.685   .05483\n",
    "#   test:    116.467   .05315\n",
    "\n",
    "# upload:    115.149   .66801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# learn.save('combo_512_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Experiment - Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = make_learner(data, 512, 512, N=6, drops=0.1, attn_type='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load('combo_512_9')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(15, max_lr=1e-3, callbacks=[SaveModelCallback(learn, name='word_sm_2')])\n",
    "\n",
    "# word 60k\n",
    "# sm, 5cycle, 1e-3\n",
    "\n",
    "# 22.903589\t22.401066\t0.520125   N:6,em_sz:512,tfms,drop:0.1,multi(8)  'word_sm_1'\n",
    "# 32.389709\t31.828810\t0.746749   N:4,em_sz:512,tfms,drop:0.1,single\n",
    "# 34.492157\t32.719650\t0.776098   \"\",em_sz:256\n",
    "\n",
    "# word 7k (auto-tokenize from data)\n",
    "# sm, 5cycle, 1e-3\n",
    "# 16.987526\t16.639275\t0.421535   N:4,em_sz:512,tfms,drop:0.1,single\n",
    "# 10.224819\t10.781199\t0.297889   2nd run\n",
    "# 6.486265\t7.934221\t0.225055   3rd run    'word_sm_4'\n",
    "\n",
    "# word 10k (itos from larger txt files)\n",
    "# sm, 15cycle, 1e-3\n",
    "# 3.893518\t6.037587\t0.209027   N:4,em_sz:512,tfms,drop:0.1,single   'word_sm_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# nn.Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, vocab, d_model, em_sz, **kwargs):\n",
    "        super(Img2Seq, self).__init__()\n",
    "        self.img_enc = ResnetBase(d_model, em_sz)\n",
    "        self.transformer = nn.Transformer(d_model, **kwargs)\n",
    "        self.tgt_embed = nn.Sequential(Embeddings(d_model, vocab), PositionalEncoding(d_model))\n",
    "        self.generator = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, src, tgt=None, tgt_mask=None):\n",
    "        feats = self.img_enc(src).permute(1,0,2)\n",
    "        \n",
    "        # inference (greedy decode)\n",
    "        if tgt is None:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                feats = self.transformer.encode(feats)\n",
    "                bs = src.size(0)\n",
    "                tgt = torch.ones((1,bs), dtype=torch.long, device=device)\n",
    "\n",
    "                res = []\n",
    "                for i in progress_bar(range(self.seq_len)):\n",
    "                    mask = subsequent_mask(tgt.size(0))\n",
    "                    dec_outs = self.transformer.decode(feats, self.tgt_embed(tgt), mask)\n",
    "                    prob = self.generator(dec_outs[:,-1])\n",
    "                    res.append(prob)\n",
    "                    pred = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "                    if (pred==0).all(): break\n",
    "                    tgt = torch.cat([tgt,pred], dim=-1)\n",
    "                out = torch.stack(res).transpose(1,0).contiguous()\n",
    "                \n",
    "        #training\n",
    "        else:\n",
    "            tgt = self.tgt_embed(tgt).permute(1,0,2)\n",
    "            dec_outs = self.transformer(feats, tgt, tgt_mask)    # ([bs, sl, d_model])\n",
    "            out = self.generator(dec_outs)            # ([bs, sl, vocab])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_learner(data, vocab, d_model, em_sz, **kwargs):\n",
    "    net = Img2Seq(vocab, d_model, em_sz, **kwargs)\n",
    "    return Learner(data, net, loss_func=LabelSmoothing(smoothing=0.1),\n",
    "                    metrics=[CER()], callback_fns=[TeacherForce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = make_learner(data, len(itos), 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, max_lr=1e-3)\n",
    "\n",
    "# 50, 5e-3\n",
    "# 58.284210  d_model:512, singlehead\n",
    "# 32.917747  d_model:256\n",
    "# 36.486694  see below...\n",
    "# 32.104549  d_model:128\n",
    "# 33.139687  d_model:256, BN instead of linear\n",
    "# 41.315910  d_model:256, no BN/linear/*8\n",
    "# 39.416367  d_model:256, LayerNorm(1e-5)\n",
    "# 36.486694  d_model:256, LayerNorm(1e-4)  same as above...\n",
    "# 33.884861  d_model:256, LayerNorm(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# learn.save('tmp_overfit_7.45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spatial Encoding (fixed, after flatten - 1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, tgt_embed, src_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_embed = src_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        return self.decode(self.encode(src), tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        return self.encoder(self.src_embed(src))\n",
    "    \n",
    "    def decode(self, src, tgt, tgt_mask=None):\n",
    "        return self.decoder(self.tgt_embed(tgt), src, tgt_mask)\n",
    "    \n",
    "    def generate(self, outs):\n",
    "        return self.generator(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResnetBase(nn.Module):\n",
    "    def __init__(self, em_sz, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        slices = {128: -4, 256: -3, 512: -2}\n",
    "        s = slices[em_sz]\n",
    "\n",
    "        net = models.resnet34(True)\n",
    "        modules = list(net.children())[:s]\n",
    "        self.base = nn.Sequential(*modules)\n",
    "        \n",
    "        self.linear = nn.Linear(em_sz, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base(x).flatten(2,3).permute(0,2,1)\n",
    "        x = self.linear(x) * 8\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_full_model(vocab, d_model, N=4, drops=0.2):\n",
    "    c = deepcopy\n",
    "#     attn = SingleHeadedAttention(d_model)\n",
    "    attn = MultiHeadedAttention(d_model, 8)\n",
    "    ff = PositionwiseFeedForward(d_model, drops)\n",
    "    pos = PositionalEncoding(d_model, drops, 2000)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), drops), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), drops), N),\n",
    "        nn.Sequential(Embeddings(d_model, vocab), pos),\n",
    "        pos,\n",
    "        nn.Linear(d_model, vocab),\n",
    "    )\n",
    "        \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, img_encoder, transformer, seq_len=500):\n",
    "        super(Img2Seq, self).__init__()\n",
    "        self.img_enc = img_encoder\n",
    "        self.transformer = transformer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, src, tgt=None, tgt_mask=None):\n",
    "        # inference (greedy decode)\n",
    "        if tgt is None:\n",
    "            with torch.no_grad():\n",
    "                feats = self.img_enc(src)\n",
    "                feats = self.transformer.encode(feats)\n",
    "                bs = src.size(0)\n",
    "                tgt = torch.ones((bs,1), dtype=torch.long, device=device)\n",
    "\n",
    "                res = []\n",
    "                for i in progress_bar(range(self.seq_len)):\n",
    "                    mask = subsequent_mask(tgt.size(-1))\n",
    "                    dec_outs = self.transformer.decode(feats, tgt, mask)\n",
    "                    prob = self.transformer.generate(dec_outs[:,-1])\n",
    "                    res.append(prob)\n",
    "                    pred = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "                    if (pred==0).all(): break\n",
    "                    tgt = torch.cat([tgt,pred], dim=-1)\n",
    "                out = torch.stack(res).transpose(1,0).contiguous()\n",
    "                \n",
    "        #training\n",
    "        else:\n",
    "            feats = self.img_enc(src)\n",
    "            dec_outs = self.transformer(feats, tgt, tgt_mask)    # ([bs, sl, d_model])\n",
    "            out = self.transformer.generate(dec_outs)            # ([bs, sl, vocab])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "em_sz = 256\n",
    "img_encoder = ResnetBase(em_sz, d_model)\n",
    "transformer = make_full_model(len(itos), d_model)  #len(itos)\n",
    "net = Img2Seq(img_encoder, transformer, seq_len)\n",
    "\n",
    "# AdamW16 = partial(optim.Adam, betas=(0.9,0.99), eps=1e-4)  ->  #.to_fp16(max_scale=256)\n",
    "# partial: way to always call a function with a given set of arguments or keywords\n",
    "\n",
    "learn = Learner(data, net, loss_func=LabelSmoothing(smoothing=0.1),\n",
    "                metrics=[CER(itos)], callback_fns=TeacherForce)\n",
    "learn.clip_grad(0.25)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Complex Batches (lines x char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rshift(tgt, bos_token=1):\n",
    "    \"Shift y to the right by prepending token\"\n",
    "    bos = torch.zeros((tgt.size(0),tgt.size(1),1), device=device).type_as(tgt) + bos_token\n",
    "    return torch.cat((bos, tgt[:,:,:-1]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Modified the PE function for 2 dimensions\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=2000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        channels = d_model//2 #dims\n",
    "        pe = torch.zeros(max_len, channels)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        log_increment = math.log(1e4) / channels\n",
    "        div_term = torch.exp(torch.arange(0.0, channels, 2) * -log_increment)  \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe.unsqueeze_(0)\n",
    "        \n",
    "        w_pe = F.pad(pe, (0,channels)).unsqueeze(2)\n",
    "        h_pe = F.pad(pe, (channels,0)).unsqueeze(1)\n",
    "        pe = w_pe + h_pe\n",
    "\n",
    "        self.register_buffer('pe', pe)    #(1,max_len,max_len,d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ([bs, h, w, d_model])\n",
    "        x = x + self.pe[:, :x.size(1), :x.size(2)]   #addition\n",
    "#         x = torch.cat([x, self.pe[:, :x.size(1), :x.size(2)]])  #concatenation\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        return self.decode(self.encode(src), tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def decode(self, src, tgt, tgt_mask=None):\n",
    "        return self.decoder(self.tgt_embed(tgt), src, tgt_mask)\n",
    "    \n",
    "    def generate(self, outs):\n",
    "        return self.generator(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.BatchNorm2d(layer.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.size = d_model\n",
    "        self.self_attn = ImageSelfAttention(d_model, dropout=dropout)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(d_model, d_model*4, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(d_model*4, d_model, 1)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(d_model)\n",
    "        self.norm2 = nn.BatchNorm2d(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.norm1(x)\n",
    "        x2 = self.self_attn(x2, x2, x2)\n",
    "        x = x + self.dropout1(x2)\n",
    "        \n",
    "        x2 = self.conv2(self.dropout(F.relu(self.conv1(self.norm2(x)))))\n",
    "        x = x + self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = d_model\n",
    "        self.self_attn = SingleHeadedAttention(d_model)\n",
    "        self.src_attn = SingleHeadedAttention(d_model)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_model*4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_model*4, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, src, tgt_mask=None):        \n",
    "        tgt2 = self.norm1(tgt)\n",
    "        tgt2 = self.self_attn(tgt2, tgt2, tgt2, tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        \n",
    "        src = src.permute(0,2,3,1)     # bs,h,w,d_model\n",
    "        tgt2 = self.src_attn(self.norm2(tgt), src, src)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        \n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResnetBase(nn.Module):\n",
    "    def __init__(self, em_sz, d_model, num_lines):\n",
    "        super().__init__()\n",
    "        \n",
    "        slices = {128: -4, 256: -3, 512: -2}\n",
    "        s = slices[em_sz]\n",
    "\n",
    "        net = models.resnet34(True)\n",
    "        modules = list(net.children())[:s]\n",
    "        self.base = nn.Sequential(*modules)\n",
    "        \n",
    "        self.conv = nn.Conv2d(em_sz, d_model, 1)\n",
    "        self.pool = nn.AdaptiveMaxPool2d((num_lines, None))\n",
    "#         self.linear = nn.Linear(em_sz, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pool(self.conv(self.base(x)))\n",
    "#         x = self.base(x) #.permute(0,2,3,1)   #bs,h(#rows),w(#cols),d_model\n",
    "#         x = self.linear(x) * 8        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ImageSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.2):\n",
    "        super(ImageSelfAttention, self).__init__()\n",
    "        self.convs = clones(nn.Conv2d(d_model, d_model, 1), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):        \n",
    "        query, key, value = [l(x) for l, x in zip(self.convs, (query, key, value))]\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        return self.convs[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_full_model(vocab, d_model, N=4, drops=0.2):\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, drops), N),\n",
    "        Decoder(DecoderLayer(d_model, drops), N),\n",
    "        nn.Sequential(\n",
    "            Embeddings(d_model, vocab), PositionalEncoding(d_model, drops, 2000)\n",
    "        ),\n",
    "        nn.Linear(d_model, vocab)\n",
    "    )\n",
    "        \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, img_encoder, transformer, num_lines, seq_len):\n",
    "        super(Img2Seq, self).__init__()\n",
    "        self.img_enc = img_encoder\n",
    "        self.transformer = transformer\n",
    "        self.num_lines = num_lines\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, src, tgt=None, tgt_mask=None):\n",
    "        # inference (greedy decode)\n",
    "        if tgt is None:\n",
    "            with torch.no_grad():\n",
    "                feats = self.transformer.encode(self.img_enc(src))\n",
    "                bs = src.size(0)\n",
    "                tgt = torch.ones((bs,self.num_lines,1), dtype=torch.long, device=device)\n",
    "\n",
    "                res = []\n",
    "                for i in progress_bar(range(self.seq_len)):\n",
    "                    mask = subsequent_mask(tgt.size(-1))\n",
    "                    dec_outs = self.transformer.decode(feats, tgt, mask)\n",
    "                    prob = self.transformer.generate(dec_outs[:,:,-1])\n",
    "                    res.append(prob)\n",
    "                    pred = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "                    if (pred==0).all(): break\n",
    "                    tgt = torch.cat([tgt,pred], dim=-1)\n",
    "                out = torch.stack(res).transpose(1,0).contiguous()\n",
    "                \n",
    "        #training\n",
    "        else:\n",
    "            feats = self.img_enc(src)\n",
    "            dec_outs = self.transformer(feats, tgt, tgt_mask)    # ([bs, sl, d_model])\n",
    "            out = self.transformer.generate(dec_outs)            # ([bs, sl, vocab])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "em_sz = 256\n",
    "img_encoder = ResnetBase(em_sz, d_model, num_lines)\n",
    "transformer = make_full_model(len(itos), d_model)\n",
    "net = Img2Seq(img_encoder, transformer, num_lines, seq_len)\n",
    "\n",
    "learn = Learner(data, net, loss_func=LabelSmoothing(smoothing=0.1), metrics=[CER(itos)], callback_fns=TeacherForce)\n",
    "learn.clip_grad(0.25)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def loss_prep(preds, target):\n",
    "    bs,tlines,tsl = target.shape\n",
    "    _,lines,sl,vocab = preds.shape\n",
    "                \n",
    "    # F.pad( front,back for dimensions: last,second to last,... )\n",
    "    if sl>tsl: target = F.pad(target, (0,sl-tsl))\n",
    "    if lines>tlines: target = F.pad(target, (0,0,0,lines-tlines))\n",
    "        \n",
    "    if tsl>sl: preds = F.pad(preds, (0,0,0,tsl-sl))\n",
    "    if tlines>lines: target = F.pad(target, (0,0,0,0,0,tlines-lines))\n",
    "    # not ideal => adds 96 logits all 0s...\n",
    "        \n",
    "    targ = target.contiguous().view(-1).long()\n",
    "    pred = preds.contiguous().view(-1, vocab)\n",
    "    return pred, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(3, 1e-3)\n",
    "#  18.494501\t15.946078\t0.178671\t03:18  3cycle,1e-3\n",
    "#  11.922337\t11.197309\t0.118325\t03:18  2nd run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# learn.save('complex_batches_128')\n",
    "learn.load('complex_batches_128')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Different size Convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        return self.decode(self.encode(src), tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def decode(self, src, tgt, tgt_mask=None):\n",
    "        return self.decoder(self.tgt_embed(tgt), src, tgt_mask)\n",
    "    \n",
    "    def generate(self, outs):\n",
    "        return self.generator(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResnetBase(nn.Module):\n",
    "    def __init__(self, em_sz, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        slices = {128: -4, 256: -3, 512: -2}\n",
    "        s = slices[em_sz]\n",
    "\n",
    "        net = models.resnet34(True)\n",
    "        modules = list(net.children())[:s]\n",
    "\n",
    "        self.base = nn.Sequential(*modules)\n",
    "#         self.base[0] = nn.Conv2d(3,64,7,stride=(2,1),padding=3,bias=False)\n",
    "        self.base[3] = Lambda(lambda x: x) #nn.MaxPool2d(3,stride=(2,1),padding=1)\n",
    "#         self.base[5][0].conv1 = nn.Conv2d(64,128,3,stride=(2,1),padding=1)\n",
    "#         self.base[5][0].downsample[0] = nn.Conv2d(64,128,1,stride=(2,1))\n",
    "        \n",
    "        self.linear = nn.Linear(em_sz, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(2,3).permute(0,2,1)\n",
    "        x = self.linear(x) * 8\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_full_model(vocab, d_model, N=4, drops=0.2):\n",
    "    c = deepcopy\n",
    "#     single_attn = SingleHeadedAttention(d_model)\n",
    "    multi_attn = MultiHeadedAttention(d_model, 4)\n",
    "    ff = PositionwiseFeedForward(d_model, drops)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(multi_attn), c(ff), drops), N),\n",
    "        Decoder(DecoderLayer(d_model, c(multi_attn), c(multi_attn), c(ff), drops), N),\n",
    "        nn.Sequential(\n",
    "            Embeddings(d_model, vocab), PositionalEncoding(d_model, drops, 2000)\n",
    "        ),\n",
    "        nn.Linear(d_model, vocab),\n",
    "    )\n",
    "        \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, img_encoder, transformer, seq_len=500):\n",
    "        super(Img2Seq, self).__init__()\n",
    "        self.img_enc = img_encoder\n",
    "        self.transformer = transformer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, src, tgt=None, tgt_mask=None):\n",
    "        # inference (greedy decode)\n",
    "        if tgt is None:\n",
    "            with torch.no_grad():\n",
    "                feats = self.transformer.encode(self.img_enc(src))\n",
    "                bs = src.size(0)\n",
    "                tgt = torch.ones((bs,1), dtype=torch.long, device=device)\n",
    "\n",
    "                res = []\n",
    "                for i in progress_bar(range(self.seq_len)):\n",
    "                    mask = subsequent_mask(tgt.size(-1))\n",
    "                    dec_outs = self.transformer.decode(feats, tgt, mask)\n",
    "                    prob = self.transformer.generate(dec_outs[:,-1])\n",
    "                    res.append(prob)\n",
    "                    pred = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "                    if (pred==0).all(): break\n",
    "                    tgt = torch.cat([tgt,pred], dim=-1)\n",
    "                out = torch.stack(res).transpose(1,0).contiguous()\n",
    "                \n",
    "        #training\n",
    "        else:\n",
    "            feats = self.img_enc(src)\n",
    "            dec_outs = self.transformer(feats, tgt, tgt_mask)    # ([bs, sl, d_model])\n",
    "            out = self.transformer.generate(dec_outs)            # ([bs, sl, vocab])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "em_sz = 512 #256\n",
    "# img_encoder = ResnetBase(em_sz, d_model)\n",
    "# transformer = make_full_model(len(itos), d_model)  #len(itos)\n",
    "net = Img2Seq(ResnetBase(em_sz, d_model), make_full_model(len(itos), d_model), seq_len)\n",
    "\n",
    "# AdamW16 = partial(optim.Adam, betas=(0.9,0.99), eps=1e-4)  ->  #.to_fp16(max_scale=256)\n",
    "# partial: way to always call a function with a given set of arguments or keywords\n",
    "\n",
    "learn = Learner(data, net, loss_func=LabelSmoothing(smoothing=0.1),\n",
    "                metrics=[CER(itos)], callback_fns=TeacherForce)\n",
    "learn.clip_grad(0.25)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deformable Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/oeway/pytorch-deform-conv/blob/master/torch_deform_conv\n",
    "\n",
    "from scipy.ndimage.interpolation import map_coordinates as sp_map_coordinates\n",
    "\n",
    "\n",
    "def th_flatten(a):\n",
    "    \"\"\"Flatten tensor\"\"\"\n",
    "    return a.contiguous().view(a.nelement())\n",
    "\n",
    "\n",
    "def th_repeat(a, repeats, axis=0):\n",
    "    \"\"\"Torch version of np.repeat for 1D\"\"\"\n",
    "    assert len(a.size()) == 1\n",
    "    return th_flatten(torch.transpose(a.repeat(repeats, 1), 0, 1))\n",
    "\n",
    "\n",
    "def np_repeat_2d(a, repeats):\n",
    "    \"\"\"Tensorflow version of np.repeat for 2D\"\"\"\n",
    "\n",
    "    assert len(a.shape) == 2\n",
    "    a = np.expand_dims(a, 0)\n",
    "    a = np.tile(a, [repeats, 1, 1])\n",
    "    return a\n",
    "\n",
    "\n",
    "def th_gather_2d(input, coords):\n",
    "    inds = coords[:, 0]*input.size(1) + coords[:, 1]\n",
    "    x = torch.index_select(th_flatten(input), 0, inds)\n",
    "    return x.view(coords.size(0))\n",
    "\n",
    "\n",
    "def th_map_coordinates(input, coords, order=1):\n",
    "    \"\"\"Tensorflow verion of scipy.ndimage.map_coordinates\n",
    "    Note that coords is transposed and only 2D is supported\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : tf.Tensor. shape = (s, s)\n",
    "    coords : tf.Tensor. shape = (n_points, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    assert order == 1\n",
    "    input_size = input.size(0)\n",
    "\n",
    "    coords = torch.clamp(coords, 0, input_size - 1)\n",
    "    coords_lt = coords.floor().long()\n",
    "    coords_rb = coords.ceil().long()\n",
    "    coords_lb = torch.stack([coords_lt[:, 0], coords_rb[:, 1]], 1)\n",
    "    coords_rt = torch.stack([coords_rb[:, 0], coords_lt[:, 1]], 1)\n",
    "\n",
    "    vals_lt = th_gather_2d(input,  coords_lt.detach())\n",
    "    vals_rb = th_gather_2d(input,  coords_rb.detach())\n",
    "    vals_lb = th_gather_2d(input,  coords_lb.detach())\n",
    "    vals_rt = th_gather_2d(input,  coords_rt.detach())\n",
    "\n",
    "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
    "\n",
    "    vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, 0]\n",
    "    vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, 0]\n",
    "    mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, 1]\n",
    "    return mapped_vals\n",
    "\n",
    "\n",
    "def sp_batch_map_coordinates(inputs, coords):\n",
    "    \"\"\"Reference implementation for batch_map_coordinates\"\"\"\n",
    "    # coords = coords.clip(0, inputs.shape[1] - 1)\n",
    "\n",
    "    assert (coords.shape[2] == 2)\n",
    "    height = coords[:,:,0].clip(0, inputs.shape[1] - 1)\n",
    "    width = coords[:,:,1].clip(0, inputs.shape[2] - 1)\n",
    "    np.concatenate((np.expand_dims(height, axis=2), np.expand_dims(width, axis=2)), 2)\n",
    "\n",
    "    mapped_vals = np.array([\n",
    "        sp_map_coordinates(input, coord.T, mode='nearest', order=1)\n",
    "        for input, coord in zip(inputs, coords)\n",
    "    ])\n",
    "    return mapped_vals\n",
    "\n",
    "\n",
    "def th_batch_map_coordinates(input, coords, order=1):\n",
    "    \"\"\"Batch version of th_map_coordinates\n",
    "    Only supports 2D feature maps\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : tf.Tensor. shape = (b, s, s)\n",
    "    coords : tf.Tensor. shape = (b, n_points, 2)\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor. shape = (b, s, s)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = input.size(0)\n",
    "    input_height = input.size(1)\n",
    "    input_width = input.size(2)\n",
    "\n",
    "    n_coords = coords.size(1)\n",
    "\n",
    "    # coords = torch.clamp(coords, 0, input_size - 1)\n",
    "\n",
    "    coords = torch.cat((torch.clamp(coords.narrow(2, 0, 1), 0, input_height - 1), torch.clamp(coords.narrow(2, 1, 1), 0, input_width - 1)), 2)\n",
    "\n",
    "    assert (coords.size(1) == n_coords)\n",
    "\n",
    "    coords_lt = coords.floor().long()\n",
    "    coords_rb = coords.ceil().long()\n",
    "    coords_lb = torch.stack([coords_lt[..., 0], coords_rb[..., 1]], 2)\n",
    "    coords_rt = torch.stack([coords_rb[..., 0], coords_lt[..., 1]], 2)\n",
    "    idx = th_repeat(torch.arange(0, batch_size), n_coords).long()\n",
    "#     idx = Variable(idx, requires_grad=False)\n",
    "    if input.is_cuda:\n",
    "        idx = idx.cuda()\n",
    "\n",
    "    def _get_vals_by_coords(input, coords):\n",
    "        indices = torch.stack([\n",
    "            idx, th_flatten(coords[..., 0]), th_flatten(coords[..., 1])\n",
    "        ], 1)\n",
    "        inds = indices[:, 0]*input.size(1)*input.size(2)+ indices[:, 1]*input.size(2) + indices[:, 2]\n",
    "        vals = th_flatten(input).index_select(0, inds)\n",
    "        vals = vals.view(batch_size, n_coords)\n",
    "        return vals\n",
    "\n",
    "    vals_lt = _get_vals_by_coords(input, coords_lt.detach())\n",
    "    vals_rb = _get_vals_by_coords(input, coords_rb.detach())\n",
    "    vals_lb = _get_vals_by_coords(input, coords_lb.detach())\n",
    "    vals_rt = _get_vals_by_coords(input, coords_rt.detach())\n",
    "\n",
    "    coords_offset_lt = coords - coords_lt.type(coords.data.type())\n",
    "    vals_t = coords_offset_lt[..., 0]*(vals_rt - vals_lt) + vals_lt\n",
    "    vals_b = coords_offset_lt[..., 0]*(vals_rb - vals_lb) + vals_lb\n",
    "    mapped_vals = coords_offset_lt[..., 1]* (vals_b - vals_t) + vals_t\n",
    "    return mapped_vals\n",
    "\n",
    "\n",
    "def sp_batch_map_offsets(input, offsets):\n",
    "    \"\"\"Reference implementation for tf_batch_map_offsets\"\"\"\n",
    "\n",
    "    batch_size = input.shape[0]\n",
    "    input_height = input.shape[1]\n",
    "    input_width = input.shape[2]\n",
    "\n",
    "    offsets = offsets.reshape(batch_size, -1, 2)\n",
    "    grid = np.stack(np.mgrid[:input_height, :input_width], -1).reshape(-1, 2)\n",
    "    grid = np.repeat([grid], batch_size, axis=0)\n",
    "    coords = offsets + grid\n",
    "    # coords = coords.clip(0, input_size - 1)\n",
    "\n",
    "    mapped_vals = sp_batch_map_coordinates(input, coords)\n",
    "    return mapped_vals\n",
    "\n",
    "\n",
    "def th_generate_grid(batch_size, input_height, input_width, dtype, cuda):\n",
    "    grid = np.meshgrid(\n",
    "        range(input_height), range(input_width), indexing='ij'\n",
    "    )\n",
    "    grid = np.stack(grid, axis=-1)\n",
    "    grid = grid.reshape(-1, 2)\n",
    "\n",
    "    grid = np_repeat_2d(grid, batch_size)\n",
    "    grid = torch.from_numpy(grid).type(dtype)\n",
    "    if cuda:\n",
    "        grid = grid.cuda()\n",
    "    return grid #Variable(grid, requires_grad=False)\n",
    "\n",
    "\n",
    "def th_batch_map_offsets(input, offsets, grid=None, order=1):\n",
    "    \"\"\"Batch map offsets into input\n",
    "    Parameters\n",
    "    ---------\n",
    "    input : torch.Tensor. shape = (b, s, s)\n",
    "    offsets: torch.Tensor. shape = (b, s, s, 2)\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor. shape = (b, s, s)\n",
    "    \"\"\"\n",
    "    batch_size = input.size(0)\n",
    "    input_height = input.size(1)\n",
    "    input_width = input.size(2)\n",
    "\n",
    "    offsets = offsets.view(batch_size, -1, 2)\n",
    "    if grid is None:\n",
    "        grid = th_generate_grid(batch_size, input_height, input_width, offsets.data.type(), offsets.data.is_cuda)\n",
    "\n",
    "    coords = offsets + grid\n",
    "\n",
    "    mapped_vals = th_batch_map_coordinates(input, coords)\n",
    "    return mapped_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ConvOffset2D(nn.Conv2d):\n",
    "    \"\"\"ConvOffset2D\n",
    "    Convolutional layer responsible for learning the 2D offsets and output the\n",
    "    deformed feature map using bilinear interpolation\n",
    "    Note that this layer does not perform convolution on the deformed feature\n",
    "    map. See get_deform_cnn in cnn.py for usage\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, init_normal_stddev=0.01, **kwargs):\n",
    "        \"\"\"Init\n",
    "        Parameters\n",
    "        ----------\n",
    "        filters : int\n",
    "            Number of channel of the input feature map\n",
    "        init_normal_stddev : float\n",
    "            Normal kernel initialization\n",
    "        **kwargs:\n",
    "            Pass to superclass. See Con2d layer in pytorch\n",
    "        \"\"\"\n",
    "        self.filters = filters\n",
    "        self._grid_param = None\n",
    "        super(ConvOffset2D, self).__init__(self.filters, self.filters*2, 3, padding=1, bias=False, **kwargs)\n",
    "        self.weight.data.copy_(self._init_weights(self.weight, init_normal_stddev))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Return the deformed featured map\"\"\"\n",
    "        x_shape = x.size()\n",
    "        offsets = super(ConvOffset2D, self).forward(x)\n",
    "\n",
    "        # offsets: (b*c, h, w, 2)\n",
    "        offsets = self._to_bc_h_w_2(offsets, x_shape)\n",
    "\n",
    "        # x: (b*c, h, w)\n",
    "        x = self._to_bc_h_w(x, x_shape)\n",
    "\n",
    "        # X_offset: (b*c, h, w)\n",
    "        x_offset = th_batch_map_offsets(x, offsets, grid=self._get_grid(self,x))\n",
    "\n",
    "        # x_offset: (b, h, w, c)\n",
    "        x_offset = self._to_b_c_h_w(x_offset, x_shape)\n",
    "\n",
    "        return x_offset\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_grid(self, x):\n",
    "        batch_size, input_height, input_width = x.size(0), x.size(1), x.size(2)\n",
    "        dtype, cuda = x.data.type(), x.data.is_cuda\n",
    "        if self._grid_param == (batch_size, input_height, input_width, dtype, cuda):\n",
    "            return self._grid\n",
    "        self._grid_param = (batch_size, input_height, input_width, dtype, cuda)\n",
    "        self._grid = th_generate_grid(batch_size, input_height, input_width, dtype, cuda)\n",
    "        return self._grid\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(weights, std):\n",
    "        fan_out = weights.size(0)\n",
    "        fan_in = weights.size(1) * weights.size(2) * weights.size(3)\n",
    "        w = np.random.normal(0.0, std, (fan_out, fan_in))\n",
    "        return torch.from_numpy(w.reshape(weights.size()))\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_bc_h_w_2(x, x_shape):\n",
    "        \"\"\"(b, 2c, h, w) -> (b*c, h, w, 2)\"\"\"\n",
    "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]), 2)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_bc_h_w(x, x_shape):\n",
    "        \"\"\"(b, c, h, w) -> (b*c, h, w)\"\"\"\n",
    "        x = x.contiguous().view(-1, int(x_shape[2]), int(x_shape[3]))\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_b_c_h_w(x, x_shape):\n",
    "        \"\"\"(b*c, h, w) -> (b, c, h, w)\"\"\"\n",
    "        x = x.contiguous().view(-1, int(x_shape[1]), int(x_shape[2]), int(x_shape[3]))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResnetBase(nn.Module):\n",
    "    def __init__(self, em_sz, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        slices = {128: -4, 256: -3, 512: -2}\n",
    "        s = slices[em_sz]\n",
    "\n",
    "        net = models.resnet34(True)\n",
    "        modules = list(net.children())[:s]\n",
    "\n",
    "        self.base = nn.Sequential(*modules)\n",
    "#         self.base[0] = nn.Conv2d(3,64,7,stride=(2,1),padding=3,bias=False)\n",
    "#         self.base[3] = Lambda(lambda x: x) #nn.MaxPool2d(3,stride=(2,1),padding=1)\n",
    "        self.base[4][2].conv2 = nn.Sequential(ConvOffset2D(64), nn.Conv2d(64,64,3,stride=1,padding=1))\n",
    "        self.base[5][3].conv2 = nn.Sequential(ConvOffset2D(128), nn.Conv2d(128,128,3,stride=1,padding=1))\n",
    "        self.base[6][5].conv2 = nn.Sequential(ConvOffset2D(256), nn.Conv2d(256,256,3,stride=1,padding=1))\n",
    "\n",
    "#         self.base[5][0].conv1 = nn.Conv2d(64,128,3,stride=(2,1),padding=1)\n",
    "#         self.base[5][0].downsample[0] = nn.Conv2d(64,128,1,stride=(2,1))\n",
    "        \n",
    "        self.linear = nn.Linear(em_sz, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(2,3).permute(0,2,1)\n",
    "        x = self.linear(x) * 8\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "em_sz = 256\n",
    "# img_encoder = ResnetBase(em_sz, d_model)\n",
    "# transformer = make_full_model(len(itos), d_model)  #len(itos)\n",
    "net = Img2Seq(ResnetBase(em_sz, d_model), make_full_model(len(itos), d_model), seq_len)\n",
    "\n",
    "# AdamW16 = partial(optim.Adam, betas=(0.9,0.99), eps=1e-4)  ->  #.to_fp16(max_scale=256)\n",
    "# partial: way to always call a function with a given set of arguments or keywords\n",
    "\n",
    "learn = Learner(data, net, loss_func=LabelSmoothing(smoothing=0.1),\n",
    "                metrics=[CER(itos)], callback_fns=TeacherForce)\n",
    "learn.clip_grad(0.25)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/clovaai/deep-text-recognition-benchmark/blob/master/modules/transformation.py\n",
    "\n",
    "class TPS_SpatialTransformerNetwork(nn.Module):\n",
    "    \"\"\" Rectification Network of RARE, namely TPS based STN \"\"\"\n",
    "\n",
    "    def __init__(self, I_size, I_r_size, I_channel_num=1, F=20):\n",
    "        \"\"\" Based on RARE TPS\n",
    "        input:\n",
    "            batch_I: Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
    "            F: num of fiducial points\n",
    "            I_size : (height, width) of the input image I\n",
    "            I_r_size : (height, width) of the rectified image I_r\n",
    "            I_channel_num : the number of channels of the input image I\n",
    "        output:\n",
    "            batch_I_r: rectified image [batch_size x I_channel_num x I_r_height x I_r_width]\n",
    "        \"\"\"\n",
    "        super(TPS_SpatialTransformerNetwork, self).__init__()\n",
    "        self.F = F\n",
    "        self.I_size = I_size\n",
    "        self.I_r_size = I_r_size  # = (I_r_height, I_r_width)\n",
    "        self.I_channel_num = I_channel_num\n",
    "        self.LocalizationNetwork = LocalizationNetwork(self.F, self.I_channel_num)\n",
    "        self.GridGenerator = GridGenerator(self.F, self.I_r_size)\n",
    "\n",
    "    def forward(self, batch_I):\n",
    "        batch_C_prime = self.LocalizationNetwork(batch_I)  # bs x K x 2\n",
    "        build_P_prime = self.GridGenerator.build_P_prime(batch_C_prime) # bs x n (= I_r_width x I_r_height) x 2\n",
    "        build_P_prime_reshape = build_P_prime.reshape([build_P_prime.size(0), self.I_r_size[0], self.I_r_size[1], 2])\n",
    "        batch_I_r = F.grid_sample(batch_I, build_P_prime_reshape, padding_mode='border')\n",
    "\n",
    "        return batch_I_r\n",
    "\n",
    "\n",
    "class LocalizationNetwork(nn.Module):\n",
    "    \"\"\" Localization Network of RARE, which predicts C' (K x 2) from I (I_width x I_height) \"\"\"\n",
    "\n",
    "    def __init__(self, F, I_channel_num):\n",
    "        super(LocalizationNetwork, self).__init__()\n",
    "        self.F = F\n",
    "        self.I_channel_num = I_channel_num\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.I_channel_num, out_channels=64, kernel_size=3, stride=1, padding=1,\n",
    "                      bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # batch_size x 64 x I_height/2 x I_width/2\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # batch_size x 128 x I_height/4 x I_width/4\n",
    "            nn.Conv2d(128, 256, 3, 1, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # batch_size x 256 x I_height/8 x I_width/8\n",
    "            nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.AdaptiveAvgPool2d(1)  # batch_size x 512\n",
    "        )\n",
    "\n",
    "        self.localization_fc1 = nn.Sequential(nn.Linear(512, 256), nn.ReLU(True))\n",
    "        self.localization_fc2 = nn.Linear(256, self.F * 2)\n",
    "\n",
    "        # Init fc2 in LocalizationNetwork\n",
    "        self.localization_fc2.weight.data.fill_(0)\n",
    "        \"\"\" see RARE paper Fig. 6 (a) \"\"\"\n",
    "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
    "        ctrl_pts_y_top = np.linspace(0.0, -1.0, num=int(F / 2))\n",
    "        ctrl_pts_y_bottom = np.linspace(1.0, 0.0, num=int(F / 2))\n",
    "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
    "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
    "        initial_bias = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
    "        self.localization_fc2.bias.data = torch.from_numpy(initial_bias).float().view(-1)\n",
    "\n",
    "    def forward(self, batch_I):\n",
    "        \"\"\"\n",
    "        input:     batch_I : Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
    "        output:    batch_C_prime : Predicted coordinates of fiducial points for input batch [batch_size x F x 2]\n",
    "        \"\"\"\n",
    "        batch_size = batch_I.size(0)\n",
    "        features = self.conv(batch_I).view(batch_size, -1)\n",
    "        batch_C_prime = self.localization_fc2(self.localization_fc1(features)).view(batch_size, self.F, 2)\n",
    "        return batch_C_prime\n",
    "\n",
    "\n",
    "class GridGenerator(nn.Module):\n",
    "    \"\"\" Grid Generator of RARE, which produces P_prime by multipling T with P \"\"\"\n",
    "\n",
    "    def __init__(self, F, I_r_size):\n",
    "        \"\"\" Generate P_hat and inv_delta_C for later \"\"\"\n",
    "        super(GridGenerator, self).__init__()\n",
    "        self.eps = 1e-6\n",
    "        self.I_r_height, self.I_r_width = I_r_size\n",
    "        self.F = F\n",
    "        self.C = self._build_C(self.F)  # F x 2\n",
    "        self.P = self._build_P(self.I_r_width, self.I_r_height)\n",
    "        self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())  # F+3 x F+3\n",
    "        self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())  # n x F+3\n",
    "\n",
    "    def _build_C(self, F):\n",
    "        \"\"\" Return coordinates of fiducial points in I_r; C \"\"\"\n",
    "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
    "        ctrl_pts_y_top = -1 * np.ones(int(F / 2))\n",
    "        ctrl_pts_y_bottom = np.ones(int(F / 2))\n",
    "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
    "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
    "        C = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
    "        return C  # F x 2\n",
    "\n",
    "    def _build_inv_delta_C(self, F, C):\n",
    "        \"\"\" Return inv_delta_C which is needed to calculate T \"\"\"\n",
    "        hat_C = np.zeros((F, F), dtype=float)  # F x F\n",
    "        for i in range(0, F):\n",
    "            for j in range(i, F):\n",
    "                r = np.linalg.norm(C[i] - C[j])\n",
    "                hat_C[i, j] = r\n",
    "                hat_C[j, i] = r\n",
    "        np.fill_diagonal(hat_C, 1)\n",
    "        hat_C = (hat_C ** 2) * np.log(hat_C)\n",
    "        # print(C.shape, hat_C.shape)\n",
    "        delta_C = np.concatenate(  # F+3 x F+3\n",
    "            [\n",
    "                np.concatenate([np.ones((F, 1)), C, hat_C], axis=1),  # F x F+3\n",
    "                np.concatenate([np.zeros((2, 3)), np.transpose(C)], axis=1),  # 2 x F+3\n",
    "                np.concatenate([np.zeros((1, 3)), np.ones((1, F))], axis=1)  # 1 x F+3\n",
    "            ],\n",
    "            axis=0\n",
    "        )\n",
    "        inv_delta_C = np.linalg.inv(delta_C)\n",
    "        return inv_delta_C  # F+3 x F+3\n",
    "\n",
    "    def _build_P(self, I_r_width, I_r_height):\n",
    "        I_r_grid_x = (np.arange(-I_r_width, I_r_width, 2) + 1.0) / I_r_width  # self.I_r_width\n",
    "        I_r_grid_y = (np.arange(-I_r_height, I_r_height, 2) + 1.0) / I_r_height  # self.I_r_height\n",
    "        P = np.stack(  # self.I_r_width x self.I_r_height x 2\n",
    "            np.meshgrid(I_r_grid_x, I_r_grid_y),\n",
    "            axis=2\n",
    "        )\n",
    "        return P.reshape([-1, 2])  # n (= self.I_r_width x self.I_r_height) x 2\n",
    "\n",
    "    def _build_P_hat(self, F, C, P):\n",
    "        n = P.shape[0]  # n (= self.I_r_width x self.I_r_height)\n",
    "        P_tile = np.tile(np.expand_dims(P, axis=1), (1, F, 1))  # n x 2 -> n x 1 x 2 -> n x F x 2\n",
    "        C_tile = np.expand_dims(C, axis=0)  # 1 x F x 2\n",
    "        P_diff = P_tile - C_tile  # n x F x 2\n",
    "        rbf_norm = np.linalg.norm(P_diff, ord=2, axis=2, keepdims=False)  # n x F\n",
    "        rbf = np.multiply(np.square(rbf_norm), np.log(rbf_norm + self.eps))  # n x F\n",
    "        P_hat = np.concatenate([np.ones((n, 1)), P, rbf], axis=1)\n",
    "        return P_hat  # n x F+3\n",
    "\n",
    "    def build_P_prime(self, batch_C_prime):\n",
    "        \"\"\" Generate Grid from batch_C_prime [batch_size x F x 2] \"\"\"\n",
    "        batch_size = batch_C_prime.size(0)\n",
    "        batch_inv_delta_C = self.inv_delta_C.repeat(batch_size, 1, 1)\n",
    "        batch_P_hat = self.P_hat.repeat(batch_size, 1, 1)\n",
    "        batch_C_prime_with_zeros = torch.cat((batch_C_prime, torch.zeros(\n",
    "            batch_size, 3, 2).float().cuda()), dim=1)  # batch_size x F+3 x 2\n",
    "        batch_T = torch.bmm(batch_inv_delta_C, batch_C_prime_with_zeros)  # batch_size x F+3 x 2\n",
    "        batch_P_prime = torch.bmm(batch_P_hat, batch_T)  # batch_size x n x 2\n",
    "        return batch_P_prime  # batch_size x n x 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# char_len resnet + base Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        return self.decode(self.encode(src), tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def decode(self, src, tgt, tgt_mask=None):\n",
    "        return self.decoder(self.tgt_embed(tgt), src, tgt_mask)\n",
    "    \n",
    "    def generate(self, outs):\n",
    "        return self.generator(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CNNBase(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        net = models.resnet34(False)\n",
    "        self.base = nn.Sequential(*list(net.children())[:-3])\n",
    "        self.cLen = nn.Sequential(*list(children(net))[-3:-1], Flatten(), nn.Linear(512,1))\n",
    "        self.linear = nn.Linear(256, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = self.base(x)\n",
    "        out = self.cLen(feats)\n",
    "        feats = feats.flatten(2,3).permute(0,2,1)\n",
    "        feats = self.linear(feats)\n",
    "        return feats,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_full_model(vocab, d_model, N=4, drops=0.2):\n",
    "    c = deepcopy\n",
    "    attn = SingleHeadedAttention(d_model)\n",
    "#     attn = MultiHeadedAttention(d_model, 4)\n",
    "    ff = PositionwiseFeedForward(d_model, drops)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), drops), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), drops), N),\n",
    "        nn.Sequential(\n",
    "            Embeddings(d_model, vocab), PositionalEncoding(d_model, drops, 2000)\n",
    "        ),\n",
    "        nn.Linear(d_model, vocab),\n",
    "    )\n",
    "        \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, img_encoder, transformer, seq_len=500):\n",
    "        super(Img2Seq, self).__init__()\n",
    "        self.img_enc = img_encoder\n",
    "        self.transformer = transformer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, src, tgt=None, tgt_mask=None):\n",
    "        # inference (greedy decode)\n",
    "        if tgt is None:\n",
    "            with torch.no_grad():\n",
    "                feats,cLen = self.transformer.encode(self.img_enc(src))\n",
    "                bs = src.size(0)\n",
    "                tgt = torch.ones((bs,1), dtype=torch.long, device=device)\n",
    "\n",
    "                res = []\n",
    "                for i in progress_bar(range(self.seq_len)):\n",
    "                    mask = subsequent_mask(tgt.size(-1))\n",
    "                    dec_outs = self.transformer.decode(feats, tgt, mask)\n",
    "                    prob = self.transformer.generate(dec_outs[:,-1])\n",
    "                    res.append(prob)\n",
    "                    pred = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "                    if (pred==0).all(): break\n",
    "                    tgt = torch.cat([tgt,pred], dim=-1)\n",
    "                out = torch.stack(res).transpose(1,0).contiguous()\n",
    "                \n",
    "        #training\n",
    "        else:\n",
    "            feats,cLen = self.img_enc(src)\n",
    "            dec_outs = self.transformer(feats, tgt, tgt_mask)    # ([bs, sl, d_model])\n",
    "            out = self.transformer.generate(dec_outs)            # ([bs, sl, vocab])\n",
    "        return (out,cLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred,cLen = pred\n",
    "        pred,targ = loss_prep(pred, target)\n",
    "        pred = F.log_softmax(pred, dim=-1)  # need this for KLDivLoss\n",
    "        true_dist = pred.data.clone()\n",
    "        true_dist.fill_(self.smoothing / pred.size(1))                  # fill with 0.0012\n",
    "        true_dist.scatter_(1, targ.data.unsqueeze(1), self.confidence)  # [0.0012, 0.0012, 0.90, 0.0012]\n",
    "        c_loss = F.kl_div(pred, true_dist, reduction='sum')/bs\n",
    "        l_loss = F.mse_loss(cLen.view(-1), (target != 0).sum(dim=1).float(), reduction='sum')/bs\n",
    "        return c_loss + l_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "class CER(Callback):\n",
    "    def __init__(self, itos):\n",
    "        super().__init__()\n",
    "        self.name = 'cer'\n",
    "        self.itos = itos\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.errors, self.total = 0, 0\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        error,size = self._cer(last_output[0], last_target)\n",
    "        self.errors += error\n",
    "        self.total += size\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        return add_metrics(last_metrics, self.errors/self.total)\n",
    "\n",
    "    def _cer(self, preds, targs):\n",
    "        bs,sl = targs.size()\n",
    "        \n",
    "        res = torch.argmax(preds, dim=2)\n",
    "        error = 0\n",
    "        for i in range(bs):\n",
    "            p = self._char_label_text(res[i])   #.replace(' ', '')\n",
    "            t = self._char_label_text(targs[i]) #.replace(' ', '')\n",
    "            error += Lev.distance(t, p)/len(t)\n",
    "        return error, bs\n",
    "\n",
    "    def _char_label_text(self, pred):\n",
    "#         return self.sp.DecodeIds(pred.tolist())\n",
    "        ints = to_np(pred).astype(int)\n",
    "        nonzero = ints[np.nonzero(ints)]\n",
    "        return ''.join([self.itos[i] for i in nonzero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "\n",
    "img_encoder = CNNBase(d_model)\n",
    "transformer = make_full_model(len(itos), d_model)\n",
    "net = Img2Seq(img_encoder, transformer, seq_len)\n",
    "\n",
    "# AdamW16 = partial(optim.Adam, betas=(0.9,0.99), eps=1e-4)  ->  #.to_fp16(max_scale=256)\n",
    "# partial: way to always call a function with a given set of arguments or keywords\n",
    "\n",
    "learn = Learner(data, net, loss_func=LabelSmoothing(smoothing=0.1),\n",
    "                metrics=[CER(itos)], callback_fns=TeacherForce)\n",
    "learn.clip_grad(0.25)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ResNet for char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# net = models.resnet34(False)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modules = list(net.children())[:-3]\n",
    "m = nn.Sequential(*modules)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m[6][0].downsample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shape = Lambda(lambda x: print(x.shape))\n",
    "\n",
    "for i,layer in enumerate(m):\n",
    "    m[i] = nn.Sequential(shape, layer)\n",
    "#         m[i] = nn.AvgPool2d((2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m[5][3] = nn.AvgPool2d((2,1))\n",
    "# bs, 1920, 4, 32 => [5,7,9][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = models.resnet34(False)\n",
    "m = nn.Sequential(*list(children(net))[:-3], Flatten(), nn.Linear(256,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomMSE(nn.MSELoss):\n",
    "    def forward(self, pred:Tensor, target:Tensor) -> Rank0Tensor:\n",
    "        lens = (target != 0).sum(dim=1).float()  # num of nonzero elements (char_len)\n",
    "        return super().forward(pred.view(-1), lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = Learner(data, m, loss_func=CustomMSE())\n",
    "l.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sd = l.model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CNNBase(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        net = models.resnet34(False)\n",
    "        modules = list(net.children())[:-3]\n",
    "        modules[3] = nn.MaxPool2d(3)\n",
    "        self.base = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(256, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(2,3).permute(0,2,1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-3)\n",
    "\n",
    "# 33.423637\t28.382051\t0.238357\t03:43   baseline - no modifications (gelu, multi(4), pre-resnet34)\n",
    "# 33.606434\t28.509262\t0.684435\t05:25   modified CER  ...something wrong...\n",
    "# 33.010765\t27.659348\t0.399081\t03:41   remove rshift in TeacherForcing\n",
    "# 33.992241\t28.652754\t0.412570\t03:36   \"\"\n",
    "# 35.508675\t30.228336\t0.305193\t03:37   keep rshift in TF; remove bos token from CharTokenizer\n",
    "\n",
    "# 36.611359\t30.429012\t0.305808\t03:34   single attn\n",
    "# 37.610634\t31.228436\t0.395280\t03:27   \"\", remove eos from CER\n",
    "# 37.335598\t31.324074\t0.325508\t03:36   multi(1)\n",
    "# 34.636471\t29.268423\t0.292452\t03:43   multi(4)\n",
    "# 33.925339\t28.797222\t0.285785\t03:42   multi(8)\n",
    "# 34.031483\t29.092997\t0.361159\t03:37   \"\", remove eos from CER\n",
    "# 35.286541\t30.412006\t0.306993\t03:50   multi(16)\n",
    "# 36.340981\t31.675489\t0.326111\t04:04   multi(32)\n",
    "\n",
    "# sm_synth, sz:128, bs:100\n",
    "# 42.141075\t37.683475\t0.463989\t03:35   multi(8)\n",
    "# 41.568932\t36.599640\t0.440583\t03:32   multi(8); add PE to flattened feats\n",
    "\n",
    "#   (3cycle, 1e-3)\n",
    "# 21.168943\t20.402149\t0.221798\t03:34   multi(8)\n",
    "# valid:     17.9091   22.6781\n",
    "# greedy:    179.729   27.0221\n",
    "# 22.707590\t20.752419\t0.227430\t03:36   multi(8); add PE to flattened feats\n",
    "# valid:     18.6755   24.6373\n",
    "# greedy:    174.812   25.5949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load('8head512mix')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, max_lr=1.58e-6, callbacks=[SaveModelCallback(learn, name='8head512mix2')])\n",
    "# 3 cycles; 1e-3\n",
    "### sm_synth, sz:128, bs:100\n",
    "# 15.269251\t13.785835\t0.106348   base arch - (gelu, single attn, pre-resnet34)\n",
    "# 15.385436\t13.587833\t0.104421   base arch (multi attn in encoder, single attn in decoder)\n",
    "# 14.136460\t12.912922\t0.097931   \"\", multi attn (no tfms)\n",
    "# 16.347567\t14.436943\t0.111617   \"\", reversed images - doesn't perform well from pretraining???\n",
    "# 23.472956\t19.903946\t0.164833   normal images, resnet34 (no pretraining)\n",
    "# 25.128386\t21.791134\t0.185044   reversed_images, resnet34 (no pretraining) - initialization???\n",
    "# --normal images (black on white) function better as inputs regardless of pre-training\n",
    "# 31.361557\t27.413263\t0.241301   densenet201 (no cut - bs,1920,4,4) (no pretraining)\n",
    "# 29.163410\t25.511379\t0.222596   densenet201 ([:-3] - bs,1792,8,8) (no pretraining)\n",
    "# 45.432755\t41.811367\t0.416656   resnet50 ([:-4] - bs,512,16,16; add BatchNorm2d) (no pretraining)\n",
    "# 39.219181\t35.872307\t0.331677   resnet34(bs,256,8,8); add BatchNorm2d (no pretraining)\n",
    "# --batchnorm doesn't help before final linear\n",
    "# --resnet34 has best results and speed\n",
    "# 28.031528\t24.709288\t0.217898   resnet34 (no pretraining)\n",
    "# 24.876863\t21.345222\t0.181399   \"\" * 8\n",
    "\n",
    "# Char_len, char\n",
    "# 54.786438\t46.906456\t0.410596   full resnet -> char_len; [:-3] -> feats\n",
    "# 48.422413\t41.097393\t0.353167   2nd run\n",
    "\n",
    "# DeformableConv Layers\n",
    "# 14.071175\t12.978416\t0.098672\t05:00   4.2, 5.3, 6.5, multi(4), gelu, pretrained\n",
    "# --no improvement, much slower\n",
    "\n",
    "# 7.596963\t7.282169\t0.054113\t07:24   remove base[3] (maxpool) => 16x16 features\n",
    "# 8.450560\t8.299768\t0.060255\t04:53   remove base[3] (maxpool); em_sz:512 (add 512 layers) => 8x8 features\n",
    "# --additional computation layer seems to improve performance over same features w/out\n",
    "# ...but not as much as 16x16 feat map output\n",
    "\n",
    "# modify stride in convs; stride(2,1) => 8x16 feature maps\n",
    "# 13.378371\t12.241001\t0.094132\t04:29   base[6][0], multi(4), gelu, pretrained\n",
    "# 11.961518\t10.865971\t0.081814\t04:34   base[5][0]\n",
    "# 9.808911\t9.199651\t0.068632\t04:44   base[3]\n",
    "# 10.367452\t9.416987\t0.070494\t04:44   base[0]\n",
    "\n",
    "\n",
    "### sm_synth, sz:256, bs:100\n",
    "# 5.859333\t5.713445\t0.043088   base, multi-attn(4), gelu, pretrained    'v1_multi_gelu_256'    ****\n",
    "# --pretraining on smaller sizes does not improve performance\n",
    "# --16x16 feature map gives better results -> finer grained attn??\n",
    "\n",
    "# 5.635329\t5.135679\t0.043417\t29:09    bs: 20, remove base[3] (maxpool) => 32x32 features\n",
    "# --potential slight improvement but much slower!!\n",
    "\n",
    "### mix(10k), sz:512, bs:10, lr:1e-4\n",
    "# 219.762802\t215.301453\t0.540762\t08:16   base, multi-attn(4), gelu, 32x32 features\n",
    "#    multi-attn(8)\n",
    "# 256.292297\t230.259369\t0.665115\t12:14(1st cycle)   multi-attn(16), bs: 5\n",
    "# 206.822525\t217.673355\t0.555488\t07:08   base, single-attn, bs: 15\n",
    "\n",
    "# 203.974548\t216.947418\t0.561815\t21:56(2nd cycle)   sz:800, bs:5, base, multi-attn(4), 50x50 features\n",
    "\n",
    "### edited_fonts(74k) - base, multi(4), gelu, preload 'v1_multi_gelu_256'\n",
    "# sz:256, bs: 20, lr:1e-4\n",
    "# 234.073456\t210.412842\t0.207377\t39:16   resize: squish     'v1_multi_gelu_256_fonts'\n",
    "# 389.043976\t349.327209\t0.410615\t40:07   resize: pad/border   'best256fonts'\n",
    "# sz:400, bs: 10, lr:2e-5\n",
    "# sz:512, bs: 10, lr:2e-5\n",
    "# 39.1337090\t33.233494\t0.029211\t1:14:35   'v1_multi_gelu_512_fonts'\n",
    "# 38.2521550\t32.473492\t0.028432\t1:13:44    2nd run,  lr: 1e-6  'best512fonts'\n",
    "\n",
    "\n",
    "# sz:512, bs: 10, lr:1e-4, 8head, no preload, remove bos from CharTokenizer, remove eos from CER\n",
    "# 56.5093610\t44.473030\t0.039634\t1:27:05  *** '8head512fonts' \n",
    "# valid:     22.1713   0.02428\n",
    "# greedy:    572.330   0.03317\n",
    "# mixed data\n",
    "# 35.9120830\t32.567410\t0.088160\t59:11    *** '8head512mix'\n",
    "# valid:     4.80507   0.15863\n",
    "# greedy:    23.6504   0.12724\n",
    "# test:      2282.64   0.13363\n",
    "# 41.1803780\t32.238232\t0.087566\t57:54   added tfms, preload above, lr: 1.58e-6, 2cycle   ***'8head512mix2'\n",
    "\n",
    "\n",
    "### Training\n",
    "# 128, 100\n",
    "# 14.136460\t12.912922\t0.097931   \"\", multi attn (no tfms)\n",
    "# 8.069926\t8.673322\t0.064796   2nd run    *** 'v1_multi_gelu_128'\n",
    "# 256, 100\n",
    "# 5.880878\t5.727545\t0.043746   \n",
    "# 2.970708\t3.868662\t0.029560   2nd run\n",
    "\n",
    "### Training - Edited Data - GeLU, singlehead attn\n",
    "# sm\n",
    "# 10.841042\t9.550449\t0.071475   sz:128, bs:100, lr:1e-3  'v1_gelu_128'\n",
    "# 2.631370\t2.669811\t0.020308   sz:256, bs:100, lr:1e-3  'v1_gelu_256'\n",
    "# mix\n",
    "# 18.652050\t14.330537\t0.021167   sz:400, bs:20, lr:1e-3   'v1_edited_gelu_400'\n",
    "# 11.513125\t8.577786\t0.016521   sz:512, bs:15, lr:5e-5, 3cycle   'v1_edited_gelu_512'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None, title=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(image2np(im.data), alpha=alpha)\n",
    "    if title: ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def full_test(learn, sl, dl=data.valid_dl, batches=20):\n",
    "    learn.model.eval()\n",
    "    iterable = iter(dl)\n",
    "    g_loss,g_cer=0,0\n",
    "    if batches is None:\n",
    "        batches = len(dl.dl.dataset)//bs\n",
    "    for i in progress_bar(range(batches)):\n",
    "        x,y = next(iterable)\n",
    "        g_preds = learn.model(x, seq_len=sl)\n",
    "        g_res = torch.argmax(g_preds, dim=-1)\n",
    "        g = [learn.loss_func(g_preds, y).item()/bs, cer(g_preds, y)[0]/bs]\n",
    "        g_loss+=g[0]\n",
    "        g_cer+=g[1]\n",
    "    return [g_loss/batches, g_cer/batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = full_test(learn, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'greedy:    {str(g[0])[:7]}   {str(g[1])[1:7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# losses = np.array([learn.loss_func(g_preds[i:i+1],y[i:i+1]).item() for i in range(bs)])\n",
    "# cers = np.array([cer(g_preds[i:i+1],y[i:i+1])[0] for i in range(bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(learn.data.train_dl))\n",
    "\n",
    "g_preds = learn.model(x, seq_len=20)\n",
    "g_res = torch.argmax(g_preds, dim=-1)\n",
    "g = [learn.loss_func(g_preds, y).item()/bs, cer(g_preds, y)[0]/bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#greedy\n",
    "fig, axes = plt.subplots(2,3, gridspec_kw={'hspace': 0.4}, figsize=(18, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    p = char_label_text(g_res[i], sep=' ')\n",
    "    ax=show_img(x[i], ax=ax, title=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(learn.data.train_dl))\n",
    "\n",
    "g_preds = learn.model(x, seq_len=seq_len)\n",
    "g_res = torch.argmax(g_preds, dim=-1)\n",
    "g = [learn.loss_func(g_preds, y).item()/bs, cer(g_preds, y)[0]/bs]\n",
    "\n",
    "print(f'  test:    {str(g[0])[:7]}   {str(g[1])[1:7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "fig, axes = plt.subplots(2,2, gridspec_kw={'hspace': 0.4}, figsize=(18, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    #i +=8\n",
    "    p = char_label_text(g_res[i])\n",
    "    ax=show_img(x[i], ax=ax, title=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {}
   },
   "source": [
    "## Single Test Image (cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xs,ys = next(iter(learn.data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "char_label_text(ys[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 8\n",
    "x = xs[i][None]\n",
    "y = ys[i][None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g_preds = learn.model(x, seq_len=seq_len)\n",
    "g_res = torch.argmax(g_preds, dim=-1)\n",
    "g = [learn.loss_func(g_preds, y).item(), cer(g_preds, y)[0]]\n",
    "\n",
    "print(f'  test:    {str(g[0])[:7]}   {str(g[1])[1:7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = char_label_text(g_res[0])\n",
    "show_img(x[0], figsize=(18,10), title=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "im = PATH/'uploads'/'test2.png'\n",
    "img = open_image(im)\n",
    "prediction = learn.predict(img)[0]\n",
    "show_img(img, title=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def cer(preds, targs):\n",
    "#     bs = targs.size(0)\n",
    "#     res = torch.argmax(preds, dim=-1)\n",
    "#     error = 0\n",
    "#     for i in range(bs):\n",
    "#         p = char_label_text(res[i])   #.replace(' ', '')\n",
    "#         t = char_label_text(targs[i]) #.replace(' ', '')\n",
    "#         error += Lev.distance(t, p)/len(t)\n",
    "#     return error\n",
    "\n",
    "# def char_label_text(pred):\n",
    "#     ints = to_np(pred).astype(int)\n",
    "#     nonzero = ints[np.nonzero(ints)] #[:-1]  #remove bos/eos token\n",
    "#     return ''.join([itos[i] for i in nonzero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def self_attn(layer=-1): return learn.model.transformer.decoder.layers[layer].self_attn.attn.data.cpu()\n",
    "def source_attn(layer=-1): return learn.model.transformer.decoder.layers[layer].src_attn.attn.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None, title=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    if im.shape[0] == 3: im = image2np(im.data)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    if title: ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(learn.data.valid_dl))\n",
    "# x,y = learn.data.one_batch(ds_type=DatasetType.Train)\n",
    "imgs = x #learn.data.denorm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Uploaded Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def thresh_edit(fname, thresh=100, bg=245):\n",
    "    im = Image.open(fname).convert('L')  #grayscale\n",
    "    np_im = np.array(im)\n",
    "    im.close()\n",
    "    thresh_mask = np_im > thresh\n",
    "    np_im[thresh_mask] = bg\n",
    "    return Image.fromarray(np_im, 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "im = thresh_edit(PATH/'test3.png')\n",
    "show_img(im, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "e = 'edit_'+str(fname.name)\n",
    "edited_fname = PATH/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "edited_im.save(edited_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = PATH/'test/edit_test4.png'\n",
    "im = open_image(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seq,res,preds = learn.predict(im)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# r = torch.tensor([g_res], dtype=torch.long, device=device)\n",
    "truth = \"This is a test letter. I hope this\\nworks but I'm not sure it will.\\nMy handwriting is not very good.\"\n",
    "Lev.distance(truth, str(seq))/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x,y = next(v_dl)\n",
    "# imgs = denorm(x)\n",
    "\n",
    "learn.model.eval()\n",
    "\n",
    "shifted_y = rshift(y).long()\n",
    "tgt_mask = subsequent_mask(shifted_y.size(-1))\n",
    "v_preds = learn.model(x, shifted_y, tgt_mask)\n",
    "v_res = torch.argmax(v_preds, dim=-1)\n",
    "v_attn = source_attn()\n",
    "\n",
    "g_preds = learn.model(x)\n",
    "g_res = torch.argmax(g_preds, dim=-1)\n",
    "g_attn = source_attn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v = [learn.loss_func(v_preds, y).item(), cer(v_preds, y)]\n",
    "print(f'valid:     {str(v[0])[:7]}   {str(v[1][0])[1:7]}')\n",
    "\n",
    "g = [learn.loss_func(g_preds, y).item(), cer(g_preds, y)]\n",
    "print(f'greedy:    {str(g[0])[:7]}   {str(g[1][0])[1:7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#valid\n",
    "fig, axes = plt.subplots(1,3, gridspec_kw={'hspace': 0.4}, figsize=(20, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    p = char_label_text(v_res[i])\n",
    "    ax=show_img(imgs[i], ax=ax, title=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#greedy\n",
    "fig, axes = plt.subplots(1,3, gridspec_kw={'hspace': 0.4}, figsize=(18, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    p = char_label_text(g_res[i])\n",
    "    ax=show_img(imgs[i], ax=ax, title=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data = (ImageList.from_df(test_df, path=PATH, folder=TEST_FOLDER)\n",
    "             .split_none()\n",
    "             .label_from_df(label_cls=TextList, sep='', pad_idx=0, vocab=vocab, processor=procs)\n",
    "             .transform([], size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "             .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "             .normalize()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tx,ty = next(iter(test_data.train_dl))\n",
    "t_imgs = test_data.denorm(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_preds = learn.model(tx)\n",
    "t_res = torch.argmax(t_preds, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = [learn.loss_func(t_preds, ty).item(), cer(t_preds, ty)]\n",
    "print(f'test:      {str(g[0])[:7]}   {str(g[1])[:7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "fig, axes = plt.subplots(1,3, gridspec_kw={'hspace': 0.4}, figsize=(18, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    p = char_label_text(t_res[i])\n",
    "    ax=show_img(t_imgs[i], ax=ax, title=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tfmr full\n",
    "#             loss       cer       acc\n",
    "# valid:     2.42856   0.02981   0.98177   3x1, 256/60, 'tfmr_full_3x1_single_attn'\n",
    "# greedy:    11.9438   0.02745   0.93385\n",
    "\n",
    "# valid:     1.98831   0.01858   0.98505   3x1, 256/60, 'exp_3x1_256'\n",
    "# greedy:    16.3832   0.02167   0.90944\n",
    "\n",
    "# valid:     1.47604   0.00797   0.99523   3x2, 400/45,  'tfmr_full_3x2'\n",
    "# greedy:    21.4682   0.01097   0.93762\n",
    "\n",
    "# valid:     7.30494   0.01206   0.99086   lg,  512/30,  'tfmr_full_lg'\n",
    "# greedy:    434.610   0.02398   0.69553\n",
    "\n",
    "# valid:     25.4131   0.04500   0.96773   lg, 512/30,   'tfmr_lg_LM_mixer'\n",
    "# greedy:    734.834   0.15256   0.48912\n",
    "\n",
    "# valid:     10.1481   0.00501   0.99602   cat9-12,  800/8,  'tfmr_cat9-12_full_800'\n",
    "# greedy:    1330.63   0.04525   0.53181\n",
    "\n",
    "# valid:     62.0793   0.05716   0.96214   pg,  512/20  'tfmr_full_paragraph'  (cpu)\n",
    "# greedy:    1739.62   0.08347   0.43309\n",
    "# beam:                0.07958\n",
    "# valid:     71.9120   0.06819   0.95457   \"\", 2nd batch (gpu)\n",
    "# greedy:    2027.24   0.11823   0.39238\n",
    "# beam:                0.10697\n",
    "# valid:     36.3009   0.03397   0.97633   \"\", 3rd batch (gpu)\n",
    "# greedy:    1759.59   0.07070   0.40949\n",
    "    \n",
    "# valid:     50.1414   0.04137   0.97004   pg,  1000/5,  'tfmr_pg_1000'\n",
    "# greedy:    2579.24   0.34178   0.18623\n",
    "\n",
    "# valid:     56.2722   0.04167   0.96923   pg,  1000/5,  'tfmr_catpg_1000'\n",
    "# greedy:    2432.54   0.37192   0.26174\n",
    "\n",
    "\n",
    "# valid:     3.43745   0.05444   0.98735   mix(new)  'tfmr_mix_words_400'\n",
    "# greedy:    43.6545   0.06126   0.91407\n",
    "\n",
    "# valid:     2.96891   0.03338   0.98932   mix(new)  'tfmr_mix_words_512'\n",
    "# greedy:    28.9982   0.03949   0.94500\n",
    "# valid:     2.02411   0.02128   0.99302   5 more cycles\n",
    "# greedy:    19.4735   0.02450   0.96104\n",
    "\n",
    "# valid:     41.9800   0.03879   0.97535   pg, 'tfmr_mix_words_512'\n",
    "# greedy:    1602.68   0.06259   0.49110\n",
    "\n",
    "\n",
    "# valid:     53.9049   0.04822   0.96622   pg, 'tfmr_8head_512_mix'\n",
    "# greedy:    1384.72   0.06304   0.51696\n",
    "\n",
    "\n",
    "# valid:     5.24461   0.00634   mix   'v1_gelu_512'\n",
    "# greedy:    217.438   0.02028\n",
    "# valid:     24.3609   0.02405   pg\n",
    "# greedy:    1935.30   0.05970\n",
    "\n",
    "# valid:     12.7074   0.00954   mix   'v1_gelu_512_wiki103_base_lm_last_layer'\n",
    "# greedy:    553.201   0.02814\n",
    "\n",
    "# valid:     11.4251   0.00474   mix   'v1_gelu_512_wiki103_base_lm_full'\n",
    "# greedy:    358.030   0.01187\n",
    "\n",
    "# valid:     12.1104   0.01533   mix   'v1_gelu_multi_512'\n",
    "# greedy:    506.505   0.04229\n",
    "\n",
    "\n",
    "# edited data\n",
    "# valid:     8.21437   0.06576   sm   'v1_gelu_128'\n",
    "# greedy:    185.981   0.12890\n",
    "\n",
    "# valid:     2.25060   0.01731   sm   'v1_gelu_256'\n",
    "# greedy:    149.525   0.06003\n",
    "\n",
    "# valid:     2.37294   0.01383   mix  'v1_edited_gelu_400'\n",
    "# greedy:    30.3637   0.01732\n",
    "# greedy:    1810.14   0.11409   test\n",
    "\n",
    "# valid:     9.09986   0.01656   mix  'v1_edited_gelu_512'\n",
    "# greedy:    529.308   0.03104\n",
    "# greedy:    1675.20   0.10819   test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Multi-Batch Beam Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def beam_cer(preds, targs):\n",
    "    bs = preds.size(0)\n",
    "    error = 0\n",
    "    for i in range(bs):\n",
    "        p = char_label_text(preds[i])\n",
    "        t = char_label_text(targs[i])\n",
    "        error += _cer(t,p)\n",
    "    return error/bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def repeat_interleave(tensor,n):\n",
    "    res = []\n",
    "    for i in range(tensor.size(0)):\n",
    "        for _ in range(n): res.append(tensor[i])\n",
    "    return torch.stack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def normalize_score(score, i, alpha=0.6):\n",
    "#     length_penalty = math.pow((5 + i)/6, alpha)\n",
    "#     return score/length_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Beam Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class Beam(object):\n",
    "    def __init__(self, beam_width=10):\n",
    "        self.heap = list()\n",
    "        self.beam_width = beam_width\n",
    "        self.best_score = None\n",
    "\n",
    "    def add(self, score, complete, seq):        \n",
    "        heapq.heappush(self.heap, (score, complete, seq))\n",
    "        if len(self.heap) > self.beam_width:\n",
    "            heapq.heappop(self.heap)\n",
    "            \n",
    "    def get_seq(self):\n",
    "        return [b[-1] for b in self.heap]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self.heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    def __init__(self, net, beam_width, seq_len, end_tok=0):\n",
    "        super(BeamSearch, self).__init__()\n",
    "        self.img_enc = net.img_enc\n",
    "        self.transformer = net.transformer\n",
    "        self.bw = beam_width\n",
    "        self.seq_len = seq_len\n",
    "        self.end_tok = end_tok\n",
    "        self.feats = None\n",
    "        self.beams = []\n",
    "            \n",
    "    def forward(self, src):\n",
    "        with torch.no_grad():\n",
    "            bs = src.size(0)\n",
    "            \n",
    "            # initialize beam per bs\n",
    "            for _ in range(bs):\n",
    "                beam = Beam(self.bw)\n",
    "                beam.add(0.0, False, [1])\n",
    "                self.beams.append(beam)\n",
    "                \n",
    "            # encode src\n",
    "            self.feats = self.transformer.encode(self.img_enc(src))\n",
    "            \n",
    "            for i in tqdm(range(seq_len)):\n",
    "                # gather sequences from beams; combine into tensor (bs*bw)\n",
    "                prev_seq = torch.from_numpy(np.stack([b.get_seq() for b in self.beams])).view(-1,i+1)\n",
    "                \n",
    "                # generate new possibilities\n",
    "                log_probs, chars = self.prob_func(prev_seq.to(device))\n",
    "                log_probs, chars = log_probs.view(bs,-1,self.bw), chars.view(bs,-1,self.bw)\n",
    "                \n",
    "                for j in range(bs):\n",
    "                    curr_beam = Beam(self.bw)\n",
    "                    for k,(score, complete, seq) in enumerate(self.beams[j]):\n",
    "                        for l,c in zip(log_probs[j,k],chars[j,k]):\n",
    "                            log_prob,char = l.item(), c.item()\n",
    "                            curr_beam.add((score+log_prob), (char==self.end_tok), seq+[char])\n",
    "                    self.beams[j] = curr_beam\n",
    "  \n",
    "                # return if all max beams are complete\n",
    "                if (self.top_complete()==True).all(): break\n",
    "                    \n",
    "                # expand feats to match beam size (only on 2nd run)\n",
    "                if i==0: self.feats = repeat_interleave(self.feats, self.bw)\n",
    "\n",
    "            return self.top_seq()\n",
    "\n",
    "    def top_complete(self): return np.stack([max(b)[1] for b in self.beams])\n",
    "    def top_seq(self): return torch.from_numpy(np.stack([max(b)[-1] for b in self.beams]))[:,1:]\n",
    "\n",
    "    def prob_func(self, tgt):\n",
    "        mask = subsequent_mask(tgt.size(-1))\n",
    "        dec_outs = self.transformer.decode(self.feats, tgt, mask)\n",
    "        logits = self.transformer.generate(dec_outs[:,-1])\n",
    "        log_probs = logits - torch.logsumexp(logits, -1, keepdim=True) # more stable than F.softmax(logits,-1).log()\n",
    "        return torch.topk(log_probs, self.bw, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search = BeamSearch(learn.model, 3, seq_len)\n",
    "b_res = search(x)\n",
    "\n",
    "# bw: 3, sl: 350, ~56s, 0.02491    # no normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "beam_cer(b_res,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    def __init__(self, net, beam_width, seq_len):\n",
    "        super(BeamSearch, self).__init__()\n",
    "        self.img_enc = net.img_enc\n",
    "        self.transformer = net.transformer\n",
    "        self.bw = beam_width\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.feats = None\n",
    "        self.beam = None\n",
    "        self.scores = None\n",
    "        \n",
    "        net.eval()\n",
    "    \n",
    "    def forward(self, src):\n",
    "        with torch.no_grad():\n",
    "            bs = src.size(0)\n",
    "            \n",
    "            # encode src\n",
    "            self.feats = self.transformer.encode(self.img_enc(src))\n",
    "            \n",
    "            # initialize globals (beam=1 for first iteration; 3 thereafter)\n",
    "            self.beam = torch.ones((bs,1), device=device, dtype=torch.long)\n",
    "            self.scores = torch.zeros((bs,1), device=device, dtype=torch.float)\n",
    "            \n",
    "            for i in tqdm(range(seq_len)):\n",
    "                # generate new topk chars per beam(bs*bw)\n",
    "                log_probs, chars = self.prob_func(self.beam)  #(bs*beam, 3)\n",
    "\n",
    "                # compute local scores\n",
    "                scores = self.scores + log_probs\n",
    "                \n",
    "                # compute new beams per batch\n",
    "                new_scores, idxs = torch.topk(scores.view(bs,-1), self.bw, dim=-1) #(bs, 3)\n",
    "                self.scores = new_scores.view(-1,1)\n",
    "                \n",
    "                # set up new beam:\n",
    "                nxt = torch.stack([c[i] for c,i in zip(chars.view(bs,-1),idxs)]).view(-1,1)\n",
    "                pre = torch.stack([b[i//self.bw] for b,i in zip(self.beam.view(bs,-1,i+1),idxs)]).view(-1,i+1)\n",
    "                                    \n",
    "                # update globals\n",
    "                self.beam = torch.cat([pre,nxt], dim=1)\n",
    "\n",
    "                # end when top of beams are complete\n",
    "                if self.top_complete(): break\n",
    "                                                        \n",
    "                # expand feats to match beam size (only on 2nd run)\n",
    "                if i==0: self.feats = repeat_interleave(self.feats, self.bw) #.repeat(self.bw,1,1)\n",
    "                    \n",
    "            return self.top_sequences(), self.top_scores() \n",
    "\n",
    "\n",
    "    def top_complete(self): return (self.top_sequences()[:,-1]==0).all().item()   #byte tensor\n",
    "    def top_sequences(self): return self.beam.squeeze()[0::self.bw][:,1:]\n",
    "    def top_scores(self): return self.scores.squeeze()[0::self.bw]\n",
    "\n",
    "    def prob_func(self, tgt):\n",
    "        mask = subsequent_mask(tgt.size(-1))\n",
    "        dec_outs = self.transformer.decode(self.feats, tgt, mask)\n",
    "        logits = self.transformer.generate(dec_outs[:,-1])\n",
    "        log_probs = logits - torch.logsumexp(logits, -1, keepdim=True) # more stable than F.softmax(logits,-1).log()\n",
    "        return torch.topk(log_probs, self.bw, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search = BeamSearch(learn.model, 3, seq_len)\n",
    "b_res,score = search(x)\n",
    "\n",
    "# lg, sl: 250\n",
    "# bw: 1, ~12s, 0.02398\n",
    "# bw: 3, ~28s, 0.02378\n",
    "# bw: 5, ~46s, 0.02378\n",
    "\n",
    "# pg: 1000,5\n",
    "# bs: 3, ~40s, 0.33227  (greedy: 0.28---)\n",
    "# ''   , ~30s, 0.22635  (greedy: 0.23726)\n",
    "\n",
    "# pg: 800,8\n",
    "# bs: 3, ~51s, 0.25424  (greedy: 0.28348)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "beam_cer(b_res,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#beam\n",
    "fig, axes = plt.subplots(1,3, gridspec_kw={'hspace': 0.4}, figsize=(20, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    p = char_label_text(b_res[i], chunk=55)\n",
    "    ax=show_img(imgs[i], ax=ax, title=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Single Beam Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html\n",
    "\n",
    "import heapq\n",
    "\n",
    "class Beam(object):\n",
    "    '''\n",
    "    For comparison of prefixes, the tuple (prefix_probability, complete_sentence) is used.\n",
    "    This is so that if two prefixes have equal probabilities then a complete sentence\n",
    "    is preferred over an incomplete one since (0.5, False) < (0.5, True)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, beam_width=10):\n",
    "        self.heap = list()\n",
    "        self.beam_width = beam_width\n",
    "        self.best_score = None\n",
    "\n",
    "    def add(self, score, complete, seq):        \n",
    "        # keep track of best_score so far\n",
    "        if self.best_score is None or score > self.best_score:\n",
    "            self.best_score = score\n",
    "            \n",
    "        # only add to beam if score is not more than beam_width below the best_score\n",
    "        if score > self.best_score-self.beam_width:\n",
    "            heapq.heappush(self.heap, (score, complete, seq))\n",
    "            \n",
    "        # maintain beam_width\n",
    "        if len(self.heap) > self.beam_width:\n",
    "            heapq.heappop(self.heap)\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return iter(self.heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def beamsearch(prob_fn, seq_len, beam_width=5, start_tok=1, end_tok=3):\n",
    "    prev_beam = Beam(beam_width)\n",
    "    prev_beam.add(0.0, False, [start_tok])\n",
    "    \n",
    "    for i in tqdm(range(seq_len)):\n",
    "        curr_beam = Beam(beam_width)\n",
    "        \n",
    "        # iterate over each beam\n",
    "        for (score, complete, seq) in prev_beam:\n",
    "            if complete == True:\n",
    "                None  # only keep the completed best beam!!\n",
    "#                 curr_beam.add(score, True, seq)\n",
    "            else:\n",
    "                # iterate through topk chars, calculating scores and adding to the beam.\n",
    "                log_probs, chars = prob_fn(seq)\n",
    "                for log_prob, char in zip(log_probs, chars): \n",
    "                    log_prob,char = log_prob.item(), char.item()\n",
    "                    score += log_prob   #log probabilities are additive\n",
    "#                     score = score_func(score, len(seq))\n",
    "                    curr_beam.add(score, (char==end_tok), seq+[char])\n",
    "        \n",
    "        (best_score, best_complete, best_seq) = max(curr_beam)\n",
    "        if best_complete == True: return (best_seq[1:], best_score)   # returns first complete beam not best...\n",
    "            \n",
    "        prev_beam = curr_beam\n",
    "        \n",
    "    (best_score, best_complete, best_seq) = max(curr_beam)\n",
    "    return (best_seq[1:], best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def beam_decode(net, src, beam_width, seq_len):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        feats = net.transformer.encode(net.img_enc(src))        \n",
    "        return beamsearch(partial(prob_func, net=net, feats=feats, beam_width=beam_width), seq_len, beam_width)\n",
    "    \n",
    "def prob_func(tgt, net=None, feats=None, beam_width=5):\n",
    "    tgt = torch.tensor([tgt], dtype=torch.long, device=device)\n",
    "    mask = subsequent_mask(tgt.size(-1))\n",
    "    dec_outs = net.transformer.decode(feats, tgt, mask)\n",
    "    logits = net.transformer.generate(dec_outs[:,-1])\n",
    "    \n",
    "    log_probs = logits - torch.logsumexp(logits, 1)  # more numerically stable\n",
    "    # log_probs = F.softmax(logits, -1).log()\n",
    "    \n",
    "    return torch.topk(log_probs.squeeze(0), beam_width, dim=-1)\n",
    "#     return zip(res[0][0].detach(),res[1][0].detach())\n",
    "\n",
    "def score_func(log_probs, i, alpha=0.6):\n",
    "    length_penalty = math.pow((5 + i)/6, alpha)\n",
    "    return log_probs/length_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "x1 = x[idx][None]\n",
    "y1 = y[idx][None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b_res, score = beam_decode(learn.model, image, 3, seq_len)    #294, 3m18s\n",
    "# 294 - 1m40s\n",
    "# 294 - 1m45s (w/ score_func)\n",
    "# 295 - 22s (beam_width=1 ~ greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = torch.tensor([b_res], dtype=torch.long, device=device)\n",
    "p = char_label_text(r)\n",
    "\n",
    "_cer(truth, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# valid\n",
    "p = char_label_text(v_res[idx][None])\n",
    "t = char_label_text(y1[0])\n",
    "_cer(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# greedy\n",
    "p = char_label_text(g_res[idx][None])\n",
    "t = char_label_text(y1[0])\n",
    "_cer(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# beam (sz=3)\n",
    "r = torch.tensor([b_res], dtype=torch.long, device=device)\n",
    "p = char_label_text(r)\n",
    "t = char_label_text(y1[0])\n",
    "_cer(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stoi = {k:i for i,k in enumerate(itos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(char_label_text(g_res[idx][None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "st = ''.join([itos[i] for i in b_res])\n",
    "p = '\\n'.join(textwrap.wrap(st, 70))\n",
    "show_img(denorm(x1)[0], figsize=(10,10), title=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Source Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "img = imgs[idx]\n",
    "\n",
    "v_chars = v_res[idx]\n",
    "v_attns = to_np(torch_scale_attns(v_attn)[idx])\n",
    "\n",
    "g_chars = g_res[idx]\n",
    "g_attns = to_np(torch_scale_attns(g_attn)[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#valid\n",
    "fig, axes = plt.subplots(5,4, gridspec_kw={'hspace': 0.3}, figsize=(20, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    a = g_filter(vv_attns[i])\n",
    "    ax.imshow(img, alpha=None)\n",
    "    ax.imshow(a, cmap='Blues', interpolation='nearest', alpha=0.3)\n",
    "    ax.set_title(itos[v_chars[i].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#greedy\n",
    "fig, axes = plt.subplots(6,4, gridspec_kw={'hspace': 0.3}, figsize=(20, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    a = g_filter(g_attns[i])\n",
    "    ax.imshow(img, alpha=None)\n",
    "    ax.imshow(a, cmap='Blues', interpolation='nearest', alpha=0.3)\n",
    "    ax.set_title(itos[g_chars[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "k=16\n",
    "\n",
    "def torch_scale_attns(attns):\n",
    "    bs,sl,hw = attns.shape\n",
    "    num = int(math.sqrt(hw))   # sz // k\n",
    "    mod = attns.view(bs,sl,num,num)\n",
    "    scaled = F.interpolate(mod, size=sz)\n",
    "    return scaled  #([bs, sl, h, w])\n",
    "\n",
    "def g_filter(att):\n",
    "    return gaussian_filter(att, sigma=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def self_attn(layer=-1): return learn.model.transformer.decoder.layers[layer].self_attn.attn.data.cpu()\n",
    "def source_attn(layer=-1): return learn.model.transformer.decoder.layers[layer].src_attn.attn.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transparent_cmap(cmap, N=5):\n",
    "    \"Copy colormap and set alpha values\"\n",
    "    mycmap = plt.cm.get_cmap(cmap, N)\n",
    "    mycmap._init()\n",
    "    mycmap._lut[:,-1] = np.linspace(0, 0.6, N+3)\n",
    "    return mycmap\n",
    "\n",
    "#Use base cmap to create transparent\n",
    "# mycmap = transparent_cmap(plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_attn(img, attns, chars, ax, color, showChars=True):\n",
    "    for i in range(attns.shape[0]):\n",
    "        c = chars[i].item()\n",
    "        if c not in [0,1,2,3]:\n",
    "            a = g_filter(attns[i])\n",
    "            y,x = scipy.ndimage.center_of_mass(a)\n",
    "            #sns.heatmap(a, cmap=mycmap, cbar=False, ax=ax)\n",
    "            ax.imshow(a, cmap=transparent_cmap(color), interpolation='nearest')\n",
    "            if showChars: ax.text(x-8,y-10,word_itos[c], fontsize=15)\n",
    "\n",
    "    ax.set_title(char_label_text(chars))\n",
    "    ax.imshow(img.permute(1,2,0), alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def thresh_attn(attn, thresh=0.1):\n",
    "    zeros = torch.zeros_like(attn)\n",
    "    new = torch.where(attn >= thresh, attn, zeros)\n",
    "    \n",
    "    # some attns will not have a value over the thresh in which case\n",
    "    # we need to insert top k value at appropriate index\n",
    "    vals, idxs = torch.topk(attn, 1, dim=-1)\n",
    "    \n",
    "    # reshape\n",
    "    flat_new = new.flatten(0,1)\n",
    "    vals = vals.flatten()\n",
    "    idxs = idxs.flatten()\n",
    "    \n",
    "    for i in range(flat_new.size(0)):\n",
    "        flat_new[i,idxs[i]] = vals[i]\n",
    "\n",
    "    new = flat_new.view_as(new)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Decoder Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(context=\"notebook\")\n",
    "\n",
    "def draw(data, x, y, ax):\n",
    "    return sns.heatmap(data, xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0,\n",
    "                       cmap='YlOrRd', linewidths=0.05, cbar=False, ax=ax)\n",
    "\n",
    "for layer in range(4):\n",
    "    print(\"Decoder Self-Attention Layer\", layer+1)\n",
    "\n",
    "    fig, axes = plt.subplots(1,4, figsize=(20, 10))\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        # greedy decoding (no access to true values)\n",
    "        pred = char_split_text(g_res[i])[20:40]\n",
    "        shifted_y = rshift(g_res.float()).long()\n",
    "        true = char_split_text(shifted_y[i])[20:40]\n",
    "        g = draw(self_attn(layer)[i].data[20:40, 20:40], true, pred, ax=ax)\n",
    "        g.set_yticklabels(g.get_yticklabels(), rotation=0) \n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=0) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Decoder Source-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,4, gridspec_kw={'hspace': 0.5}, figsize=(20, 10))\n",
    "    \n",
    "for idx in range(len(axes.flat)//4):\n",
    "    img = x[idx]\n",
    "    g_chars = g_res[idx]\n",
    "    \n",
    "    # 4 attn layers\n",
    "    for h in range(4):\n",
    "        attn = source_attn(h)\n",
    "        g_attns = to_np(torch_scale_attns(attn)[idx])\n",
    "\n",
    "        show_attn(img, g_attns, g_chars, axes[idx,h], 'YlGn', showChars=False)\n",
    "        axes[idx,h].set_title(f'layer {h+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation vs Greedy (final layer src-attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v_scaled_attns = torch_scale_attns(thresh_attn(v_attn))\n",
    "g_scaled_attns = torch_scale_attns(thresh_attn(g_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, gridspec_kw={'hspace': 0.5}, figsize=(20, 20))\n",
    "for idx in range(len(axes.flat)//2):\n",
    "    img = imgs[idx]\n",
    "\n",
    "    v_chars = v_res[idx]\n",
    "    v_attns = to_np(v_scaled_attns[idx])\n",
    "\n",
    "    g_chars = g_res[idx]\n",
    "    g_attns = to_np(g_scaled_attns[idx])\n",
    "    \n",
    "    # valid\n",
    "    show_attn(img, v_attns, v_chars, axes[idx,0], 'YlOrRd')\n",
    "    # greedy\n",
    "    show_attn(img, g_attns, g_chars, axes[idx,1], 'YlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx=1\n",
    "img = x[idx]\n",
    "\n",
    "g_chars = g_res[idx]\n",
    "g_scaled_attns = torch_scale_attns(thresh_attn(g_attn)[0:1])  # passing in bs of 1\n",
    "g_attns = to_np(g_scaled_attns[0])  # removing bs\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(20, 20))\n",
    "show_attn(img, g_attns, g_chars, ax, 'YlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(context=\"notebook\")\n",
    "\n",
    "def draw(data, x, y, ax):\n",
    "    mask = np.zeros_like(data)\n",
    "    mask[np.triu_indices_from(mask, k=1)] = True\n",
    "    return sns.heatmap(data, xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0,\n",
    "                       mask=mask, cmap='YlOrRd', linewidths=0.05, cbar=False, ax=ax)\n",
    "\n",
    "for layer in range(4):\n",
    "    print(\"Decoder Self-Attention Layer\", layer+1)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(20, 10))\n",
    "    i = 1\n",
    "    # greedy decoding (no access to true values)\n",
    "    pred = char_split_text(g_res[i])[230:280]\n",
    "    shifted_y = rshift(g_res.float()).long()\n",
    "    true = char_split_text(shifted_y[i])[230:280]\n",
    "    g = draw(self_attn(layer)[i].data[230:280, 230:280], true, pred, ax=ax)\n",
    "    g.set_yticklabels(g.get_yticklabels(), rotation=0) \n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=0) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Backprop - chart dependencies (batch leakage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xb,yb = next(iter(learn.data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.model.eval()   # this is important!!!  otherwise batchnorm will mess things up\n",
    "learn.model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xb.requires_grad_(True)\n",
    "xb.grad.zero_()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shifted_y = rshift(yb).long()\n",
    "tgt_mask = subsequent_mask(shifted_y.size(-1))\n",
    "pb = learn.model(xb, shifted_y, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# loss = learn.loss_func(pb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss = pb[2].sum()\n",
    "loss.backward()\n",
    "assert (xb.grad[2] != 0).any()\n",
    "assert (xb.grad[1] == 0.).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xb.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
