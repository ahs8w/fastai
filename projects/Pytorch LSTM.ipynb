{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:45:05.109103Z",
     "start_time": "2018-08-03T22:45:04.834084Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:45:05.188830Z",
     "start_time": "2018-08-03T22:45:05.165327Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:45:05.878652Z",
     "start_time": "2018-08-03T22:45:05.688848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1240545f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "# Why a tuple of 2 x (1,1,3) tensors?? --> Specific to LSTM!!  (hidden state, cell state)\n",
    "# GRU only takes one tensor as hidden\n",
    "\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(out.shape)\n",
    "    print(hidden)\n",
    "    # hidden returns a tuple of 2 x tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.randn(1, 1, 3).shape, inputs[0].view(1,1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gru = nn.GRU(3,3)\n",
    "o,h = gru(inputs[0].view(1,1,-1), hidden[0])\n",
    "o.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "print(inputs.shape)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out.shape)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = torch.randn(5,1,3)\n",
    "hidden = torch.randn(1,1,3)\n",
    "o,h = gru(inputs,hidden)\n",
    "o.shape, h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T15:18:54.434026Z",
     "start_time": "2018-08-03T15:18:54.397100Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "ix_to_tag = {0: \"DET\", 1: \"NN\", 2: \"V\"}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T15:33:03.492417Z",
     "start_time": "2018-08-03T15:33:03.439714Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):  #=> ([ 0,  1,  2,  3,  4])\n",
    "        embeds = self.word_embeddings(sentence) #=> ([5, 6])\n",
    "        lstm_out, self.hidden = self.lstm(embeds.unsqueeze(1), self.hidden)  #=> ([5, 1, 6])\n",
    "        #=> ([5, 1, 6]), (([1, 1, 6]),([1, 1, 6]))\n",
    "        tag_space = self.hidden2tag(lstm_out.squeeze()) #=> ([5, 6])\n",
    "        #=> ([5, 3])\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:45:18.962197Z",
     "start_time": "2018-08-03T22:45:18.936685Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:18:45.292418Z",
     "start_time": "2018-08-02T17:18:45.266507Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The', 'dog', 'ate', 'the', 'apple'],\n",
       " {'Everybody': 5,\n",
       "  'The': 0,\n",
       "  'apple': 4,\n",
       "  'ate': 2,\n",
       "  'book': 8,\n",
       "  'dog': 1,\n",
       "  'read': 6,\n",
       "  'that': 7,\n",
       "  'the': 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0], word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T15:20:10.704765Z",
     "start_time": "2018-08-03T15:20:10.672322Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), tensor([ 0,  1,  2,  3,  4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "inputs.shape, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T15:33:08.136815Z",
     "start_time": "2018-08-03T15:33:06.840142Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1153, -1.0948, -1.0859],\n",
      "        [-1.1902, -1.0613, -1.0502],\n",
      "        [-1.1256, -1.0805, -1.0903],\n",
      "        [-1.2653, -1.0699, -0.9814],\n",
      "        [-1.2593, -1.0399, -1.0144]])\n",
      "tensor([[-0.0692, -4.4247, -2.9032],\n",
      "        [-6.8791, -0.0068, -5.1656],\n",
      "        [-3.8429, -4.3300, -0.0352],\n",
      "        [-0.0318, -5.8369, -3.5614],\n",
      "        [-6.4012, -0.0086, -4.9726]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(500):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T15:33:14.346349Z",
     "start_time": "2018-08-03T15:33:14.318707Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'NN', 'V', 'DET', 'NN']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def check(output):\n",
    "#     return [ix_to_tag[i] for i in (torch.argmax(output, 1).numpy)]\n",
    "\n",
    "# check(tag_scores)\n",
    "\n",
    "idxs = torch.argmax(tag_scores, 1).numpy()\n",
    "[ix_to_tag[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Augmenting the LSTM part-of-speech tagger with character-level features\n",
    "\n",
    "In the example above, each word had an embedding, which served as the inputs to our sequence model. Let’s augment the word embeddings with a representation derived from the characters of the word. We expect that this should help significantly, since character-level information like affixes have a large bearing on part-of-speech. For example, words with the affix -ly are almost always tagged as adverbs in English.\n",
    "\n",
    "To do this, let cw be the character-level representation of word w. Let xw be the word embedding as before. Then the input to our sequence model is the concatenation of xw and cw. So if xw has dimension 5, and cw dimension 3, then our LSTM should accept an input of dimension 8.\n",
    "\n",
    "To get the character level representation, do an LSTM over the characters of a word, and let cw be the final hidden state of this LSTM. Hints:\n",
    "\n",
    "There are going to be two LSTM’s in your new model. The original one that outputs POS tag scores, and the new one that outputs a character-level representation of each word.\n",
    "To do a sequence model over characters, you will have to embed characters. The character embeddings will be the input to the character LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:45:09.584301Z",
     "start_time": "2018-08-03T22:45:09.538557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "{0: '_pad_', 'T': 1, 'h': 2, 'e': 3, 'd': 4, 'o': 5, 'g': 6, 'a': 7, 't': 8, 'p': 9, 'l': 10, 'E': 11, 'v': 12, 'r': 13, 'y': 14, 'b': 15, 'k': 16}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "char_to_ix = {0: '_pad_'}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        for char in list(word):\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "                \n",
    "print(word_to_ix)\n",
    "print(char_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "ix_to_tag = {0: \"DET\", 1: \"NN\", 2: \"V\"}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "CHAR_DIM = 3\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:45:26.582763Z",
     "start_time": "2018-08-03T22:45:26.552916Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_char_sequence(seq):\n",
    "    '''pad to the longest word in sequence and return a 2d tensor'''\n",
    "    h = len(seq)\n",
    "    w = len(max(seq, key=len))\n",
    "    tens = torch.zeros(h,w, dtype=torch.long)\n",
    "    for i,w in enumerate(seq):\n",
    "        idxs = [char_to_ix[c] for c in w]\n",
    "        tens[i,:len(idxs)] = torch.tensor(idxs, dtype=torch.long)\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:35:05.670781Z",
     "start_time": "2018-08-03T22:35:05.644545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_inputs = prepare_char_sequence(training_data[0][0])\n",
    "char_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:19:19.335021Z",
     "start_time": "2018-08-03T22:19:19.308096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embeddings = nn.Embedding(len(char_to_ix), 3)\n",
    "res = char_embeddings(char_inputs)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T22:45:33.869046Z",
     "start_time": "2018-08-03T22:45:33.839890Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(seq):\n",
    "    res = []\n",
    "    w_idxs = [word_to_ix[w] for w in seq]\n",
    "    res.append(torch.tensor(w_idxs, dtype=torch.long))\n",
    "    for w in seq:\n",
    "        idxs = [char_to_ix[c] for c in list(w)]\n",
    "        res.append((torch.tensor([word_to_ix[w]], dtype=torch.long), torch.tensor(idxs, dtype=torch.long)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T21:39:50.710506Z",
     "start_time": "2018-08-03T21:39:50.680298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([ 0]), tensor([ 0,  1,  2])),\n",
       " (tensor([ 1]), tensor([ 3,  4,  5])),\n",
       " (tensor([ 2]), tensor([ 6,  7,  2])),\n",
       " (tensor([ 3]), tensor([ 7,  1,  2])),\n",
       " (tensor([ 4]), tensor([ 6,  8,  8,  9,  2]))]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_inputs(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T23:10:33.714540Z",
     "start_time": "2018-08-03T23:10:33.649088Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CHAR_LSTM_Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, char_size, tagset_size):\n",
    "        super(CHAR_LSTM_Tagger, self).__init__()\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(char_size, CHAR_DIM)\n",
    "        self.char_lstm = nn.LSTM(CHAR_DIM, CHAR_DIM)\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "        self.lstm = nn.LSTM(EMBEDDING_DIM+CHAR_DIM, HIDDEN_DIM)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(HIDDEN_DIM, tagset_size)\n",
    "        \n",
    "    def init_hidden(self, dim, bs=1):\n",
    "        return (torch.zeros(1, bs, dim), torch.zeros(1, bs, dim))\n",
    "\n",
    "    def forward(self, sentence, characters):        \n",
    "#         char_lstm_out = []\n",
    "#         for w in characters:\n",
    "#             char_embeds = self.char_embeddings(w) #([?, 3])\n",
    "#             out, _ = self.char_lstm(char_embeds.unsqueeze(1), self.init_hidden(char_dim)) #=> ([?, 1, 3])\n",
    "#             char_lstm_out.append(out[-1])\n",
    "        \n",
    "#         char_lstm_out = torch.stack(char_lstm_out) #([5, 1, 3])\n",
    "\n",
    "        char_embeds = self.char_embeddings(characters) #([?, 5, 3])\n",
    "        char_out, _ = self.char_lstm(char_embeds, self.init_hidden(CHAR_DIM, len(characters[0]))) #=> ([?, 1, 3])\n",
    "        \n",
    "        char_out = torch.mean(char_out, 1) #([5,3])\n",
    "        \n",
    "        word_embeds = self.word_embeddings(sentence) #=> ([5, 6])\n",
    "        inp = torch.cat((word_embeds,char_out), 1)       #=> ([5, 9])\n",
    "        out, _ = self.lstm(inp.unsqueeze(1), self.init_hidden(EMBEDDING_DIM))\n",
    "        tag_space = self.hidden2tag(out.squeeze(1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T23:11:35.449344Z",
     "start_time": "2018-08-03T23:11:33.938113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1564, -0.9660, -1.1881],\n",
      "        [-1.0685, -1.0727, -1.1571],\n",
      "        [-1.1041, -0.9955, -1.2075],\n",
      "        [-1.0790, -1.1032, -1.1139],\n",
      "        [-1.1232, -1.0572, -1.1167]])\n",
      "tensor([[-0.3203, -1.7639, -2.2762],\n",
      "        [-4.1823, -0.0193, -5.5623],\n",
      "        [-2.6876, -4.8802, -0.0787],\n",
      "        [-0.0381, -3.6916, -4.3832],\n",
      "        [-2.3719, -0.1010, -5.8905]])\n"
     ]
    }
   ],
   "source": [
    "model = CHAR_LSTM_Tagger(len(word_to_ix), len(char_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    word_inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    char_inputs = prepare_char_sequence(training_data[0][0])\n",
    "    tag_scores = model(word_inputs, char_inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "#         model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        chars_in = prepare_char_sequence(sentence)\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in, chars_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    word_inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    char_inputs = prepare_char_sequence(training_data[0][0])\n",
    "    tag_scores = model(word_inputs, char_inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:13:24.481373Z",
     "start_time": "2018-08-03T16:13:24.441351Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_pad_': 0, '_unk_': 1, 'E': 2, 'T': 3, 'a': 4, 'b': 5, 'd': 6, 'e': 7, 'g': 8, 'h': 9, 'k': 10, 'l': 11, 'o': 12, 'p': 13, 'r': 14, 't': 15, 'v': 16, 'y': 17}\n"
     ]
    }
   ],
   "source": [
    "char_training_data = [\"The dog ate the apple\", \"Everybody read that book\"]\n",
    "\n",
    "chars = set()\n",
    "for sentence in char_training_data:\n",
    "    charrs = set(list(sentence.replace(' ', '')))\n",
    "    chars = chars.union(charrs)\n",
    "    \n",
    "itos = sorted(list(chars))\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(1, '_unk_')\n",
    "\n",
    "stoi = {v:k for k,v in enumerate(itos)}\n",
    "print(stoi)\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:08:00.148115Z",
     "start_time": "2018-08-03T17:08:00.057477Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LSTM_CHARTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, char_size, tagset_size):\n",
    "        super(LSTM_CHARTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(char_size, embedding_dim, padding_idx=0)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.char_lstm = nn.LSTM(embedding_dim, embedding_dim)\n",
    "        self.pos_lstm = nn.LSTM(embedding_dim+embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "#         self.char_hidden = self.init_hidden()\n",
    "        self.pos_hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, c=1):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, c, self.hidden_dim),\n",
    "                torch.zeros(1, c, self.hidden_dim))\n",
    "\n",
    "#     def forward(self, sentence):  #=> ([ 0,  1,  2,  3,  4])\n",
    "#         embeds = self.word_embeddings(sentence) #=> ([5, 6])\n",
    "#         lstm_out, self.hidden = self.lstm(embeds.unsqueeze(1), self.hidden)  #=> ([5, 1, 6])\n",
    "#         #=> ([5, 1, 6]), (([1, 1, 6]),([1, 1, 6]))\n",
    "#         tag_space = self.hidden2tag(lstm_out.squeeze()) #=> ([5, 6])\n",
    "#         #=> ([5, 3])\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         return tag_scores\n",
    "    \n",
    "    def forward(self, sentence, characters):\n",
    "        # sentence   => ([5]) (words)\n",
    "        # characters => ([5, 5]) (words, chars)\n",
    "        pdb.set_trace()\n",
    "        char_embeds = self.char_embeddings(characters)  #=> ([5, 5, 6]) (words, chars, embeddings)\n",
    "        self.char_hidden = self.init_hidden(characters.size()[1]) #=> (([1, 5, 6]), ([1, 5, 6]))\n",
    "        char_lstm_out, self.char_hidden = self.char_lstm(char_embeds, self.char_hidden) #=> ([5, 5, 6])\n",
    "        \n",
    "        word_embeds = self.word_embeddings(sentence).unsqueeze(1) #=> ([5, 1, 6])\n",
    "        pos_inp = torch.cat((word_embeds,char_lstm_out), 1)       #=> ([5, 6, 6])\n",
    "        pos_lstm_out, self.pos_hidden = self.pos_lstm(pos_inp, self.pos_hidden)\n",
    "        tag_space = self.hidden2tag(pos_lstm_out.squeeze())\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:03:13.513737Z",
     "start_time": "2018-08-03T17:03:13.483876Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_char_sequence(seq, to_ix):\n",
    "    '''pad to the longest word in sequence and return a 2d tensor'''\n",
    "    arr = seq.split()\n",
    "    h = len(arr)\n",
    "    w = len(max(arr, key=len))\n",
    "    tens = torch.zeros(h,w, dtype=torch.long)\n",
    "    for i,w in enumerate(arr):\n",
    "        idxs = [to_ix[c] for c in w]\n",
    "        tens[i,:len(idxs)] = torch.tensor(idxs, dtype=torch.long)\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:57:14.128224Z",
     "start_time": "2018-08-03T16:57:14.101523Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3.,   9.,   7.,   0.,   0.],\n",
       "        [  6.,  12.,   8.,   0.,   0.],\n",
       "        [  4.,  15.,   7.,   0.,   0.],\n",
       "        [ 15.,   9.,   7.,   0.,   0.],\n",
       "        [  4.,  13.,  13.,  11.,   7.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_inputs = prepare_char_sequence(char_training_data[0], stoi)\n",
    "char_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:29:38.085848Z",
     "start_time": "2018-08-03T16:29:38.058881Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "word_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:13:51.392252Z",
     "start_time": "2018-08-03T17:08:15.017522Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-67-e8691a7c46c2>(42)forward()\n",
      "-> char_embeds = self.char_embeddings(characters)  #=> ([5, 5, 6])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-e8691a7c46c2>(43)forward()\n",
      "-> self.char_hidden = self.init_hidden(characters.size()[1])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-e8691a7c46c2>(44)forward()\n",
      "-> char_lstm_out, self.char_hidden = self.char_lstm(char_embeds, self.char_hidden)\n",
      "(Pdb) self.char_hidden.shape()\n",
      "*** AttributeError: 'tuple' object has no attribute 'shape'\n",
      "(Pdb) self.char_hidden.shape\n",
      "*** AttributeError: 'tuple' object has no attribute 'shape'\n",
      "(Pdb) self.char_hidden[0].shape\n",
      "torch.Size([1, 5, 6])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-e8691a7c46c2>(46)forward()\n",
      "-> word_embeds = self.word_embeddings(sentence)\n",
      "(Pdb) l\n",
      " 41  \t        pdb.set_trace()\n",
      " 42  \t        char_embeds = self.char_embeddings(characters)  #=> ([5, 5, 6])\n",
      " 43  \t        self.char_hidden = self.init_hidden(characters.size()[1])\n",
      " 44  \t        char_lstm_out, self.char_hidden = self.char_lstm(char_embeds, self.char_hidden)\n",
      " 45  \t\n",
      " 46  ->\t        word_embeds = self.word_embeddings(sentence)\n",
      " 47  \t        pos_inp = word_embeds.view(len(sentence), 1, -1) + char_lstm_out\n",
      " 48  \t        pos_lstm_out, self.pos_hidden = self.pos_lstm(pos_inp, self.pos_hidden)\n",
      " 49  \t        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
      " 50  \t        tag_scores = F.log_softmax(tag_space, dim=1)\n",
      " 51  \t        return tag_scores\n",
      "(Pdb) char_lstm_out\n",
      "tensor([[[-0.0763,  0.1352,  0.0137, -0.0201,  0.0392,  0.0284],\n",
      "         [ 0.0022, -0.2613, -0.0324, -0.0466,  0.0389, -0.1961],\n",
      "         [ 0.0772, -0.3701, -0.0846, -0.0033,  0.0574,  0.0420],\n",
      "         [-0.0261,  0.0610, -0.0483,  0.0282,  0.0287,  0.0662],\n",
      "         [-0.0261,  0.0610, -0.0483,  0.0282,  0.0287,  0.0662]],\n",
      "\n",
      "        [[-0.0541,  0.2106,  0.2715,  0.0020,  0.0274,  0.1516],\n",
      "         [ 0.0317, -0.3129, -0.0895, -0.1141,  0.1070, -0.0140],\n",
      "         [ 0.0906, -0.0834,  0.0486, -0.0704,  0.0698, -0.0475],\n",
      "         [-0.0308,  0.0905, -0.0669,  0.0397,  0.0448,  0.1007],\n",
      "         [-0.0308,  0.0905, -0.0669,  0.0397,  0.0448,  0.1007]],\n",
      "\n",
      "        [[-0.1872,  0.2675,  0.1847, -0.0799,  0.0442, -0.0349],\n",
      "         [-0.1063,  0.1201,  0.0181, -0.0848,  0.0900, -0.0650],\n",
      "         [ 0.1058, -0.4032, -0.0511, -0.0447,  0.1280,  0.0233],\n",
      "         [-0.0290,  0.1053, -0.0723,  0.0445,  0.0544,  0.1200],\n",
      "         [-0.0290,  0.1053, -0.0723,  0.0445,  0.0544,  0.1200]],\n",
      "\n",
      "        [[-0.1909,  0.2859,  0.1949, -0.0719,  0.0829, -0.0271],\n",
      "         [-0.0603, -0.2285, -0.0180, -0.0634,  0.0847, -0.2165],\n",
      "         [ 0.1329, -0.4950, -0.1015, -0.0271,  0.1460,  0.0481],\n",
      "         [-0.0256,  0.1128, -0.0729,  0.0468,  0.0603,  0.1315],\n",
      "         [-0.0256,  0.1128, -0.0729,  0.0468,  0.0603,  0.1315]],\n",
      "\n",
      "        [[-0.2972,  0.2221,  0.1703, -0.0939,  0.0650, -0.1487],\n",
      "         [-0.0410, -0.3826,  0.0258, -0.2527,  0.1767,  0.0699],\n",
      "         [ 0.0655, -0.4334, -0.0289, -0.2395,  0.1761,  0.2154],\n",
      "         [ 0.1039,  0.0841, -0.1496,  0.2066, -0.0134,  0.1263],\n",
      "         [ 0.0716, -0.3121, -0.1203,  0.0255,  0.0973,  0.0947]]])\n",
      "(Pdb) char_lstm_out.shape\n",
      "torch.Size([5, 5, 6])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-e8691a7c46c2>(47)forward()\n",
      "-> pos_inp = word_embeds.view(len(sentence), 1, -1) + char_lstm_out\n",
      "(Pdb) l\n",
      " 42  \t        char_embeds = self.char_embeddings(characters)  #=> ([5, 5, 6])\n",
      " 43  \t        self.char_hidden = self.init_hidden(characters.size()[1])\n",
      " 44  \t        char_lstm_out, self.char_hidden = self.char_lstm(char_embeds, self.char_hidden)\n",
      " 45  \t\n",
      " 46  \t        word_embeds = self.word_embeddings(sentence)\n",
      " 47  ->\t        pos_inp = word_embeds.view(len(sentence), 1, -1) + char_lstm_out\n",
      " 48  \t        pos_lstm_out, self.pos_hidden = self.pos_lstm(pos_inp, self.pos_hidden)\n",
      " 49  \t        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
      " 50  \t        tag_scores = F.log_softmax(tag_space, dim=1)\n",
      " 51  \t        return tag_scores\n",
      "[EOF]\n",
      "(Pdb) word_embeds.shape\n",
      "torch.Size([5, 6])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-e8691a7c46c2>(48)forward()\n",
      "-> pos_lstm_out, self.pos_hidden = self.pos_lstm(pos_inp, self.pos_hidden)\n",
      "(Pdb) pos_inp.shape\n",
      "torch.Size([5, 5, 6])\n",
      "(Pdb) cat = torch.cat((word_embeds.view(len(sentence), 1, -1), char_lstm_out), 1) \n",
      "(Pdb) cat.shape\n",
      "torch.Size([5, 6, 6])\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-6be3707a9a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mword_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mchar_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_char_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_training_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-e8691a7c46c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence, characters)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mword_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpos_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchar_lstm_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpos_lstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-e8691a7c46c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence, characters)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mword_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpos_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchar_lstm_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpos_lstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTM_CHARTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(stoi), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    word_inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    char_inputs = prepare_char_sequence(char_training_data[0], stoi)\n",
    "    tag_scores = model(word_inputs, char_inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        chars_in = prepare_char_sequence(sentence, stoi)\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in, chars_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T17:13:51.395709Z",
     "start_time": "2018-08-03T17:08:15.085Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## https://github.com/FraLotito/partofspeech-tagger/blob/master/post.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T18:19:50.932615Z",
     "start_time": "2018-08-03T18:19:50.883541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "car_to_ix = {}\n",
    "\n",
    "def get_index_of_max(input):\n",
    "    index = 0\n",
    "    for i in range(1, len(input)):\n",
    "        if input[i] > input[index]:\n",
    "            index = i \n",
    "    return index\n",
    "\n",
    "def get_max_prob_result(input, ix_to_tag):\n",
    "    return ix_to_tag[get_index_of_max(input)]\n",
    "\n",
    "def prepare_car_sequence(word, to_ix):\n",
    "    idxs = []\n",
    "    for car in word:\n",
    "        idxs.append(to_ix[car])\n",
    "    return idxs\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    res = []\n",
    "    for w in seq:\n",
    "        res.append((to_ix[w], prepare_car_sequence(w, car_to_ix)))\n",
    "    return res\n",
    "\n",
    "def prepare_target(seq, to_ix):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        idxs.append(to_ix[w])\n",
    "    return autograd.Variable(torch.LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T18:20:59.204778Z",
     "start_time": "2018-08-03T18:20:59.178301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 1, 2]),\n",
       " (1, [3, 4, 5]),\n",
       " (2, [6, 7, 2]),\n",
       " (3, [7, 1, 2]),\n",
       " (4, [6, 8, 8, 9, 2])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence(training_data[0][0], word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T18:20:55.472617Z",
     "start_time": "2018-08-03T18:20:55.435245Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        for car in word:\n",
    "        \tif car not in car_to_ix:\n",
    "        \t\tcar_to_ix[car] = len(car_to_ix)\n",
    "\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "ix_to_tag = {0: \"DET\", 1: \"NN\", 2: \"V\"}\n",
    "\n",
    "CAR_EMBEDDING_DIM = 3\n",
    "WORD_EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T19:04:51.836374Z",
     "start_time": "2018-08-03T19:04:51.741622Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class CustomTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, word_embedding_dim, car_embedding_dim, hidden_dim, vocab_size, alphabet_size, tagset_size):\n",
    "\n",
    "        super(CustomTagger, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.car_embedding_dim = car_embedding_dim\n",
    "\n",
    "        self.car_embeddings = nn.Embedding(alphabet_size, car_embedding_dim)\n",
    "        self.lstm_car = nn.LSTM(car_embedding_dim, car_embedding_dim)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.lstm_word = nn.LSTM(word_embedding_dim+car_embedding_dim, hidden_dim)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "        self.hidden = self.init_hidden(hidden_dim)\n",
    "        self.hidden_car = self.init_hidden(car_embedding_dim)\n",
    "\n",
    "    def init_hidden(self, dim):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        word_idxs = []\n",
    "        lstm_car_result = []\n",
    "        for word in sentence:\n",
    "            self.hidden_car = self.init_hidden(self.car_embedding_dim) #(([1, 1, 3]),([1, 1, 3]))\n",
    "            word_idxs.append(word[0])\n",
    "            char_idx = autograd.Variable(torch.LongTensor(word[1])) # ([3])\n",
    "            car_embeds = self.car_embeddings(char_idx) #([3, 3])\n",
    "            lstm_car_out, self.hidden_car = self.lstm_car(car_embeds.unsqueeze(1), self.hidden_car) #([3, 1, 3])\n",
    "            lstm_car_result.append(lstm_car_out[-1])  #([1, 3]) ????  why save only last row??\n",
    "\n",
    "        lstm_car_result = torch.stack(lstm_car_result) #([5, 1, 3])\n",
    "        word_embeds = self.word_embeddings(autograd.Variable(torch.LongTensor(word_idxs))).unsqueeze(1) #([5, 1, 6])\n",
    "        lstm_in = torch.cat((word_embeds, lstm_car_result), 2) #([5, 1, 9])\n",
    "        lstm_out, self.hidden = self.lstm_word(lstm_in, self.hidden) #([5, 1, 6])\n",
    "\n",
    "        tag_space = self.hidden2tag(lstm_out.squeeze(1)) #([5, 6])\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T19:04:55.417927Z",
     "start_time": "2018-08-03T19:04:52.203874Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CustomTagger(WORD_EMBEDDING_DIM, CAR_EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(car_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(300):  \n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "\n",
    "        model.hidden = model.init_hidden(HIDDEN_DIM)\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        \n",
    "        targets = prepare_target(tags, tag_to_ix)\n",
    "\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T19:04:56.235487Z",
     "start_time": "2018-08-03T19:04:56.204530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The: DET\n",
      "dog: NN\n",
      "ate: V\n",
      "the: DET\n",
      "apple: NN\n"
     ]
    }
   ],
   "source": [
    "# ======================= TEST\n",
    "\n",
    "test_sentence = training_data[0][0]\n",
    "inputs = prepare_sequence(test_sentence, word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "for i in range(len(test_sentence)):\n",
    "\tprint('{}: {}'.format(test_sentence[i],get_max_prob_result(tag_scores[i].data.numpy(), ix_to_tag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
