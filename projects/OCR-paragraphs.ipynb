{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T16:52:33.945660Z",
     "start_time": "2018-11-06T16:52:33.330278Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T16:52:37.238227Z",
     "start_time": "2018-11-06T16:52:33.996835Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamschiller/anaconda3/envs/fastai-cpu/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.text import *\n",
    "\n",
    "# from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T16:52:37.420965Z",
     "start_time": "2018-11-06T16:52:37.366756Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = Path('data/IAM_handwriting')\n",
    "TMP_PATH = PATH/'tmp'\n",
    "\n",
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T16:52:37.949984Z",
     "start_time": "2018-11-06T16:52:37.900847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Converting forms to paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## crop forms via xml info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T14:45:02.395018Z",
     "start_time": "2018-10-04T14:45:02.340453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "xml = PATH/'xml'\n",
    "\n",
    "def parse_xml(fname):\n",
    "    tree = ET.parse(xml/fname)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    words = []\n",
    "    for part in root:\n",
    "        for line in part:\n",
    "            for word in line.findall('word'):\n",
    "                for cmp in word:\n",
    "                    x,y = int(cmp.attrib['x']), int(cmp.attrib['y'])\n",
    "                    rx,ry = x+int(cmp.attrib['width']), y+int(cmp.attrib['height'])\n",
    "                    words.append((x,y,rx,ry))\n",
    "    \n",
    "    # convert to numpy\n",
    "    w = np.array(words)\n",
    "    # create box w/ padding    # need top,left and height,width\n",
    "    min_x = np.min(w[:,0])-50\n",
    "    min_y = np.min(w[:,1])-50\n",
    "    max_x = np.max(w[:,2])+50\n",
    "    max_y = np.max(w[:,3])+50\n",
    "    box = (min_x, min_y, max_x, max_y)\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T14:45:03.851890Z",
     "start_time": "2018-10-04T14:45:03.803192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def crop_and_save(fname, box):\n",
    "    im = Image.open(PATH/'forms'/fname)\n",
    "    im = im.crop(box)\n",
    "    im.save(PATH/'paragraphs'/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:26:31.853262Z",
     "start_time": "2018-11-05T19:26:31.797033Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folder = PATH/'forms'\n",
    "files = []\n",
    "for file in os.listdir(folder):\n",
    "    files.append(file)\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T14:54:22.598881Z",
     "start_time": "2018-10-03T14:54:22.548521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(PATH/'paragraphs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T15:22:02.716587Z",
     "start_time": "2018-10-03T15:02:53.664883Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for fname in files:\n",
    "    box = parse_xml(fname[:-4]+'.xml')\n",
    "    crop_and_save(fname, box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:06:01.848242Z",
     "start_time": "2018-11-02T15:06:01.799339Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_rand_paragraph(fnames):\n",
    "    r = random.randint(0, len(fnames))\n",
    "    fname = fnames[r]\n",
    "    im = Image.open(PATH/'paragraphs'/fname)\n",
    "    print(fname)\n",
    "    print(im.size)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:06:12.733661Z",
     "start_time": "2018-11-02T15:06:12.071177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_rand_paragraph(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Combining Line labels into paragraph labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:26:42.361902Z",
     "start_time": "2018-11-05T19:26:41.909184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lines = pd.read_csv(f'{PATH}/ascii/lines.txt', names=['filename','result','value'], escapechar='\\\\', delim_whitespace=True, skiprows=23, header=None, usecols=[0,1,8])\n",
    "lines['text'] = lines.apply(lambda row: row.value.replace('|', ' '), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:26:42.970585Z",
     "start_time": "2018-11-05T19:26:42.909384Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lines.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:37:11.717951Z",
     "start_time": "2018-11-05T19:37:03.268927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for fname in files:\n",
    "    fname = fname[:-4]\n",
    "    text = []\n",
    "    rows = lines[lines.filename.str.startswith(fname+'-')]\n",
    "    for i,t in rows.text.iteritems():\n",
    "        text.append(t+' ')\n",
    "    paragraphs.append((fname+'.png', ''.join(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:34:49.172208Z",
     "start_time": "2018-11-05T19:34:49.123278Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(paragraphs), len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:37:14.315140Z",
     "start_time": "2018-11-05T19:37:14.260152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(paragraphs, columns=['filename', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:37:23.884670Z",
     "start_time": "2018-11-05T19:37:23.818137Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/'paragraphs_df.csv'\n",
    "df.to_csv(CSV, columns=['filename', 'text'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chars (~400 chars/paragraph - max: 705)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:39:33.814297Z",
     "start_time": "2018-11-05T19:39:33.764131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for line in df.text:\n",
    "    chars = chars.union(set(list(line)))\n",
    "    \n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:43:05.871593Z",
     "start_time": "2018-11-05T19:43:05.821293Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itos = sorted(list(chars))\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(1, '_bos_')\n",
    "itos.insert(2, '_eos_')\n",
    "itos.insert(3, '_unk_')\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:47:28.502404Z",
     "start_time": "2018-11-05T19:47:28.362739Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joined_labels = list(df.text)\n",
    "\n",
    "stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "ids = np.array([np.array([1] + [stoi[letter] for letter in word] + [2]) for word in joined_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:48:19.926906Z",
     "start_time": "2018-11-05T19:48:19.409127Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert to strings (as labels)\n",
    "str_ids = np.array([' '.join(str(l) for l in w) for w in ids]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:48:50.852522Z",
     "start_time": "2018-11-05T19:48:50.784842Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['char_ids'] = str_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:51:55.782561Z",
     "start_time": "2018-11-05T19:51:55.702118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# length of longest string -> seq_len\n",
    "df.char_ids.map(lambda x: len(x.split(' '))).mean()\n",
    "#mean = 386\n",
    "#max  = 705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T19:54:22.762227Z",
     "start_time": "2018-11-05T19:54:22.712397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# np.save(TMP_PATH/'paragraph_char_ids.npy', ids)\n",
    "pickle.dump(itos, open(TMP_PATH/'paragraph_char_itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:20.770826Z",
     "start_time": "2018-11-05T23:38:20.624510Z"
    }
   },
   "outputs": [],
   "source": [
    "CSV = PATH/'paragraph_chars.csv'\n",
    "# df.to_csv(CSV, columns=['filename', 'char_ids'], index=False)\n",
    "csv = pd.read_csv(CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:20.928141Z",
     "start_time": "2018-11-05T23:38:20.899959Z"
    }
   },
   "outputs": [],
   "source": [
    "itos = pickle.load(open(TMP_PATH/'paragraph_char_itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:21.186527Z",
     "start_time": "2018-11-05T23:38:21.155582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idxs = np.array(csv.sample(frac=0.15).index)\n",
    "len(val_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word prep  (~120 words/paragraph - max: 170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tokenize/Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:32:14.348192Z",
     "start_time": "2018-11-02T15:32:10.856220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens = Tokenizer().proc_all_mp(partition_by_cores(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:32:18.237185Z",
     "start_time": "2018-11-02T15:32:17.310761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:32:28.946287Z",
     "start_time": "2018-11-02T15:32:28.887598Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = np.argmax([len(o) for o in tokens])\n",
    "idx, len(tokens[idx]), tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T21:03:45.852312Z",
     "start_time": "2018-10-03T21:03:44.766806Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image.open(PATH/'paragraphs'/'g07-000a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:41:06.471379Z",
     "start_time": "2018-11-02T15:41:06.419260Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.percentile([len(o) for o in tokens], 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:38:03.022617Z",
     "start_time": "2018-11-02T15:38:02.906060Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq = Counter(word for line in tokens for word in line)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:33:22.939241Z",
     "start_time": "2018-11-02T15:33:22.889530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_vocab=50000\n",
    "min_freq=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:38:07.362255Z",
     "start_time": "2018-11-02T15:38:07.264496Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itos = [word for word,count in freq.most_common(max_vocab) if count>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(1, '_bos_')\n",
    "itos.insert(2, '_eos_')\n",
    "itos.insert(3, '_unk_')\n",
    "\n",
    "stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "ids = np.array([np.array([1]+[stoi[word] for word in line]+[2]) for line in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:39:39.839543Z",
     "start_time": "2018-11-02T15:39:39.765874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TMP_PATH = PATH/'tmp'\n",
    "np.save(TMP_PATH/'paragraph_ids.npy', ids)\n",
    "pickle.dump(itos, open(TMP_PATH/'paragraph_itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:39:13.915947Z",
     "start_time": "2018-11-02T15:39:13.727071Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ids'] = [' '.join(str(p) for p in o) for o in ids]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:40:37.904478Z",
     "start_time": "2018-11-02T15:40:37.844488Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# length of longest string -> seq_len\n",
    "csv.ids.map(lambda x: len(x.split(' '))).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T16:04:54.832180Z",
     "start_time": "2018-11-02T16:04:54.698811Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/'paragraphs.csv'\n",
    "# df.to_csv(CSV, columns=['filename', 'ids'], index=False)\n",
    "csv = pd.read_csv(CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T16:04:55.293828Z",
     "start_time": "2018-11-02T16:04:55.242331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paragraph_ids = np.load(TMP_PATH/'paragraph_ids.npy')\n",
    "itos = pickle.load(open(TMP_PATH/'paragraph_itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T16:04:55.696717Z",
     "start_time": "2018-11-02T16:04:55.643791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_idxs = np.array(csv.sample(frac=0.15).index)\n",
    "len(val_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (rotate, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:57:54.306616Z",
     "start_time": "2018-11-05T23:57:54.282826Z"
    }
   },
   "outputs": [],
   "source": [
    "f = resnet34\n",
    "sz = 224\n",
    "bs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:57:55.194842Z",
     "start_time": "2018-11-05T23:57:54.700886Z"
    }
   },
   "outputs": [],
   "source": [
    "aug_tfms = [RandomRotate(3, mode=1), RandomLighting(0.05, 0.05)]\n",
    "\n",
    "tfms = tfms_from_model(f, sz, crop_type=CropType.NO, aug_tfms=aug_tfms)\n",
    "data = ImageClassifierData.from_csv(PATH, 'paragraphs', CSV, bs=bs, val_idxs=val_idxs, tfms=tfms, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:57:55.308503Z",
     "start_time": "2018-11-05T23:57:55.284984Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad ends of lines with pad token for language model\n",
    "data.aug_dl.pre_pad=False\n",
    "data.trn_dl.pre_pad=False\n",
    "data.val_dl.pre_pad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Verify dataset transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:48:16.811431Z",
     "start_time": "2018-11-05T22:48:07.522050Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "denorm = data.trn_ds.denorm\n",
    "x,y = next(iter(data.aug_dl))\n",
    "x = denorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:48:19.369121Z",
     "start_time": "2018-11-05T22:48:19.322268Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def label_text(pred):\n",
    "    ints = to_np(pred).astype(int)\n",
    "    ints = np.trim_zeros(ints)   # remove padding (0)\n",
    "    return ' '.join([itos[i] for i in ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:48:20.326007Z",
     "start_time": "2018-11-05T22:48:20.271554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None, title=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    if title: ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:49:04.324626Z",
     "start_time": "2018-11-05T22:48:58.872615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, gridspec_kw={'hspace': 0.5}, figsize=(50,50))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    t = label_text(y[i])\n",
    "    ax=show_img(x[i], ax=ax, title=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:38.769675Z",
     "start_time": "2018-11-05T23:38:38.739763Z"
    }
   },
   "outputs": [],
   "source": [
    "# categorical cross entropy loss\n",
    "# list of probabilities for each word in vocab; target is correct word\n",
    "\n",
    "def seq2seq_loss(input, target):\n",
    "    target = target.permute(1,0).contiguous()\n",
    "    tsl = target.size(0)\n",
    "    sl,bs,nc = input.size()\n",
    "        \n",
    "    if sl>tsl: target = F.pad(target, (0,0,0,sl-tsl))\n",
    "    if tsl>sl: target = target[:sl]\n",
    "        \n",
    "    targ = target.view(-1)\n",
    "    pred = input.view(-1, nc)\n",
    "\n",
    "    # combination of LogSoftmax and NLLLoss\n",
    "    return F.cross_entropy(pred, targ.long(), reduction='sum')/bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:41.760341Z",
     "start_time": "2018-11-05T23:38:41.729248Z"
    }
   },
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "# pulled from Sean Nareen's deepspeech decoder module\n",
    "# https://github.com/SeanNaren/deepspeech.pytorch/blob/master/decoder.py\n",
    "\n",
    "def cer(t, p):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "    Arguments:\n",
    "        t (string): target space-separated sentence\n",
    "        p (string): prediction space-separated sentence\n",
    "    \"\"\"\n",
    "    t, p, = t.replace(' ', ''), p.replace(' ', '')\n",
    "    return Lev.distance(t, p)/len(t)\n",
    "\n",
    "def wer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:41.961138Z",
     "start_time": "2018-11-05T23:38:41.933575Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_text(pred):\n",
    "    ints = to_np(pred).astype(int)\n",
    "    ints = ints[np.nonzero(ints)]\n",
    "    return ''.join([itos[i] for i in ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:42.283433Z",
     "start_time": "2018-11-05T23:38:42.255224Z"
    }
   },
   "outputs": [],
   "source": [
    "def char_error_rate(preds, targs):\n",
    "    bs,sl = targs.size()      #=> ([bs, sl])\n",
    "    # preds.size()            #=> ([sl, bs, vs])\n",
    "        \n",
    "    res = torch.argmax(preds, dim=2)\n",
    "    error = 0\n",
    "    for i in range(bs):\n",
    "        p = label_text(res[:,i])\n",
    "        t = label_text(targs[i])\n",
    "        error += cer(t,p)\n",
    "    return error/bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show, Attend, Tell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:44.489762Z",
     "start_time": "2018-11-05T23:38:44.460860Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, drop=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        slices = {128: -4, 256: -3, 512: -2}\n",
    "        s = slices[embed_size]\n",
    "        \n",
    "        net = f(True)\n",
    "        modules = list(net.children())[:s]    #(8,8,256)\n",
    "        self.base = nn.Sequential(*modules)\n",
    "\n",
    "        #mlp\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.linear1 = nn.Linear(embed_size, embed_size)\n",
    "        self.linear2 = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        bs,nf,h,w = inp.size()\n",
    "        \n",
    "        features = self.base(inp)\n",
    "        features = features.flatten(2, 3).permute(2, 0, 1)\n",
    "        \n",
    "        # initial hidden is an average of features fed through an MLP\n",
    "        h = self.drop(features)\n",
    "        h = self.linear2(F.relu(self.linear1(h))).mean(0)\n",
    "                        \n",
    "        return features, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:44.739520Z",
     "start_time": "2018-11-05T23:38:44.711285Z"
    }
   },
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size, drop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_1 = nn.Linear(embed_size, embed_size)\n",
    "        self.attn_2 = nn.Linear(embed_size, embed_size)\n",
    "        self.drop   = nn.Dropout(drop)\n",
    "        self.attn_v = rand_p(embed_size)\n",
    "        \n",
    "    def forward(self, hidden, feats):\n",
    "        # this is called once for each timestep\n",
    "        w1e = self.attn_1(feats)                    # ([64, 30, 256])\n",
    "        w2h = self.attn_2(hidden)                   # ([30, 256])\n",
    "        u = torch.tanh(w1e + w2h)                   # ([64, 30, 256])\n",
    "        u = self.drop(u)\n",
    "        a = torch.softmax(u @ self.attn_v, 0)       # ([64, 30]) - attention mask\n",
    "        context = (a.unsqueeze(2) * feats).sum(0)   # ([30, 256]) - weighted sum of features w/ attention\n",
    "        \n",
    "        return context, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:45.363953Z",
     "start_time": "2018-11-05T23:38:45.336818Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, drop=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(embed_size, embed_size, num_layers)\n",
    "        if self.num_layers > 1: self.rnn.dropout = drop\n",
    "\n",
    "    def forward(self, hidden, context):\n",
    "        _, h = self.rnn(context.unsqueeze(0), hidden.expand(self.num_layers, -1, -1).contiguous())\n",
    "        \n",
    "        return h[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:38:46.008406Z",
     "start_time": "2018-11-05T23:38:45.983477Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepOutputLayer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, drop=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w0 = nn.Linear(embed_size, embed_size)\n",
    "        self.w1 = nn.Linear(embed_size, embed_size)\n",
    "        self.w2 = nn.Linear(embed_size, embed_size)\n",
    "        self.w3 = nn.Linear(embed_size*3, vocab_size)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, prev, hidden, context):\n",
    "        # this is called once for each timestep\n",
    "        #(30,256)\n",
    "        a = self.w0(prev)\n",
    "        b = self.w1(hidden)\n",
    "        c = self.w2(context)\n",
    "        out = torch.cat([a,b,c], 1)\n",
    "        \n",
    "        return self.w3(self.drop(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:58:06.513666Z",
     "start_time": "2018-11-05T23:58:06.485782Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, seq_len, n_layers=1, drop=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = Encoder(hidden_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.decoder = Decoder(hidden_size, n_layers)\n",
    "        \n",
    "        self.embed   = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.drop    = nn.Dropout(drop)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        self.output  = DeepOutputLayer(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, ret_attn=False):\n",
    "        bs = x.size(0)\n",
    "        feats, hidden = self.encoder(x)\n",
    "        \n",
    "        res,attns = [],[]\n",
    "        dec_inp = torch.ones(bs, device=device).long()  #bos token\n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            prev = self.drop(self.embed(dec_inp))\n",
    "            \n",
    "            context, a = self.attention(hidden, feats)\n",
    "            hidden = self.decoder(hidden, context)\n",
    "            out = self.output(prev, hidden, context)\n",
    "            \n",
    "#             #bptt\n",
    "#             if i%30 == 0:\n",
    "#                 context = repackage_var(context)\n",
    "#                 a       = repackage_var(a)\n",
    "#                 hidden  = repackage_var(hidden)\n",
    "#                 out     = repackage_var(out)\n",
    "            \n",
    "            res.append(out)\n",
    "            attns.append(a)\n",
    "            dec_inp = out.data.max(1)[1]\n",
    "            \n",
    "        res = torch.stack(res)\n",
    "        if ret_attn: res = res,torch.stack(attns)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:58:07.592692Z",
     "start_time": "2018-11-05T23:58:06.925210Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "rnn = Net(256, len(itos), 50, 1)\n",
    "learn = RNN_Learner(data, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.clip = 0.25\n",
    "learn.crit = seq2seq_loss\n",
    "learn.metrics = [char_error_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:58:44.300645Z",
     "start_time": "2018-11-05T23:58:10.372684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead2ee16633e404a9ae3600b5f1852f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   char_error_rate         \n",
      "    0      161.285217 150.115528 0.981975  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[150.11552810668945, 0.9819754754156605]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-3\n",
    "learn.fit(lr, 1, cycle_len=1)\n",
    "\n",
    "# em_sz: 256\n",
    "# 149.260142 0.978963    seq_len: 50, sz: 500, bs: 10\n",
    "# 150.115528 0.981975    seq_len: 50, sz: 224, bs: 30\n",
    "\n",
    "# em_sz: 512, sz: 500, bs: 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T00:01:40.718198Z",
     "start_time": "2018-11-05T23:58:48.863605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880628336fdb4ecba8f53435bbee9b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   char_error_rate         \n",
      "    0      149.500004 149.762295 0.978805  \n",
      "    1      149.030719 149.657045 0.978805                \n",
      "    2      148.803087 149.563997 0.978805                \n",
      "    3      148.683454 149.429966 0.975582                \n",
      "    4      148.485525 149.466808 0.975582                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[149.4668083190918, 0.9755815051725074]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 5, cycle_len=1, use_clr=(10, 4))\n",
    "\n",
    "# em_sz: 256\n",
    "# 150.466808 0.985582    seq_len: 50, sz: 500, bs: 10\n",
    "# 149.466808 0.975582    seq_len: 50, sz: 224, bs: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:32:35.908812Z",
     "start_time": "2018-11-05T23:32:31.092446Z"
    }
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(data.val_dl))\n",
    "learn.model.eval()\n",
    "preds,attns = learn.model(x, ret_attn=True)   #attns ([sl, 64, bs])\n",
    "res = torch.argmax(preds, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:47:43.518368Z",
     "start_time": "2018-11-05T22:46:06.882102Z"
    }
   },
   "outputs": [],
   "source": [
    "seq2seq_loss(preds, y), char_error_rate(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:13:13.015820Z",
     "start_time": "2018-11-05T22:13:12.809116Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_text(pred):\n",
    "    ints = to_np(pred).astype(int)\n",
    "    ints = np.trim_zeros(ints)   # remove padding (0)\n",
    "    return ''.join([itos[i] for i in ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None, title=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    if title: ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = data.trn_ds.denorm(x)\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(10, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "#     t = label_text(y[i])     #=>  char indices\n",
    "    p = label_text(res[:,i])\n",
    "    ax=show_img(imgs[i], ax=ax, title=p)#t+' '+p)\n",
    "    \n",
    "# plt.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_attn_to_img(att, sz):\n",
    "    p = 16\n",
    "    num = sz // p\n",
    "    a = att.reshape(p,p)\n",
    "    return np.repeat(np.repeat(a,num, axis=0), num, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx  = 0\n",
    "img  = imgs[idx]\n",
    "pred = res[:,idx]\n",
    "attn = to_np(attns[:,:,idx])  # first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 3, figsize=(20, 20))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "#     ax.plot(attn[i])\n",
    "    a = scale_attn_to_img(attn[i], 500)\n",
    "    ax.imshow(img, alpha=None)\n",
    "    ax.imshow(a, cmap='Blues', interpolation='nearest', alpha=0.3)\n",
    "    ax.set_title(itos[pred[i].item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
