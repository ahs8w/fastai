{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:26.765963Z",
     "start_time": "2018-06-15T15:08:22.238885Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:26.792410Z",
     "start_time": "2018-06-15T15:08:26.789805Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = Path('data/aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:26.814549Z",
     "start_time": "2018-06-15T15:08:26.811100Z"
    }
   },
   "outputs": [],
   "source": [
    "clas_path = PATH/'imdb_clas/'\n",
    "os.makedirs(clas_path, exist_ok=True)\n",
    "\n",
    "lm_path = PATH/'imdb_lm/'\n",
    "os.makedirs(lm_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data in standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:44:44.997569Z",
     "start_time": "2018-06-14T15:43:48.867196Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = ['pos', 'neg', 'unsup']\n",
    "\n",
    "def get_text(path):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(classes):\n",
    "        for fname in Path(path/label).glob('*.txt'):\n",
    "            texts.append(fname.open('r').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "trn_texts, trn_labels = get_text(PATH/'train')\n",
    "val_texts, val_labels = get_text(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:44:45.022849Z",
     "start_time": "2018-06-14T15:44:45.017412Z"
    }
   },
   "outputs": [],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:19.071637Z",
     "start_time": "2018-06-14T15:45:19.064363Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_idxs = np.random.permutation(len(trn_texts))\n",
    "val_idxs = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:22.414356Z",
     "start_time": "2018-06-14T15:45:20.081501Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_texts = trn_texts[trn_idxs]\n",
    "trn_labels = trn_labels[trn_idxs]\n",
    "\n",
    "val_texts = val_texts[val_idxs]\n",
    "val_labels = val_labels[val_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:24.137028Z",
     "start_time": "2018-06-14T15:45:24.133276Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['labels','text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:27.650867Z",
     "start_time": "2018-06-14T15:45:26.586889Z"
    }
   },
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text': trn_texts, 'labels': trn_labels}, columns=cols)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'labels': val_labels}, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:33.318237Z",
     "start_time": "2018-06-14T15:45:31.879340Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove unsup labels for classifier\n",
    "df_trn[df_trn['labels'] != 2].to_csv(clas_path/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(clas_path/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:35.620084Z",
     "start_time": "2018-06-14T15:45:35.616633Z"
    }
   },
   "outputs": [],
   "source": [
    "# save classes.txt \n",
    "(clas_path/'classes.txt').open('w').writelines(f'{o}\\n' for o in classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Model  \n",
    "Language model doesn't need any labels and can be trained on combination of trn/val data.  Predicts next word only..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:45.684828Z",
     "start_time": "2018-06-14T15:45:41.053667Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_texts,val_texts = sklearn.model_selection.train_test_split(np.concatenate([trn_texts,val_texts]), test_size=0.1)\n",
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:51.393768Z",
     "start_time": "2018-06-14T15:45:47.477028Z"
    }
   },
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text': trn_texts, 'labels': [0]*len(trn_texts)}, columns=cols)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'labels': [0]*len(val_texts)}, columns=cols)\n",
    "\n",
    "df_trn.to_csv(lm_path/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(lm_path/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:56.399002Z",
     "start_time": "2018-06-14T15:45:56.393304Z"
    }
   },
   "outputs": [],
   "source": [
    "chunksize=24000\n",
    "\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:58.406504Z",
     "start_time": "2018-06-14T15:45:58.401136Z"
    }
   },
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:45:59.773376Z",
     "start_time": "2018-06-14T15:45:59.768646Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "\n",
    "    # process all multiprocessing -> parallelization\n",
    "    # partition_by_cores -> takes a list and splits into number of sublists = to number of cores in computer\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:46:01.746216Z",
     "start_time": "2018-06-14T15:46:01.739794Z"
    }
   },
   "outputs": [],
   "source": [
    "# this loops through the chunks of the dataframe.  see *chunksize* below\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:46:04.598746Z",
     "start_time": "2018-06-14T15:46:04.568380Z"
    }
   },
   "outputs": [],
   "source": [
    "# read csv one chunk at a time\n",
    "df_trn = pd.read_csv(lm_path/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(lm_path/'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:47:48.022186Z",
     "start_time": "2018-06-14T15:46:06.855798Z"
    }
   },
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:48:09.264359Z",
     "start_time": "2018-06-14T15:48:09.259729Z"
    }
   },
   "outputs": [],
   "source": [
    "' '.join(tok_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:48:32.695788Z",
     "start_time": "2018-06-14T15:48:22.928824Z"
    }
   },
   "outputs": [],
   "source": [
    "(lm_path/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(lm_path/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(lm_path/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:03:10.975526Z",
     "start_time": "2018-06-14T15:03:06.096406Z"
    }
   },
   "outputs": [],
   "source": [
    "# tok_trn = np.load(lm_path/'tmp'/'tok_trn.npy')\n",
    "# tok_val = np.load(lm_path/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:48:44.299500Z",
     "start_time": "2018-06-14T15:48:39.363939Z"
    }
   },
   "outputs": [],
   "source": [
    "# o - text, p - word\n",
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:48:59.209824Z",
     "start_time": "2018-06-14T15:48:59.206094Z"
    }
   },
   "outputs": [],
   "source": [
    "max_vocab=60000\n",
    "min_freq=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:49:06.291815Z",
     "start_time": "2018-06-14T15:49:06.083040Z"
    }
   },
   "outputs": [],
   "source": [
    "itos = [word for word,count in freq.most_common(max_vocab) if count>min_freq]\n",
    "itos.insert(0, '_unk_')\n",
    "itos.insert(1, '_pad_')\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:49:13.254111Z",
     "start_time": "2018-06-14T15:49:13.223778Z"
    }
   },
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda: 0, {v:k for k,v in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:49:26.400541Z",
     "start_time": "2018-06-14T15:49:20.206569Z"
    }
   },
   "outputs": [],
   "source": [
    "# call stoi for every word in every sentence\n",
    "trn_lm = np.array([[stoi[word] for word in sentence] for sentence in tok_trn])\n",
    "val_lm = np.array([[stoi[word] for word in sentence] for sentence in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T15:49:34.051198Z",
     "start_time": "2018-06-14T15:49:33.037003Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(lm_path/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(lm_path/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(lm_path/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:43.023679Z",
     "start_time": "2018-06-15T15:08:41.469870Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_lm = np.load(lm_path/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(lm_path/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(lm_path/'tmp'/'itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:43.577793Z",
     "start_time": "2018-06-15T15:08:43.570258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60002, 90000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = len(itos)\n",
    "vs, len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Wikitext103 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:49.890750Z",
     "start_time": "2018-06-15T15:08:49.886793Z"
    }
   },
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:50.172004Z",
     "start_time": "2018-06-15T15:08:50.168529Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_path = PATH/'models'/'wt103'\n",
    "pre_lm_path = pre_path/'fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T16:08:28.339238Z",
     "start_time": "2018-06-14T16:08:27.002266Z"
    }
   },
   "outputs": [],
   "source": [
    "wgts = torch.load(pre_lm_path, map_location=lambda storage, loc: storage )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T16:08:51.133094Z",
     "start_time": "2018-06-14T16:08:51.082920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238462, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_mean = enc_wgts.mean(0)\n",
    "enc_wgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T16:09:13.774643Z",
     "start_time": "2018-06-14T16:09:13.653663Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki_itos = pickle.load((pre_path/'itos_wt103.pkl').open('rb'))\n",
    "wiki_stoi = collections.defaultdict(lambda: -1, {v:k for k,v in enumerate(wiki_itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T16:09:36.819096Z",
     "start_time": "2018-06-14T16:09:36.700037Z"
    }
   },
   "outputs": [],
   "source": [
    "# map weights from pretrained wikitext103 model onto our itos; filling unmatched values with the mean weight\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for idx,word in enumerate(itos):\n",
    "    wiki_int = wiki_stoi[word]\n",
    "    new_w[idx] = enc_wgts[wiki_int] if wiki_int >= 0 else row_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T16:10:04.403408Z",
     "start_time": "2018-06-14T16:09:59.934997Z"
    }
   },
   "outputs": [],
   "source": [
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "# weight tying\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T16:11:46.982608Z",
     "start_time": "2018-06-14T16:11:46.427430Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(wgts, pre_path/'pretrained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:08:59.311376Z",
     "start_time": "2018-06-15T15:08:54.613935Z"
    }
   },
   "outputs": [],
   "source": [
    "wgts = torch.load(pre_path/'pretrained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:09:06.960822Z",
     "start_time": "2018-06-15T15:09:06.956946Z"
    }
   },
   "outputs": [],
   "source": [
    "wd = 1e-7\n",
    "bptt = 70\n",
    "bs = 128\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:09:16.711500Z",
     "start_time": "2018-06-15T15:09:14.486815Z"
    }
   },
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:09:24.284234Z",
     "start_time": "2018-06-15T15:09:24.278888Z"
    }
   },
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:09:33.670157Z",
     "start_time": "2018-06-15T15:09:31.744314Z"
    }
   },
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:09:33.670157Z",
     "start_time": "2018-06-15T15:09:31.744314Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.freeze_to(-1)\n",
    "\n",
    "# http://forums.fast.ai/t/pytorch-internal-error-while-doing-the-imdb-notebook/16828\n",
    "# need to use this when training only one layer:  learner.freeze_to(-1)\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:09:41.093131Z",
     "start_time": "2018-06-15T15:09:41.074835Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T22:50:28.420540Z",
     "start_time": "2018-06-14T22:48:26.278880Z"
    }
   },
   "outputs": [],
   "source": [
    "# learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:09:48.501680Z",
     "start_time": "2018-06-15T15:09:48.497051Z"
    }
   },
   "outputs": [],
   "source": [
    "lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T00:15:27.739734Z",
     "start_time": "2018-06-14T22:50:39.216135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9721223eb44b268298ca5d04e1e76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      4.62491    4.40579    0.259161  \n",
      "    1      4.531553   4.339325   0.263369                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.339324908378797, 0.263369329369221]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(lr, 1, wds=wd, use_clr=(32,2), cycle_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T00:15:35.521075Z",
     "start_time": "2018-06-15T00:15:35.328614Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.save('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:10:00.056821Z",
     "start_time": "2018-06-15T15:09:59.477755Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.load('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T15:10:07.685709Z",
     "start_time": "2018-06-15T15:10:07.680810Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T16:38:39.320823Z",
     "start_time": "2018-06-15T15:11:02.860230Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.fit(lr, 1, wds=wd, use_clr=(20,10), cycle_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T16:38:51.064314Z",
     "start_time": "2018-06-15T16:38:50.905165Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.save('lm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T16:38:59.675202Z",
     "start_time": "2018-06-15T16:38:59.486640Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.save_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv one chunk at a time\n",
    "df_trn = pd.read_csv(clas_path/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(clas_path/'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clas_path/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(clas_path/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(clas_path/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "np.save(clas_path/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(clas_path/'tmp'/'val_labels.npy', val_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "186px",
    "left": "1004px",
    "right": "20px",
    "top": "129px",
    "width": "408px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
