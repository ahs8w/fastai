{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Test/Validation set choices\n",
    "\n",
    "Choosing a test set that is reflective of real world scenarios is **the most important** piece of machine learning.\n",
    "Choosing a validation set which reflects the reality of the test set is the **second most important** piece.\n",
    "\n",
    "Strategy for ensuring test/validation sets are in sync:\n",
    "- spin up 5 models of varying accuracy\n",
    "- plot results on both test/validation\n",
    "- where the results align in a line is the most accurate validation set to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "randomly shuffle the data and separate into n chunks  \n",
    "run model n times using different chunk as validation set each time\n",
    "\n",
    "- takes n times longer to run\n",
    "- data must be randomizable (i.e. not temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembling:** set of weak learners are combined to create a strong learner that obtains better performance than a single one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of nearest neighbors (in tree space)   \n",
    "Data limited because each branch halves the data  \n",
    "    + Highly interpretable, scalable, flexible, work well for most data  \n",
    "    - don't extrapolate well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForests\n",
    "    + harder to screw up, easier to scale \n",
    "    + don't need to normalize the data (sort order determines splits) - Immune to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T18:06:31.583858Z",
     "start_time": "2018-04-03T18:06:31.571075Z"
    }
   },
   "source": [
    "In Boosting algorithms each classifier is trained on data, taking into account the previous classifiers’ success. After each training step, the weights are redistributed. Misclassified data increases its weights to emphasise the most difficult cases. In this way, subsequent learners will focus on them during their training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability.\n",
    "\n",
    "If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.\n",
    "\n",
    "By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### SGD - Chain Rule\n",
    "\n",
    "- x: inputs\n",
    "- f(): linear layer\n",
    "- g(): sigmoid/softmax (non-linearity)\n",
    "- h(): x-entropy/RMSE (loss function)  \n",
    "h(g(f(x))) = 0.6\n",
    "\n",
    "- d: derivative wrt weights (w)  \n",
    "Chain Rule (allows us to calculate all of the derivatives at the same time):  \n",
    "\n",
    "d(h(g(f(x),w))) / dw = h'(u)*g'(v)*f'(x)\n",
    "\n",
    "- v = f(x)\n",
    "- u = g(v)  \n",
    "- h(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative is a vector of same length as all the weights  \n",
    "Can be thought of as how much does changing w1 affect the loss, how much does changing w2 affect the loss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General rule of thumb for loss functions:  \n",
    "- RMSE - regression\n",
    "- X-entropy - classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP embedding matrix size: ** ~600 has been proven empirically to be the best choice.  \n",
    "Embedding size -> dependent on the complexity and nuance of variable  \n",
    "Human language is incredibly complex.  Normally don't need embedding matrices approaching 600 in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ML pro-tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- over-parameterize model for better accuracy\n",
    "- regularization (dropout/weight decay) to limit overfitting and generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
